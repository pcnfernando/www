{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"download/","text":"Siddhi 5.2 Download Siddhi \u00b6 Select the appropriate Siddhi distribution for your use case. Siddhi Distribution \u00b6 Siddhi 5.2 ( Distribution 0.1.0 ) Siddhi Tooling Siddhi Runner Refer the user guide to use Siddhi as a Local Microservice Siddhi Docker \u00b6 Siddhi 5.2 (based on Distribution 0.1.0) Siddhi Tooling Siddhi Runner - Alpine Siddhi Runner - Ubuntu Refer the user guide to use Siddhi as a Docker Microservice Siddhi Kubernetes \u00b6 Siddhi 5.2 (based on Distribution 0.1.1) Siddhi CRD Refer the user guide to use Siddhi as Kubernetes Microservice Siddhi Libs \u00b6 Siddhi 5.2.x Siddhi Core Siddhi Query API Siddhi Query Compiler Siddhi Annotation Refer the user guide to use Siddhi as a Java library For other Siddhi Versions refer the Download Archives","title":"Download"},{"location":"download/#siddhi-52-download-siddhi","text":"Select the appropriate Siddhi distribution for your use case.","title":"Siddhi 5.2 Download Siddhi"},{"location":"download/#siddhi-distribution","text":"Siddhi 5.2 ( Distribution 0.1.0 ) Siddhi Tooling Siddhi Runner Refer the user guide to use Siddhi as a Local Microservice","title":"Siddhi Distribution"},{"location":"download/#siddhi-docker","text":"Siddhi 5.2 (based on Distribution 0.1.0) Siddhi Tooling Siddhi Runner - Alpine Siddhi Runner - Ubuntu Refer the user guide to use Siddhi as a Docker Microservice","title":"Siddhi Docker"},{"location":"download/#siddhi-kubernetes","text":"Siddhi 5.2 (based on Distribution 0.1.1) Siddhi CRD Refer the user guide to use Siddhi as Kubernetes Microservice","title":"Siddhi Kubernetes"},{"location":"download/#siddhi-libs","text":"Siddhi 5.2.x Siddhi Core Siddhi Query API Siddhi Query Compiler Siddhi Annotation Refer the user guide to use Siddhi as a Java library For other Siddhi Versions refer the Download Archives","title":"Siddhi Libs"},{"location":"introduction/","text":"Siddhi Deployment Guide \u00b6 This section provides information on developing and running Siddhi. Siddhi Application \u00b6 A self contained stream processing logic can be written as a Siddhi Application and put together in a single file with .siddhi extension. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logic that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, configure them using a common topic using In-Memory Sink and In-Memory Source . For writing Siddhi Application using Streaming SQL refer Siddhi Query Guide Execution Environments \u00b6 Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP) System Requirements \u00b6 For all execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"Siddhi Deployment Guide"},{"location":"introduction/#siddhi-deployment-guide","text":"This section provides information on developing and running Siddhi.","title":"Siddhi Deployment Guide"},{"location":"introduction/#siddhi-application","text":"A self contained stream processing logic can be written as a Siddhi Application and put together in a single file with .siddhi extension. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logic that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, configure them using a common topic using In-Memory Sink and In-Memory Source . For writing Siddhi Application using Streaming SQL refer Siddhi Query Guide","title":"Siddhi Application"},{"location":"introduction/#execution-environments","text":"Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP)","title":"Execution Environments"},{"location":"introduction/#system-requirements","text":"For all execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"System Requirements"},{"location":"release-notes/","text":"Release Notes \u00b6 WIP","title":"Release Notes"},{"location":"release-notes/#release-notes","text":"WIP","title":"Release Notes"},{"location":"development/","text":"Siddhi 5.2 Development Guide \u00b6 Obtaining and Building Project Source code \u00b6 Find the project source code here and the instruction to building the project repos here . Getting Involved in Project Development \u00b6 Siddhi design-related discussions are carried out in the Siddhi-Dev Google Group , you can subscribe to it to get notifications on the discussions and please feel free to get involved by contributing and sharing your thoughts and ideas. You can also propose changes or improvements by starting a thread in the Siddhi-Dev Google Group, and also by reporting issues in the Siddhi GitHub repository with the label type/improvement or type/new-feature . Project Architecture \u00b6 Find out about the architecture of Siddhi for the Siddhi Architecture documentation.","title":"Introduction"},{"location":"development/#siddhi-52-development-guide","text":"","title":"Siddhi 5.2 Development Guide"},{"location":"development/#obtaining-and-building-project-source-code","text":"Find the project source code here and the instruction to building the project repos here .","title":"Obtaining and Building Project Source code"},{"location":"development/#getting-involved-in-project-development","text":"Siddhi design-related discussions are carried out in the Siddhi-Dev Google Group , you can subscribe to it to get notifications on the discussions and please feel free to get involved by contributing and sharing your thoughts and ideas. You can also propose changes or improvements by starting a thread in the Siddhi-Dev Google Group, and also by reporting issues in the Siddhi GitHub repository with the label type/improvement or type/new-feature .","title":"Getting Involved in Project Development"},{"location":"development/#project-architecture","text":"Find out about the architecture of Siddhi for the Siddhi Architecture documentation.","title":"Project Architecture"},{"location":"development/architecture/","text":"Siddhi 5.2 Architecture \u00b6 Siddhi is an open source, cloud-native, stream processing and complex event processing engine. It can be utilized in any of the following ways: Run as a server on its own Run as a micro service on bare metal, VM, Docker and natively in Kubernetes Embedded into any Java or Python based application Run on an Android application Siddhi provides streaming data integration and data analytical operators. It connects multiple disparate live data sources, orchestrates data flows, calculates analytics, and also detects complex event patterns. This allows developers to build applications that collect data, perform data transformation and analytics, and publish the results to data sinks in real time. This section illustrates the architecture of the Siddhi Engine and guides you through its key functionality. We hope this article helps developers to understand Siddhi and its codebase better, and also help them to contribute and improve Siddhi. Main Design Decisions \u00b6 Event-by-event processing of real-time streaming data to achieve low latency. Ease of use with Streaming SQL providing an intuitive way to express stream processing logic and complex event processing constructs such as Patterns. Achieve high performance by processing events in-memory and using data stores for long term data storage. Optimize performance by enforcing a strict event stream schema and by pre-compiling the queries. Optimize memory consumption by having only the absolutely necessary information in-memory and dropping the rest as soon as possible. Supporting multiple extension points to accommodate a diverse set of functionality such as supporting multiple sources, sinks, functions, aggregation operations, windows, etc. High-Level Architecture \u00b6 At a high level, Siddhi consumes events from various events sources, processes them according to the defined Siddhi application, and produces results to the subscribed event sinks. Siddhi can store and consume events from in-memory tables or from external data stores such as RDBMS , MongoDB , Hazelcast in-memory grid, etc. (i.e., when configured to do so). Siddhi also allows applications and users to query Siddhi via its Store Query API to interactively retrieve data from in-memory and other stores. Main Modules in Siddhi Engine \u00b6 Siddhi Engine comprises four main modules, they are: Siddhi Query API : This allows users to define the execution logic of the Siddhi application as queries and definitions using POJOs (Plain Old Java Objects). Internally, Siddhi uses these objects to identify the logic that it is expected to perform. Siddhi Query Compiler : This allows users to define the Siddhi application using the Siddhi Streaming SQL, and it compiles the Streaming SQL script to Siddhi Query API POJOs so that Siddhi can execute them. Siddhi Core : This builds the execution runtime based on the defined Siddhi Application POJOs and processes the incoming events as and when they arrive. Siddhi Annotation : This is a helper module that allows all extensions to be annotated so that they can be picked by Siddhi Core for processing. This also helps Siddhi to generate the extension documentation. Siddhi Component Architecture \u00b6 The following diagram illustrates the main components of Siddhi and how they work together. Here the Siddhi Core module maintains the execution logic. It also interacts with the external environment and systems for consuming, processing and publishing events. It uses the following components to achieve its tasks: SiddhiManager : This is a key component of Siddhi Core that manages Siddhi Application Runtimes and facilitates their functionality via Siddhi Context with periodic state persistence, statistics reporting and extension loading. It is recommended to use one Siddhi Manager for a single JVM. SiddhiAppRuntime : Siddhi Application Runtime can be generated for each Siddhi Application through the Siddhi Manager. Siddhi Application Runtimes provide an isolated execution environment for each defined Siddhi Application. These Siddhi Application Runtimes can have their own lifecycle and they execute based on the logic defined in their Siddhi Application. SiddhiContext : This is a shared object across all the Siddhi Application Runtimes within the same Siddhi manager. It contains references to the persistence store for periodic persistence, statistics manager to report performance statistics of Siddhi Application Runtimes, and extension holders for loading Siddhi extensions. Siddhi Application Creation \u00b6 Execution logic of the Siddhi Engine is composed as a Siddhi Application, and this is usually passed as a string to SiddhiManager to create the SiddhiAppRuntime for execution. When a Siddhi Application is passed to the SiddhiManager.createSiddhiAppRuntime() , it is processed internally with the SiddhiCompiler . Here, the SiddhiApp String is compiled to SiddhiApp object model by the SiddhiQLBaseVisitorImpl class. This validates the syntax of the given Siddhi Application. The model is then passed to the SiddhiAppParser to create the SiddhiAppRuntime . During this phase, the semantics of the Siddhi Application is validated and the execution logic of the Siddhi Application is optimized. Siddhi App Execution Flow \u00b6 Following diagram depicts the execution flow within a Siddhi App Runtime. The path taken by events within Siddhi Engine is indicated in blue. The components that are involved in handling the events are the following: StreamJunction This routes events of a particular stream to various components within the Siddhi App Runtime. A stream junction is generated for each defined or inferred Stream in the Siddhi Application. A stream junction by default uses the incoming event's thread and passes all the events to its subscribed components as soon as they arrive, but this behaviour can be altered by configuring @Async annotation to buffer the events at the and stream junction and to use another one or more threads to collect the events from the buffer and process the subsequent executions. InputHandler Input handler is used to push Event and Event[] objects into stream junctions from defined event sources, and from Java/Python programmes. StreamCallback This receives Event[] s from stream junction and passes them to event sinks to publish to external endpoints, and/or passes them to subscribed Java/Python programmes for further processing. Queries & Partitions These components process events by filtering, transforming, aggregating, joining, pattern matching, etc. They consume events from one or more stream junctions, process them and publish the processed events into a set of stream junctions based on the defined queries or partitions. Source Sources consume events from external sources in various data formats, convert them into Siddhi events using SourceMapper s and pass them to corresponding stream junction via their associated input handlers. A source is generated for each @Source annotation defined above a stream definition. SourceMapper A source mapper is a sub-component of source, and it needs to be configured for each source in order to convert the incoming event into Siddhi event. The source mapper type can be configured using the @Map annotation within the @Source annotation. When the @Map annotation is not defined, Siddhi uses the PassThroughSourceMapper , where it assumes that the incoming message is already in the Siddhi Event format (i.e Event or Event[] ), and therefore makes no changes to the incoming event format. Sink Sinks consumes events from its associated stream junction, convert them to various data formats via SinkMapper and publish them to external endpoints as defined in the @Sink annotation. A sink is generated for each @Sink annotation defined above a stream definition. SinkMapper A sink mapper is a sub-component of sink. and its need to be configured for each sink in order to map the Siddhi events to the specified data format so that they can be published via the sink. The sink mapper type can be configured using the @Map annotation within the @Sink annotation. When the @Map annotation is not defined, Siddhi uses PassThroughSinkMapper , where it passes the Siddhi Event (i.e Event or Event[] ) without any formatting to the Sink. Table Tables are used to store events. When tables are defined by default, Siddhi uses the InMemoryTable implementation to store events in-memory. When @Store annotation is used on top of the table definition, it loads the associated external data store connector based on the defined store type. Most table implementations are extended from either AbstractRecordTable or AbstractQueryableRecordTable abstract classes the former provides the functionality to query external data store based on a given filtering condition, and the latter queries external data store by providing projection, limits, and ordering parameters in addition to data filter condition. Window Windows store events as and when they arrive and automatically expire/clean them based on the given window constraint. Multiple types of windows are can be implemented by extending the WindowProcessor abstract class. IncrementalAggregation Long running time series aggregates defined via the aggregation definition is calculated in an incremental manner using the Incremental Aggregation Processor for the defined time periods. Incremental aggregation functions can be implemented by extending IncrementalAttributeAggregator . By default, incremental aggregations aggregate all the values in-memory, but when it is associated with a store by adding @store annotation it uses in-memory to aggregate partial results and uses data stores to persist those increments. When requested for aggregate results it retrieves data from data stores and (if needed from) in-memory, computes combined aggregate results and provides as the output. Trigger A trigger triggers events at a given interval as given in the trigger definition. The triggered events are pushed to a stream junction having the same name as the trigger. QueryCallback A query callback taps into the events that are emitted by a particular query. It notifies the event occurrence timestamp and classifies the output events into currentEvents , and expiredEvents . Siddhi Query Execution \u00b6 Siddhi QueryRuntimes can be categorized into three main types: SingleInputStream : Queries that consist of query types such as filters and windows. JoinInputStream : Queries that consist of joins. StateInputStream : Queries that consist of patterns and sequences. The following section explains the internals of each query type. SingleInputStream Query Runtime (Filter & Windows) \u00b6 A single input stream query runtime is generated for filter and window queries. They consume events from a stream junction or a window and convert the incoming events according to the expected output stream format at the ProcessStreamReceiver by dropping all the unrelated incoming stream attributes. Then the converted events are passed through a few Processors such as FilterProcessor , StreamProcessor , StreamFunctionProcessor , WindowProcessor , and QuerySelector . Here, the StreamProcessor , StreamFunctionProcessor , and WindowProcessor can be extended with various stream processing capabilities. The last processor of the chain of processors must always be a QuerySelector and it can't appear anywhere else. When the query runtime consumes events from a stream, its processor chain can maximum contain one WindowProcessor , and when query runtime consumes events from a window, its chain of processors cannot contain any WindowProcessor . The FilterProcessor is implemented using expressions that return a boolean value. ExpressionExecutor is used to process conditions, mathematical operations, unary operations, constant values, variables, and functions. Expressions have a tree structure, and they are processed based using the Depth First search algorithm. To achieve high performance, Siddhi currently depends on the user to formulate the least successful case in the leftmost side of the condition, thereby increasing the chance of early false detection. The condition expression price >= 100 and ( Symbol == 'IBM' or Symbol == 'MSFT' ) is represented as shown below. These expressions also support the execution of user-defined functions (UDFs), and they can be implemented by extending the FunctionExecutor class. After getting processed by all the processors, events reach the QuerySelector for transformation. At the QuerySelector , events are transformed based on the select clause of the query. The select clause produces one AttributeProcessor for each output stream attribute, and these AttributeProcessor s contain expressions defining data transformation including constant values, variables, user-defined functions, etc. They can also contain AttributeAggregatorExecutor s to process aggregation operations such as sum , count , etc. If there is a Group By clause defined, then the GroupByKeyGenerator is used to identify the composite group-by key, and then for each key, an AttributeAggregatorExecutor state is generated to maintain per group-by key aggregations. When each time AttributeProcessor is executed the AttributeAggregatorExecutor calculates per group-by aggregation results and output the values. When AttributeAggregatorExecutor group-by states become obsolete, they are destroyed and automatically cleaned. After an event is transformed to the output format through the above process, it is evaluated against the having condition executor if a having clause is provided. The succeeding events are then ordered, and limited based on order by , limit and offset clauses before they pushed to the OutputRateLimiter . At OutputRateLimiter , the event output is controlled before sending the events to the stream junction or to the query callback. When the output clause is not defined, the PassThroughOutputRateLimiter is used by passing all the events without any rate limiting. Temporal Processing with Windows \u00b6 The temporal event processing aspect is achieved via Window and AttributeAggregators To achieve temporal processing, Siddhi uses the following four type of events: Current Events : Events that are newly arriving to the query from streams. Expired Events : Events that have expired from a window. Timer Events : Events that inform the query about an update of execution time. These events are usually generated by schedulers. Reset Events : Events that resets the Siddhi query states. In Siddhi, when an event comes into a WindowProcessor , it creates an appropriate expired event corresponding to the incoming current event with the expiring timestamp, and stores that event in the window. At the same time, WindowProcessor also forwards the current event to the next processor for further processing. It uses a scheduler or some other counting approach to determine when to emit the events that are stored in in-memory. When the expired events meet the condition for expiry based on the window contains, it emits the expired events to the next processor. At times like in window.timeBatch() there can be cases that need emitting all the events in-memory at once and the output does not need individual expired events values, in this cases the window emits a single reset event instead of sending one expired event for each event it has stored, so that it can reset the states in one go. For the QuerySelector aggregations to work correctly the window must emit a corresponding expired event for each current event it has emitted or it must send a reset event . In the QuerySelector , the arrived current events increase the aggregation values, expired events decrease the values, and reset events reset the aggregation calculation to produce correct query output. For example, the sliding TimeWindow ( window.time() ) creates a corresponding expired event for each current event that arrives, adds the expired event s to the window, adds an entry to the scheduler to notify when that event need to be expired, and finally sends the current event to the next processor for subsequent processing. The scheduler notifies the window by sending a timer event , and when the window receives an indication that the expected expiry time has come for the oldest event in the window via a timer event or by other means, it removes the expired event from the window and passes that to the next processor. JoinInputStream Query Runtime (Join) \u00b6 Join input stream query runtime is generated for join queries. This can consume events from two stream junctions and perform a join operation as depicted above. It can also perform a join by consuming events from one stream junction and join against itself, or it can also join against a table, window or an aggregation. When a join is performed with a table, window or aggregation, the WindowProcessor in the above image is replaced with the corresponding table, window or aggregation and no basic processors are used on their side. The joining operation is triggered by the events that arrive from the stream junction. Here, when an event from one stream reaches the pre JoinProcessor , it matches against all the available events of the other stream's WindowProcessor . When a match is found, those matched events are sent to the QuerySelector as current events , and at the same time, the original event is added to the WindowProcessor where it remains until it expires. Similarly, when an event expires from the WindowProcessor , it matches against all the available events of the other stream's WindowProcessor , and when a match is found, those matched events are sent to the QuerySelector as expired events . Note Despite the optimizations, a join query is quite expensive when it comes to performance. This is because the WindowProcessor is locked during the matching process to avoid race conditions and to achieve accuracy while joining. Therefore, when possible avoid matching large (time or length) windows in high volume streams. StateInputStream Query Runtime (Pattern & Sequence) \u00b6 The state input stream query runtime is generated for pattern and sequence queries. This consumes events from one or more stream junctions via ProcessStreamReceiver s and checks whether the events match each pattern or sequence condition by processing the set of basic processors associated with each ProcessStreamReceiver . The PreStateProcessor s usually contains lists of state events that are already matched by previous conditions, and if its the first condition then it will have an empty state event in its list. When ProcessStreamReceiver consumes an event, it passes the event to the PreStateProcessor which updates the list of state events it has with the incoming event and executes the condition by passing the events to the basic processors. The state events that match the conditions reach the PostStateProcessor which will then stores the events to the state event list of the following PreStateProcessor . If it is the final condition's PostStateProcessor , then it will pass the state event to the QuerySelector to generate and emit the output. Siddhi Partition Execution \u00b6 A partition is a wrapper around one or more Siddhi queries and inner streams that connect them. A partition is implemented in Siddhi as a PartitionRuntime which contains multiple QueryRuntime s and inner stream junctions. Each partitioned stream entering the partition goes through a designated PartitionStreamReceiver . The PartitionExecutor of PartitionStreamReceiver evaluates the incoming events to identify their associated partition-key using either RangePartitionExecutor or ValuePartitionExecutor . The identified partition-key is then set as thread local variable and the event is passed to the QueryRuntime s of processing. The QueryRuntime s process events by maintaining separate states for each partition-key such that producing separate output per partition. When a partition query consumes a non-partitioned global stream, the QueryRuntime s are executed for each available partition-key in the system such that allowing all partitions to receive the same event. When the partitions are obsolete PartitionRuntime deletes all the partition states from its QueryRuntime s. Siddhi Aggregation \u00b6 Siddhi supports long duration time series aggregations via its aggregation definition. AggregationRuntime implements this by the use of streaming lambda architecture , where it processes part of the data in-memory and gets part of the data from data stores. AggregationRuntime creates an in-memory table or external store for each time granularity (i.e seconds, minutes, days, etc) it has to process the events, and when events enter it calculates the aggregations in-memory for its least granularity (usually seconds) using the IncrementalExecutor and maintains the running aggregation values in its BaseIncrementalValueStore . At each clock end time of the granularity (end of each second) IncrementalExecutor stores the summarized values to the associated granularity table and also passes the summarized values to the IncrementalExecutor of the next granularity level, which also follows the same methodology in processing the events. Through this approach each time granularities, the current time duration will be in-memory and all the historical time durations will be in stored in the tables. The aggregations results are calculated by IncrementalAttributeAggregator s and stored in such a way that allows proper data composition upon retrial, for example, avg() is stored as sum and count . This allows data composition across various granularity time durations when retrieving, for example, results for avg() composed by returning sum of sum s divided by the sum of count s. Aggregation can also work in a distributed manner and across system restarts. This is done by storing node specific IDs and granularity time duration information in the tables. To make sure tables do not go out of memory IncrementalDataPurger is used to purge old data. When aggregation is queried through join or store query for a given time granularity it reads the data from the in-memory BaseIncrementalValueStore and from the tables computes the composite results as described, and presents the results. Siddhi Event Formats \u00b6 Siddhi has three event formats. Event This is the format exposed to external systems when they send events via Input Handler and consume events via Stream Callback or Query Callback. This consists of a timestamp and an Object[] that contains all the values in accordance to the corresponding stream. StreamEvent (Subtype of ComplexEvent ) This is used within queries. This contains a timestamp and the following three Object[] s: beforeWindowData : This contains values that are only used in processors that are executed before the WindowProcessor . onAfterWindowData : This contains values that are only used by the WindowProcessor and the other processors that follow it, but not sent as output. outputData : This contains the values that are sent via the output stream of the query. In order to optimize the amount of data that is stored in the in-memory at windows, the content in beforeWindowData is cleared before the event enters the WindowProcessor . StreamEvents can also be chained by linking each other via the next property in them. StateEvent (Subtype of ComplexEvent ) This is used in joins, patterns and sequences queries when we need to associate events of multiple streams, tables, windows or aggregations together. This contains a timestamp , a collection of StreamEvent s representing different streams, tables, etc, that are used in the query, and an Object[] to contain outputData values that are needed for query output. The StreamEvent s within the StateEvent and the StateEvent themselves can be chained by linking each other with the next property in them. Event Chunks Event Chunks provide an easier way of manipulating the chain of StreamEvent s and StateEvent s so that they are be easily iterated, inserted and removed. Summary \u00b6 This article focuses on describing the architecture of Siddhi and rationalizing some of the architectural decisions made when implementing the system. It also explains the key features of Siddhi. We hope this will be a good starting point for new developers to understand Siddhi and to start contributing to it.","title":"Architecture"},{"location":"development/architecture/#siddhi-52-architecture","text":"Siddhi is an open source, cloud-native, stream processing and complex event processing engine. It can be utilized in any of the following ways: Run as a server on its own Run as a micro service on bare metal, VM, Docker and natively in Kubernetes Embedded into any Java or Python based application Run on an Android application Siddhi provides streaming data integration and data analytical operators. It connects multiple disparate live data sources, orchestrates data flows, calculates analytics, and also detects complex event patterns. This allows developers to build applications that collect data, perform data transformation and analytics, and publish the results to data sinks in real time. This section illustrates the architecture of the Siddhi Engine and guides you through its key functionality. We hope this article helps developers to understand Siddhi and its codebase better, and also help them to contribute and improve Siddhi.","title":"Siddhi 5.2 Architecture"},{"location":"development/architecture/#main-design-decisions","text":"Event-by-event processing of real-time streaming data to achieve low latency. Ease of use with Streaming SQL providing an intuitive way to express stream processing logic and complex event processing constructs such as Patterns. Achieve high performance by processing events in-memory and using data stores for long term data storage. Optimize performance by enforcing a strict event stream schema and by pre-compiling the queries. Optimize memory consumption by having only the absolutely necessary information in-memory and dropping the rest as soon as possible. Supporting multiple extension points to accommodate a diverse set of functionality such as supporting multiple sources, sinks, functions, aggregation operations, windows, etc.","title":"Main Design Decisions"},{"location":"development/architecture/#high-level-architecture","text":"At a high level, Siddhi consumes events from various events sources, processes them according to the defined Siddhi application, and produces results to the subscribed event sinks. Siddhi can store and consume events from in-memory tables or from external data stores such as RDBMS , MongoDB , Hazelcast in-memory grid, etc. (i.e., when configured to do so). Siddhi also allows applications and users to query Siddhi via its Store Query API to interactively retrieve data from in-memory and other stores.","title":"High-Level Architecture"},{"location":"development/architecture/#main-modules-in-siddhi-engine","text":"Siddhi Engine comprises four main modules, they are: Siddhi Query API : This allows users to define the execution logic of the Siddhi application as queries and definitions using POJOs (Plain Old Java Objects). Internally, Siddhi uses these objects to identify the logic that it is expected to perform. Siddhi Query Compiler : This allows users to define the Siddhi application using the Siddhi Streaming SQL, and it compiles the Streaming SQL script to Siddhi Query API POJOs so that Siddhi can execute them. Siddhi Core : This builds the execution runtime based on the defined Siddhi Application POJOs and processes the incoming events as and when they arrive. Siddhi Annotation : This is a helper module that allows all extensions to be annotated so that they can be picked by Siddhi Core for processing. This also helps Siddhi to generate the extension documentation.","title":"Main Modules in Siddhi Engine"},{"location":"development/architecture/#siddhi-component-architecture","text":"The following diagram illustrates the main components of Siddhi and how they work together. Here the Siddhi Core module maintains the execution logic. It also interacts with the external environment and systems for consuming, processing and publishing events. It uses the following components to achieve its tasks: SiddhiManager : This is a key component of Siddhi Core that manages Siddhi Application Runtimes and facilitates their functionality via Siddhi Context with periodic state persistence, statistics reporting and extension loading. It is recommended to use one Siddhi Manager for a single JVM. SiddhiAppRuntime : Siddhi Application Runtime can be generated for each Siddhi Application through the Siddhi Manager. Siddhi Application Runtimes provide an isolated execution environment for each defined Siddhi Application. These Siddhi Application Runtimes can have their own lifecycle and they execute based on the logic defined in their Siddhi Application. SiddhiContext : This is a shared object across all the Siddhi Application Runtimes within the same Siddhi manager. It contains references to the persistence store for periodic persistence, statistics manager to report performance statistics of Siddhi Application Runtimes, and extension holders for loading Siddhi extensions.","title":"Siddhi Component Architecture"},{"location":"development/architecture/#siddhi-application-creation","text":"Execution logic of the Siddhi Engine is composed as a Siddhi Application, and this is usually passed as a string to SiddhiManager to create the SiddhiAppRuntime for execution. When a Siddhi Application is passed to the SiddhiManager.createSiddhiAppRuntime() , it is processed internally with the SiddhiCompiler . Here, the SiddhiApp String is compiled to SiddhiApp object model by the SiddhiQLBaseVisitorImpl class. This validates the syntax of the given Siddhi Application. The model is then passed to the SiddhiAppParser to create the SiddhiAppRuntime . During this phase, the semantics of the Siddhi Application is validated and the execution logic of the Siddhi Application is optimized.","title":"Siddhi Application Creation"},{"location":"development/architecture/#siddhi-app-execution-flow","text":"Following diagram depicts the execution flow within a Siddhi App Runtime. The path taken by events within Siddhi Engine is indicated in blue. The components that are involved in handling the events are the following: StreamJunction This routes events of a particular stream to various components within the Siddhi App Runtime. A stream junction is generated for each defined or inferred Stream in the Siddhi Application. A stream junction by default uses the incoming event's thread and passes all the events to its subscribed components as soon as they arrive, but this behaviour can be altered by configuring @Async annotation to buffer the events at the and stream junction and to use another one or more threads to collect the events from the buffer and process the subsequent executions. InputHandler Input handler is used to push Event and Event[] objects into stream junctions from defined event sources, and from Java/Python programmes. StreamCallback This receives Event[] s from stream junction and passes them to event sinks to publish to external endpoints, and/or passes them to subscribed Java/Python programmes for further processing. Queries & Partitions These components process events by filtering, transforming, aggregating, joining, pattern matching, etc. They consume events from one or more stream junctions, process them and publish the processed events into a set of stream junctions based on the defined queries or partitions. Source Sources consume events from external sources in various data formats, convert them into Siddhi events using SourceMapper s and pass them to corresponding stream junction via their associated input handlers. A source is generated for each @Source annotation defined above a stream definition. SourceMapper A source mapper is a sub-component of source, and it needs to be configured for each source in order to convert the incoming event into Siddhi event. The source mapper type can be configured using the @Map annotation within the @Source annotation. When the @Map annotation is not defined, Siddhi uses the PassThroughSourceMapper , where it assumes that the incoming message is already in the Siddhi Event format (i.e Event or Event[] ), and therefore makes no changes to the incoming event format. Sink Sinks consumes events from its associated stream junction, convert them to various data formats via SinkMapper and publish them to external endpoints as defined in the @Sink annotation. A sink is generated for each @Sink annotation defined above a stream definition. SinkMapper A sink mapper is a sub-component of sink. and its need to be configured for each sink in order to map the Siddhi events to the specified data format so that they can be published via the sink. The sink mapper type can be configured using the @Map annotation within the @Sink annotation. When the @Map annotation is not defined, Siddhi uses PassThroughSinkMapper , where it passes the Siddhi Event (i.e Event or Event[] ) without any formatting to the Sink. Table Tables are used to store events. When tables are defined by default, Siddhi uses the InMemoryTable implementation to store events in-memory. When @Store annotation is used on top of the table definition, it loads the associated external data store connector based on the defined store type. Most table implementations are extended from either AbstractRecordTable or AbstractQueryableRecordTable abstract classes the former provides the functionality to query external data store based on a given filtering condition, and the latter queries external data store by providing projection, limits, and ordering parameters in addition to data filter condition. Window Windows store events as and when they arrive and automatically expire/clean them based on the given window constraint. Multiple types of windows are can be implemented by extending the WindowProcessor abstract class. IncrementalAggregation Long running time series aggregates defined via the aggregation definition is calculated in an incremental manner using the Incremental Aggregation Processor for the defined time periods. Incremental aggregation functions can be implemented by extending IncrementalAttributeAggregator . By default, incremental aggregations aggregate all the values in-memory, but when it is associated with a store by adding @store annotation it uses in-memory to aggregate partial results and uses data stores to persist those increments. When requested for aggregate results it retrieves data from data stores and (if needed from) in-memory, computes combined aggregate results and provides as the output. Trigger A trigger triggers events at a given interval as given in the trigger definition. The triggered events are pushed to a stream junction having the same name as the trigger. QueryCallback A query callback taps into the events that are emitted by a particular query. It notifies the event occurrence timestamp and classifies the output events into currentEvents , and expiredEvents .","title":"Siddhi App Execution Flow"},{"location":"development/architecture/#siddhi-query-execution","text":"Siddhi QueryRuntimes can be categorized into three main types: SingleInputStream : Queries that consist of query types such as filters and windows. JoinInputStream : Queries that consist of joins. StateInputStream : Queries that consist of patterns and sequences. The following section explains the internals of each query type.","title":"Siddhi Query Execution"},{"location":"development/architecture/#singleinputstream-query-runtime-filter-windows","text":"A single input stream query runtime is generated for filter and window queries. They consume events from a stream junction or a window and convert the incoming events according to the expected output stream format at the ProcessStreamReceiver by dropping all the unrelated incoming stream attributes. Then the converted events are passed through a few Processors such as FilterProcessor , StreamProcessor , StreamFunctionProcessor , WindowProcessor , and QuerySelector . Here, the StreamProcessor , StreamFunctionProcessor , and WindowProcessor can be extended with various stream processing capabilities. The last processor of the chain of processors must always be a QuerySelector and it can't appear anywhere else. When the query runtime consumes events from a stream, its processor chain can maximum contain one WindowProcessor , and when query runtime consumes events from a window, its chain of processors cannot contain any WindowProcessor . The FilterProcessor is implemented using expressions that return a boolean value. ExpressionExecutor is used to process conditions, mathematical operations, unary operations, constant values, variables, and functions. Expressions have a tree structure, and they are processed based using the Depth First search algorithm. To achieve high performance, Siddhi currently depends on the user to formulate the least successful case in the leftmost side of the condition, thereby increasing the chance of early false detection. The condition expression price >= 100 and ( Symbol == 'IBM' or Symbol == 'MSFT' ) is represented as shown below. These expressions also support the execution of user-defined functions (UDFs), and they can be implemented by extending the FunctionExecutor class. After getting processed by all the processors, events reach the QuerySelector for transformation. At the QuerySelector , events are transformed based on the select clause of the query. The select clause produces one AttributeProcessor for each output stream attribute, and these AttributeProcessor s contain expressions defining data transformation including constant values, variables, user-defined functions, etc. They can also contain AttributeAggregatorExecutor s to process aggregation operations such as sum , count , etc. If there is a Group By clause defined, then the GroupByKeyGenerator is used to identify the composite group-by key, and then for each key, an AttributeAggregatorExecutor state is generated to maintain per group-by key aggregations. When each time AttributeProcessor is executed the AttributeAggregatorExecutor calculates per group-by aggregation results and output the values. When AttributeAggregatorExecutor group-by states become obsolete, they are destroyed and automatically cleaned. After an event is transformed to the output format through the above process, it is evaluated against the having condition executor if a having clause is provided. The succeeding events are then ordered, and limited based on order by , limit and offset clauses before they pushed to the OutputRateLimiter . At OutputRateLimiter , the event output is controlled before sending the events to the stream junction or to the query callback. When the output clause is not defined, the PassThroughOutputRateLimiter is used by passing all the events without any rate limiting.","title":"SingleInputStream Query Runtime (Filter &amp; Windows)"},{"location":"development/architecture/#temporal-processing-with-windows","text":"The temporal event processing aspect is achieved via Window and AttributeAggregators To achieve temporal processing, Siddhi uses the following four type of events: Current Events : Events that are newly arriving to the query from streams. Expired Events : Events that have expired from a window. Timer Events : Events that inform the query about an update of execution time. These events are usually generated by schedulers. Reset Events : Events that resets the Siddhi query states. In Siddhi, when an event comes into a WindowProcessor , it creates an appropriate expired event corresponding to the incoming current event with the expiring timestamp, and stores that event in the window. At the same time, WindowProcessor also forwards the current event to the next processor for further processing. It uses a scheduler or some other counting approach to determine when to emit the events that are stored in in-memory. When the expired events meet the condition for expiry based on the window contains, it emits the expired events to the next processor. At times like in window.timeBatch() there can be cases that need emitting all the events in-memory at once and the output does not need individual expired events values, in this cases the window emits a single reset event instead of sending one expired event for each event it has stored, so that it can reset the states in one go. For the QuerySelector aggregations to work correctly the window must emit a corresponding expired event for each current event it has emitted or it must send a reset event . In the QuerySelector , the arrived current events increase the aggregation values, expired events decrease the values, and reset events reset the aggregation calculation to produce correct query output. For example, the sliding TimeWindow ( window.time() ) creates a corresponding expired event for each current event that arrives, adds the expired event s to the window, adds an entry to the scheduler to notify when that event need to be expired, and finally sends the current event to the next processor for subsequent processing. The scheduler notifies the window by sending a timer event , and when the window receives an indication that the expected expiry time has come for the oldest event in the window via a timer event or by other means, it removes the expired event from the window and passes that to the next processor.","title":"Temporal Processing with Windows"},{"location":"development/architecture/#joininputstream-query-runtime-join","text":"Join input stream query runtime is generated for join queries. This can consume events from two stream junctions and perform a join operation as depicted above. It can also perform a join by consuming events from one stream junction and join against itself, or it can also join against a table, window or an aggregation. When a join is performed with a table, window or aggregation, the WindowProcessor in the above image is replaced with the corresponding table, window or aggregation and no basic processors are used on their side. The joining operation is triggered by the events that arrive from the stream junction. Here, when an event from one stream reaches the pre JoinProcessor , it matches against all the available events of the other stream's WindowProcessor . When a match is found, those matched events are sent to the QuerySelector as current events , and at the same time, the original event is added to the WindowProcessor where it remains until it expires. Similarly, when an event expires from the WindowProcessor , it matches against all the available events of the other stream's WindowProcessor , and when a match is found, those matched events are sent to the QuerySelector as expired events . Note Despite the optimizations, a join query is quite expensive when it comes to performance. This is because the WindowProcessor is locked during the matching process to avoid race conditions and to achieve accuracy while joining. Therefore, when possible avoid matching large (time or length) windows in high volume streams.","title":"JoinInputStream Query Runtime (Join)"},{"location":"development/architecture/#stateinputstream-query-runtime-pattern-sequence","text":"The state input stream query runtime is generated for pattern and sequence queries. This consumes events from one or more stream junctions via ProcessStreamReceiver s and checks whether the events match each pattern or sequence condition by processing the set of basic processors associated with each ProcessStreamReceiver . The PreStateProcessor s usually contains lists of state events that are already matched by previous conditions, and if its the first condition then it will have an empty state event in its list. When ProcessStreamReceiver consumes an event, it passes the event to the PreStateProcessor which updates the list of state events it has with the incoming event and executes the condition by passing the events to the basic processors. The state events that match the conditions reach the PostStateProcessor which will then stores the events to the state event list of the following PreStateProcessor . If it is the final condition's PostStateProcessor , then it will pass the state event to the QuerySelector to generate and emit the output.","title":"StateInputStream Query Runtime (Pattern &amp; Sequence)"},{"location":"development/architecture/#siddhi-partition-execution","text":"A partition is a wrapper around one or more Siddhi queries and inner streams that connect them. A partition is implemented in Siddhi as a PartitionRuntime which contains multiple QueryRuntime s and inner stream junctions. Each partitioned stream entering the partition goes through a designated PartitionStreamReceiver . The PartitionExecutor of PartitionStreamReceiver evaluates the incoming events to identify their associated partition-key using either RangePartitionExecutor or ValuePartitionExecutor . The identified partition-key is then set as thread local variable and the event is passed to the QueryRuntime s of processing. The QueryRuntime s process events by maintaining separate states for each partition-key such that producing separate output per partition. When a partition query consumes a non-partitioned global stream, the QueryRuntime s are executed for each available partition-key in the system such that allowing all partitions to receive the same event. When the partitions are obsolete PartitionRuntime deletes all the partition states from its QueryRuntime s.","title":"Siddhi Partition Execution"},{"location":"development/architecture/#siddhi-aggregation","text":"Siddhi supports long duration time series aggregations via its aggregation definition. AggregationRuntime implements this by the use of streaming lambda architecture , where it processes part of the data in-memory and gets part of the data from data stores. AggregationRuntime creates an in-memory table or external store for each time granularity (i.e seconds, minutes, days, etc) it has to process the events, and when events enter it calculates the aggregations in-memory for its least granularity (usually seconds) using the IncrementalExecutor and maintains the running aggregation values in its BaseIncrementalValueStore . At each clock end time of the granularity (end of each second) IncrementalExecutor stores the summarized values to the associated granularity table and also passes the summarized values to the IncrementalExecutor of the next granularity level, which also follows the same methodology in processing the events. Through this approach each time granularities, the current time duration will be in-memory and all the historical time durations will be in stored in the tables. The aggregations results are calculated by IncrementalAttributeAggregator s and stored in such a way that allows proper data composition upon retrial, for example, avg() is stored as sum and count . This allows data composition across various granularity time durations when retrieving, for example, results for avg() composed by returning sum of sum s divided by the sum of count s. Aggregation can also work in a distributed manner and across system restarts. This is done by storing node specific IDs and granularity time duration information in the tables. To make sure tables do not go out of memory IncrementalDataPurger is used to purge old data. When aggregation is queried through join or store query for a given time granularity it reads the data from the in-memory BaseIncrementalValueStore and from the tables computes the composite results as described, and presents the results.","title":"Siddhi Aggregation"},{"location":"development/architecture/#siddhi-event-formats","text":"Siddhi has three event formats. Event This is the format exposed to external systems when they send events via Input Handler and consume events via Stream Callback or Query Callback. This consists of a timestamp and an Object[] that contains all the values in accordance to the corresponding stream. StreamEvent (Subtype of ComplexEvent ) This is used within queries. This contains a timestamp and the following three Object[] s: beforeWindowData : This contains values that are only used in processors that are executed before the WindowProcessor . onAfterWindowData : This contains values that are only used by the WindowProcessor and the other processors that follow it, but not sent as output. outputData : This contains the values that are sent via the output stream of the query. In order to optimize the amount of data that is stored in the in-memory at windows, the content in beforeWindowData is cleared before the event enters the WindowProcessor . StreamEvents can also be chained by linking each other via the next property in them. StateEvent (Subtype of ComplexEvent ) This is used in joins, patterns and sequences queries when we need to associate events of multiple streams, tables, windows or aggregations together. This contains a timestamp , a collection of StreamEvent s representing different streams, tables, etc, that are used in the query, and an Object[] to contain outputData values that are needed for query output. The StreamEvent s within the StateEvent and the StateEvent themselves can be chained by linking each other with the next property in them. Event Chunks Event Chunks provide an easier way of manipulating the chain of StreamEvent s and StateEvent s so that they are be easily iterated, inserted and removed.","title":"Siddhi Event Formats"},{"location":"development/architecture/#summary","text":"This article focuses on describing the architecture of Siddhi and rationalizing some of the architectural decisions made when implementing the system. It also explains the key features of Siddhi. We hope this will be a good starting point for new developers to understand Siddhi and to start contributing to it.","title":"Summary"},{"location":"development/build/","text":"Building Siddhi 5.2 Repos \u00b6 Building Java Repos \u00b6 Prerequisites \u00b6 Oracle JDK 8 , OpenJDK 8 , or JDK 11 (Java 8 should be used for building in order to support both Java 8 and Java 11 at runtime) Maven 3.5.x or later version Steps to Build \u00b6 Get a clone or download source from Github repo, E.g. git clone https://github.com/siddhi-io/siddhi.git Run the Maven command mvn clean install from the root directory Command Description mvn clean install Build and install the artifacts into the local repository. mvn clean install -Dmaven.test.skip=true Build and install the artifacts into the local repository, without running any of the unit tests.","title":"Build"},{"location":"development/build/#building-siddhi-52-repos","text":"","title":"Building Siddhi 5.2 Repos"},{"location":"development/build/#building-java-repos","text":"","title":"Building Java Repos"},{"location":"development/build/#prerequisites","text":"Oracle JDK 8 , OpenJDK 8 , or JDK 11 (Java 8 should be used for building in order to support both Java 8 and Java 11 at runtime) Maven 3.5.x or later version","title":"Prerequisites"},{"location":"development/build/#steps-to-build","text":"Get a clone or download source from Github repo, E.g. git clone https://github.com/siddhi-io/siddhi.git Run the Maven command mvn clean install from the root directory Command Description mvn clean install Build and install the artifacts into the local repository. mvn clean install -Dmaven.test.skip=true Build and install the artifacts into the local repository, without running any of the unit tests.","title":"Steps to Build"},{"location":"development/source/","text":"Siddhi 5.2 Source Code \u00b6 Project Source Code \u00b6 Siddhi Core Java Library \u00b6 https://github.com/siddhi-io/siddhi (Java) Siddhi repo, containing the core Java libraries of Siddhi. PySiddhi \u00b6 https://github.com/siddhi-io/pysiddhi (Python) The Python wrapper for Siddhi core Java libraries. This depends on the siddhi-io/siddhi repo. Siddhi Local Microservice Distribution \u00b6 https://github.com/siddhi-io/distribution (Java) The Microservice distribution of the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi repo. Siddhi Docker Microservice Distribution \u00b6 https://github.com/siddhi-io/docker-siddhi (Docker) The Docker wrapper for the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi and siddhi-io/distribution repos. Siddhi Kubernetes Operator \u00b6 https://github.com/siddhi-io/siddhi-operator (Go) The Siddhi Kubernetes CRD repo deploying Siddhi on Kubernetes. This depends on the siddhi-io/siddhi , siddhi-io/distribution and siddhi-io/docker-siddhi repos. Siddhi Extensions \u00b6 Find the supported Siddhi extensions and source here","title":"Source"},{"location":"development/source/#siddhi-52-source-code","text":"","title":"Siddhi 5.2 Source Code"},{"location":"development/source/#project-source-code","text":"","title":"Project Source Code"},{"location":"development/source/#siddhi-core-java-library","text":"https://github.com/siddhi-io/siddhi (Java) Siddhi repo, containing the core Java libraries of Siddhi.","title":"Siddhi Core Java Library"},{"location":"development/source/#pysiddhi","text":"https://github.com/siddhi-io/pysiddhi (Python) The Python wrapper for Siddhi core Java libraries. This depends on the siddhi-io/siddhi repo.","title":"PySiddhi"},{"location":"development/source/#siddhi-local-microservice-distribution","text":"https://github.com/siddhi-io/distribution (Java) The Microservice distribution of the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi repo.","title":"Siddhi Local Microservice Distribution"},{"location":"development/source/#siddhi-docker-microservice-distribution","text":"https://github.com/siddhi-io/docker-siddhi (Docker) The Docker wrapper for the Siddhi Tooling and Siddhi Runtime. This depends on the siddhi-io/siddhi and siddhi-io/distribution repos.","title":"Siddhi Docker Microservice Distribution"},{"location":"development/source/#siddhi-kubernetes-operator","text":"https://github.com/siddhi-io/siddhi-operator (Go) The Siddhi Kubernetes CRD repo deploying Siddhi on Kubernetes. This depends on the siddhi-io/siddhi , siddhi-io/distribution and siddhi-io/docker-siddhi repos.","title":"Siddhi Kubernetes Operator"},{"location":"development/source/#siddhi-extensions","text":"Find the supported Siddhi extensions and source here","title":"Siddhi Extensions"},{"location":"docs/","text":"Siddhi 5.2 User Guide \u00b6 This section provides information on using and running Siddhi. Checkout the Siddhi features to get and idea on what it can do in brief. Writing Siddhi Applications \u00b6 Writing steam processing logic in Siddhi is all about building Siddhi Applications. A Siddhi Application is a script with .siddhi file extension having self-contained stream processing logic. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logics that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, users can configure common topic using In-Memory Sink and In-Memory Source in order to communicate between them. To write Siddhi Applications using Siddhi Streaming SQL refer Siddhi Query Guide for details. For specific API information on Siddhi functions and features refer Siddhi API Guide . Find out about the supported Siddhi extensions and their versions here . Executing Siddhi Applications \u00b6 Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP) Siddhi Configurations \u00b6 Refer the Siddhi Config Guide for information on advance Siddhi execution configurations. System Requirements \u00b6 For all Siddhi execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"Introduction"},{"location":"docs/#siddhi-52-user-guide","text":"This section provides information on using and running Siddhi. Checkout the Siddhi features to get and idea on what it can do in brief.","title":"Siddhi 5.2 User Guide"},{"location":"docs/#writing-siddhi-applications","text":"Writing steam processing logic in Siddhi is all about building Siddhi Applications. A Siddhi Application is a script with .siddhi file extension having self-contained stream processing logic. The stream processing constructs, such as streams and queries, defined within a Siddhi App is not visible even to the other Siddhi Apps running in the same JVM. It is recommended to have different business usecase in separate Siddhi Applications, where it allow users to selectively deploy the applications based on business needs. It is also recommended to move the repeated steam processing logics that exist in multiple Siddhi Applications, such as message retrieval and preprocessing, to a common Siddhi Application, whereby reducing code duplication and improving maintainability. In this case, to pass the events from one Siddhi App to another, users can configure common topic using In-Memory Sink and In-Memory Source in order to communicate between them. To write Siddhi Applications using Siddhi Streaming SQL refer Siddhi Query Guide for details. For specific API information on Siddhi functions and features refer Siddhi API Guide . Find out about the supported Siddhi extensions and their versions here .","title":"Writing Siddhi Applications"},{"location":"docs/#executing-siddhi-applications","text":"Siddhi can run in multiple environments as follows. As a Java Library As a Local Microservice As a Docker Microservice As a Kubernetes Microservice As a Python Library (WIP)","title":"Executing Siddhi Applications"},{"location":"docs/#siddhi-configurations","text":"Refer the Siddhi Config Guide for information on advance Siddhi execution configurations.","title":"Siddhi Configurations"},{"location":"docs/#system-requirements","text":"For all Siddhi execution modes following are the general system requirements. Memory - 128 MB (minimum), 500 MB (recommended), higher memory might be needed based on in-memory data stored for processing Cores - 2 cores (recommended), use lower number of cores after testing Siddhi Apps for performance JDK - 8 or 11 To build Siddhi from the Source distribution, it is necessary that you have JDK version 8 or 11 and Maven 3.0.4 or later","title":"System Requirements"},{"location":"docs/config-guide/","text":"Siddhi 5.2 Config Guide \u00b6 This section covers the following. Configuring Databases Configuring Periodic State Persistence Configuring Siddhi Sources, Sinks, Stores and Extensions Configuring Authentication Adding Extensions and Third Party Dependencies Configuring Statistics Converting Jars to OSGi Bundles Configuring Databases \u00b6 Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. It is recommended to configure RDBMS databases as datasources under wso2.datasources section of Siddhi configuration yaml, and pass it during startup, this will allow database to reuse connections across multiple Siddhi Apps. By default Siddhi stores product-specific data in predefined embedded H2 database located in <SIDDHI_RUNNER_HOME>/wso2/runner/database directory. Here, the default H2 database is only suitable for development, testing, and some production environments which do not store data. However, for most production environments we recommend using industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MSSQL. In this case users are expected to add the relevant database drivers to Siddhi's class-path. Including database drivers. The database driver corresponding to the database should be an OSGi bundle and it need to be added to <SIDDHI_RUNNER_HOME>/lib/ directory. If the driver is a jar then this should be converted to an OSGi bundle before adding . Converting Non OSGi drivers. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. The necessary table schemas are self generated by the features themselves, other than the tables needed for statistics reporting via databases . Below are the sample datasource configuration for each supported database types: MySQL Oracle There are two ways to configure Oracle. If you have a System Identifier (SID), use this (older) format: jdbc:oracle:thin:@[HOST][:PORT]:SID If you have an Oracle service name, use this (newer) format: jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE PostgreSQL MSSQL Configuring Periodic State Persistence \u00b6 Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. This explains how to periodically persisting the state of Siddhi either into a database system or file system, in order to prevent data losses that can result from a system failure. Persistence on Database \u00b6 To perform periodic state persistence on a database, the database should be configured as a datasource and the relevant jdbc drivers should be added to Siddhi's class-path. Refer Database Configuration section for more information. To configure database based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.DBPersistenceStore config > datasource The datasource to be used in persisting the state. The datasource should be defined in the Siddhi configuration yaml. For detailed instructions of how to configure a datasource, see Database Configuration . SIDDHI_PERSISTENCE_DB (A datasource that is defined in wso2.datasources in Siddhi configuration yaml) config > table The table that should be created and used for persisting states. PERSISTENCE_TABLE The following is a sample configuration for database based state persistence. Persistence on File System \u00b6 To configure file system based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config > location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample configuration for file system based state persistence. Configuring Siddhi Elements \u00b6 Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. You can configure some of there environment specific configurations in the Siddhi Configuration yaml rather than configuring in-line, such that your Siddhi Application can become potable between environments. Configuring Sources, Sinks and Stores \u00b6 Multiple sources, sinks, and stores could be defined in Siddhi Configuration yaml as ref , and referred by several Siddhi Applications as described below. The following is the syntax for the configuration. siddhi: refs: - ref: name: '<name>' type: '<type>' properties: <property1>: <value1> <property2>: <value2> For each separate refs you want to configure, add a sub-section named ref under the refs subsection. The ref configured in Siddhi Configuration yaml can be referred from a Siddhi Application Source as follows. @Source(ref='<name>', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); Similarly Sinks and Store Tables can also be configured and referred from Siddhi Apps. For each separate refs you want to configure, add a sub-section named ref under the refs subsection. Example : Configuring http source using ref Following configuration defines the url and details about basic.auth , in the Siddhi Configuration yaml. siddhi: refs: - ref: name: 'http-passthrough' type: 'http' properties: receiver.url: 'http://0.0.0.0:8008/sweet-production' basic.auth.enabled: false This can be referred in the Siddhi Applications as follows. @Source(ref='http-passthrough', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); Configuring Extensions \u00b6 Siddhi extensions cater use-case specific logic that is not available by default in Siddhi. Some of these extensions have system parameter configurations to define/modify their behavior. These extensions usually have default values for the parameters, but when needed, they can be overridden by configuring the parameters in Siddhi Configuration yaml and passing it at startup. The following is the syntax for the configuration. siddhi: extensions: - extension: name: <extension name> namespace: <extension namespace> properties: <key>: <value> For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. Following are some examples on overriding default system properties via Siddhi Configuration yaml Example 1 : Defining service host and port for the TCP source siddhi: extensions: - extension: name: tcp namespace: source properties: host: 0.0.0.0 port: 5511 Example 2 : Overwriting the default RDBMS extension configuration siddhi: extensions: - extension: name: rdbms namespace: store properties: mysql.batchEnable: true mysql.batchSize: 1000 mysql.indexCreateQuery: \"CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}})\" mysql.recordDeleteQuery: \"DELETE FROM {{TABLE_NAME}} {{CONDITION}}\" mysql.recordExistsQuery: \"SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1\" Configuring Authentication \u00b6 Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. By default, Siddhi is configured with user name admin , and password admin . This can be updated by adding related user management configuration as auth.configs to the Siddhi Configuration yaml, and pass it at startup. A sample auth.configs is as follows. # Authentication configuration auth.configs: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin Adding Extensions and Third Party Dependencies \u00b6 Applicable for all modes. For certain use-cases, Siddhi might require extensions and/or third party dependencies to fulfill some characteristics that it does not provide by default. This section provides details on how to add or update extension and/or third party dependencies that is needed by Siddhi. Adding to Siddhi Java Program \u00b6 When running Siddhi as a Java library, the extension jars and/or third-party dependencies needed for Siddhi can be simply added to Siddhi class-path. When Maven is used as the build tool add them to the pom.xml file along with the other mandatory jars needed by Siddhi as given is Using Siddhi as a library guide. A sample on adding siddhi-io-http extension to the Maven pom.xml is as follows. <!--HTTP extension--> <dependency> <groupId>org.wso2.extension.siddhi.io.http</groupId> <artifactId>siddhi-io-http</artifactId> <version>${siddhi.io.http.version}</version> </dependency> Refer guide for more details on using Siddhi as a Java Library. Adding to Siddhi Local Microservice \u00b6 The most used Siddhi extensions are packed by default with the Siddhi Local Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, adding or replacing the relevant OSGi JARs in <SIDDHI_RUNNER_HOME>/lib directory. Since Local Microservice is OSGi-based, when adding libraries/drivers they need to be checked if they are OSGi bundles, and if not convert to OSGi before adding them to the <SIDDHI_RUNNER_HOME>/lib directory. Converting Jars to OSGi Bundles.. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Local Microservice. Adding to Siddhi Docker Microservice \u00b6 The most used Siddhi extensions are packed by default with the Siddhi Docker Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, a new docker image has to be built from either siddhi-runner-base-ubuntu or siddhi-runner-base-alpine images. These images contain Linux OS, JDK and the Siddhi distribution. Sample docker file using siddhi-runner-base-alpine is as follows. Find the necessary artifacts to build the docker from docker-siddhi repository. The necessary OSGi jars and extensions that need to be added to the Siddhi Docker Microservice should be placed at ${BUNDLE_JAR_DIR} ( ./files/lib ) folder as defined in the above docker file, such that they would be bundled during the docker build phase. Converting Jars to OSGi Bundles If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Docker Microservice. Adding to Siddhi Kubernetes Microservice \u00b6 To add or update Siddhi extensions and/or third-party dependencies, a custom docker image has to be created using the steps described in Adding to Siddhi Docker Microservice documentation including the necessary extensions and dependencies. The created image can be then referenced in the sepc.pod subsection in the SiddhiProcess Kubernetes artifact created to deploy Siddhi in Kubernetes. For details on creating the Kubernetes artifacts refer Using Siddhi as Kubernetes Microservice documentation. Configuring Statistics \u00b6 Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi uses dropwizard metrics library to calculate Siddhi and JVM statistics, and it can report the results via JMX Mbeans, console or database. To enable statistics, the relevant configuration should be added to the Siddhi Configuration yaml as follows, and at the same time the statistics collection should be enabled in the Siddhi Application which is being monitored. Refer Siddhi Application Statistics documentation for enabling Siddhi Application level statistics. To enable statistics the relevant matrics related configurations should be added under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. Configuring Metrics reporting level. To modify the statistics reporting, relevant metric names can be added under the wso2.metrics.levels subsection in the Siddhi Configurations yaml, along with the matrics level (i.e., OFF, INFO, DEBUG, TRACE, or ALL) as given bellow. wso2.metrics: # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO The available metrics reporting options are as follows. Reporting via JMX Mbeans \u00b6 JMX Mbeams is the default statistics reporting option of Siddhi. To enable stats with the default configuration add the metric-related properties under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. A sample configuration is as follows. wso2.metrics: enabled: true This will report JMX Mbeans in the name of org.wso2.carbon.metrics . However, in this default configuration the JVM metrics will not be measured. A detail JMX configuration along with the metrics reporting level is as follows. wso2.metrics: # Enable Metrics enabled: true jmx: # Register MBean when initializing Metrics registerMBean: true # MBean Name name: org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO Reporting via Console \u00b6 To enable statistics by periodically printing the metrics on console add the following configuration to the the Siddhi Configurations yaml file, and pass that during startup. # This is the main configuration for metrics wso2.metrics: # Enable Metrics enabled: false reporting: console: - # The name for the Console Reporter name: Console # Enable Console Reporter enabled: false # Polling Period in seconds. # This is the period for polling metrics from the metric registry and printing in the console pollingPeriod: 5 Reporting via Database \u00b6 To enable JDBC reporting and to periodically clean up the outdated statistics from the database, first a datasource should be created with the relevant database configurations and then the related metrics properties as given bellow should be added to in the Siddhi Configurations yaml file, and pass that during startup. The bellow sample is referring to the datasource with JNDI name jdbc/SiddhiMetricsDB , hence the datasource configuration in yaml should have jndiConfig.name as jdbc/SiddhiMetricsDB . For detailed instructions on configuring a datasource, refer Configuring Databases . . The scripts to create these tables are provided in the <SIDDHI_RUNNER_HOME>/wso2/runner/dbscripts directory. Sample configuration of reporting via database. wso2.metrics: # Enable Metrics enabled: true jdbc: # Data Source Configurations for JDBC Reporters dataSource: # Default Data Source Configuration - &JDBC01 # JNDI name of the data source to be used by the JDBC Reporter. # This data source should be defined under wso2.datasources. dataSourceName: java:comp/env/jdbc/SiddhiMetricsDB # Schedule regular deletion of metrics data older than a set number of days. # It is recommended that you enable this job to ensure your metrics tables do not get extremely large. # Deleting data older than seven days should be sufficient. scheduledCleanup: # Enable scheduled cleanup to delete Metrics data in the database. enabled: false # The scheduled job will cleanup all data older than the specified days daysToKeep: 7 # This is the period for each cleanup operation in seconds. scheduledCleanupPeriod: 86400 reporting: jdbc: - # The name for the JDBC Reporter name: JDBC # Enable JDBC Reporter enabled: true # Source of Metrics, which will be used to identify each metric in database --> # Commented to use the hostname by default # source: Siddhi # Alias referring to the Data Source configuration dataSource: *JDBC01 # Polling Period in seconds. # This is the period for polling metrics from the metric registry and updating the database with the values pollingPeriod: 60 Metrics history and reporting interval If the wso2.metrics.reporting.jdbc subsection is not enabled, the information relating to metrics history will not be persisted for future references. Also note the that the reporting will only start to update the database after the given pollingPeriod time has elapsed. Information about the parameters configured under the jdbc.dataSource subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description dataSourceName java:comp/env/jdbc/SiddhiMetricsDB java:comp/env/<datasource JNDI name> . The JNDI name of the datasource used to store metric data. scheduledCleanup.enabled false If this is set to true, metrics data stored in the database is cleared periodically based on scheduled time interval. scheduledCleanup.daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. scheduledCleanup.scheduledCleanupPeriod 86400 The parameter specifies the time interval in seconds at which metric data should be cleaned. Converting Jars to OSGi Bundles \u00b6 To convert jar files to OSGi bundles, first download and save the non-OSGi jar it in a preferred directory in your machine. Then from the CLI, navigate to the <SIDDHI_RUNNER_HOME>/bin directory, and issue the following command. ./jartobundle.sh <path to non OSGi jar> ../lib This converts the Jar to OSGi bundles and place it in <SIDDHI_RUNNER_HOME>/lib directory.","title":"Configuration Guide"},{"location":"docs/config-guide/#siddhi-52-config-guide","text":"This section covers the following. Configuring Databases Configuring Periodic State Persistence Configuring Siddhi Sources, Sinks, Stores and Extensions Configuring Authentication Adding Extensions and Third Party Dependencies Configuring Statistics Converting Jars to OSGi Bundles","title":"Siddhi 5.2 Config Guide"},{"location":"docs/config-guide/#configuring-databases","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. It is recommended to configure RDBMS databases as datasources under wso2.datasources section of Siddhi configuration yaml, and pass it during startup, this will allow database to reuse connections across multiple Siddhi Apps. By default Siddhi stores product-specific data in predefined embedded H2 database located in <SIDDHI_RUNNER_HOME>/wso2/runner/database directory. Here, the default H2 database is only suitable for development, testing, and some production environments which do not store data. However, for most production environments we recommend using industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, or MSSQL. In this case users are expected to add the relevant database drivers to Siddhi's class-path. Including database drivers. The database driver corresponding to the database should be an OSGi bundle and it need to be added to <SIDDHI_RUNNER_HOME>/lib/ directory. If the driver is a jar then this should be converted to an OSGi bundle before adding . Converting Non OSGi drivers. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. The necessary table schemas are self generated by the features themselves, other than the tables needed for statistics reporting via databases . Below are the sample datasource configuration for each supported database types: MySQL Oracle There are two ways to configure Oracle. If you have a System Identifier (SID), use this (older) format: jdbc:oracle:thin:@[HOST][:PORT]:SID If you have an Oracle service name, use this (newer) format: jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE PostgreSQL MSSQL","title":"Configuring Databases"},{"location":"docs/config-guide/#configuring-periodic-state-persistence","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. This explains how to periodically persisting the state of Siddhi either into a database system or file system, in order to prevent data losses that can result from a system failure.","title":"Configuring Periodic State Persistence"},{"location":"docs/config-guide/#persistence-on-database","text":"To perform periodic state persistence on a database, the database should be configured as a datasource and the relevant jdbc drivers should be added to Siddhi's class-path. Refer Database Configuration section for more information. To configure database based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.DBPersistenceStore config > datasource The datasource to be used in persisting the state. The datasource should be defined in the Siddhi configuration yaml. For detailed instructions of how to configure a datasource, see Database Configuration . SIDDHI_PERSISTENCE_DB (A datasource that is defined in wso2.datasources in Siddhi configuration yaml) config > table The table that should be created and used for persisting states. PERSISTENCE_TABLE The following is a sample configuration for database based state persistence.","title":"Persistence on Database"},{"location":"docs/config-guide/#persistence-on-file-system","text":"To configure file system based periodic data persistence, add state.persistence section with the following properties on the Siddhi configuration yaml, and pass that during startup. Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. Here when a new persistence takes place, the older revisions are removed. 3 persistenceStore The persistence store io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config > location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample configuration for file system based state persistence.","title":"Persistence on File System"},{"location":"docs/config-guide/#configuring-siddhi-elements","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. You can configure some of there environment specific configurations in the Siddhi Configuration yaml rather than configuring in-line, such that your Siddhi Application can become potable between environments.","title":"Configuring Siddhi Elements"},{"location":"docs/config-guide/#configuring-sources-sinks-and-stores","text":"Multiple sources, sinks, and stores could be defined in Siddhi Configuration yaml as ref , and referred by several Siddhi Applications as described below. The following is the syntax for the configuration. siddhi: refs: - ref: name: '<name>' type: '<type>' properties: <property1>: <value1> <property2>: <value2> For each separate refs you want to configure, add a sub-section named ref under the refs subsection. The ref configured in Siddhi Configuration yaml can be referred from a Siddhi Application Source as follows. @Source(ref='<name>', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); Similarly Sinks and Store Tables can also be configured and referred from Siddhi Apps. For each separate refs you want to configure, add a sub-section named ref under the refs subsection. Example : Configuring http source using ref Following configuration defines the url and details about basic.auth , in the Siddhi Configuration yaml. siddhi: refs: - ref: name: 'http-passthrough' type: 'http' properties: receiver.url: 'http://0.0.0.0:8008/sweet-production' basic.auth.enabled: false This can be referred in the Siddhi Applications as follows. @Source(ref='http-passthrough', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double);","title":"Configuring Sources, Sinks and Stores"},{"location":"docs/config-guide/#configuring-extensions","text":"Siddhi extensions cater use-case specific logic that is not available by default in Siddhi. Some of these extensions have system parameter configurations to define/modify their behavior. These extensions usually have default values for the parameters, but when needed, they can be overridden by configuring the parameters in Siddhi Configuration yaml and passing it at startup. The following is the syntax for the configuration. siddhi: extensions: - extension: name: <extension name> namespace: <extension namespace> properties: <key>: <value> For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. Following are some examples on overriding default system properties via Siddhi Configuration yaml Example 1 : Defining service host and port for the TCP source siddhi: extensions: - extension: name: tcp namespace: source properties: host: 0.0.0.0 port: 5511 Example 2 : Overwriting the default RDBMS extension configuration siddhi: extensions: - extension: name: rdbms namespace: store properties: mysql.batchEnable: true mysql.batchSize: 1000 mysql.indexCreateQuery: \"CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}})\" mysql.recordDeleteQuery: \"DELETE FROM {{TABLE_NAME}} {{CONDITION}}\" mysql.recordExistsQuery: \"SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1\"","title":"Configuring Extensions"},{"location":"docs/config-guide/#configuring-authentication","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. By default, Siddhi is configured with user name admin , and password admin . This can be updated by adding related user management configuration as auth.configs to the Siddhi Configuration yaml, and pass it at startup. A sample auth.configs is as follows. # Authentication configuration auth.configs: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin","title":"Configuring Authentication"},{"location":"docs/config-guide/#adding-extensions-and-third-party-dependencies","text":"Applicable for all modes. For certain use-cases, Siddhi might require extensions and/or third party dependencies to fulfill some characteristics that it does not provide by default. This section provides details on how to add or update extension and/or third party dependencies that is needed by Siddhi.","title":"Adding Extensions and Third Party Dependencies"},{"location":"docs/config-guide/#adding-to-siddhi-java-program","text":"When running Siddhi as a Java library, the extension jars and/or third-party dependencies needed for Siddhi can be simply added to Siddhi class-path. When Maven is used as the build tool add them to the pom.xml file along with the other mandatory jars needed by Siddhi as given is Using Siddhi as a library guide. A sample on adding siddhi-io-http extension to the Maven pom.xml is as follows. <!--HTTP extension--> <dependency> <groupId>org.wso2.extension.siddhi.io.http</groupId> <artifactId>siddhi-io-http</artifactId> <version>${siddhi.io.http.version}</version> </dependency> Refer guide for more details on using Siddhi as a Java Library.","title":"Adding to Siddhi Java Program"},{"location":"docs/config-guide/#adding-to-siddhi-local-microservice","text":"The most used Siddhi extensions are packed by default with the Siddhi Local Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, adding or replacing the relevant OSGi JARs in <SIDDHI_RUNNER_HOME>/lib directory. Since Local Microservice is OSGi-based, when adding libraries/drivers they need to be checked if they are OSGi bundles, and if not convert to OSGi before adding them to the <SIDDHI_RUNNER_HOME>/lib directory. Converting Jars to OSGi Bundles.. If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Local Microservice.","title":"Adding to Siddhi Local Microservice"},{"location":"docs/config-guide/#adding-to-siddhi-docker-microservice","text":"The most used Siddhi extensions are packed by default with the Siddhi Docker Microservice distribution. To add or update Siddhi extensions and/or third-party dependencies, a new docker image has to be built from either siddhi-runner-base-ubuntu or siddhi-runner-base-alpine images. These images contain Linux OS, JDK and the Siddhi distribution. Sample docker file using siddhi-runner-base-alpine is as follows. Find the necessary artifacts to build the docker from docker-siddhi repository. The necessary OSGi jars and extensions that need to be added to the Siddhi Docker Microservice should be placed at ${BUNDLE_JAR_DIR} ( ./files/lib ) folder as defined in the above docker file, such that they would be bundled during the docker build phase. Converting Jars to OSGi Bundles If the database driver is not an OSGi bundle, then it should be converted to OSGi. Please refer Converting Jars to OSGi Bundles documentation for details. Refer guide for more details on using Siddhi as Docker Microservice.","title":"Adding to Siddhi Docker Microservice"},{"location":"docs/config-guide/#adding-to-siddhi-kubernetes-microservice","text":"To add or update Siddhi extensions and/or third-party dependencies, a custom docker image has to be created using the steps described in Adding to Siddhi Docker Microservice documentation including the necessary extensions and dependencies. The created image can be then referenced in the sepc.pod subsection in the SiddhiProcess Kubernetes artifact created to deploy Siddhi in Kubernetes. For details on creating the Kubernetes artifacts refer Using Siddhi as Kubernetes Microservice documentation.","title":"Adding to Siddhi Kubernetes Microservice"},{"location":"docs/config-guide/#configuring-statistics","text":"Applicable only for Local, Docker, and Kubernetes modes. This section is not applicable for Java and Python modes. Siddhi uses dropwizard metrics library to calculate Siddhi and JVM statistics, and it can report the results via JMX Mbeans, console or database. To enable statistics, the relevant configuration should be added to the Siddhi Configuration yaml as follows, and at the same time the statistics collection should be enabled in the Siddhi Application which is being monitored. Refer Siddhi Application Statistics documentation for enabling Siddhi Application level statistics. To enable statistics the relevant matrics related configurations should be added under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. Configuring Metrics reporting level. To modify the statistics reporting, relevant metric names can be added under the wso2.metrics.levels subsection in the Siddhi Configurations yaml, along with the matrics level (i.e., OFF, INFO, DEBUG, TRACE, or ALL) as given bellow. wso2.metrics: # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO The available metrics reporting options are as follows.","title":"Configuring Statistics"},{"location":"docs/config-guide/#reporting-via-jmx-mbeans","text":"JMX Mbeams is the default statistics reporting option of Siddhi. To enable stats with the default configuration add the metric-related properties under wso2.metrics section in the Siddhi Configurations yaml file, and pass that during startup. A sample configuration is as follows. wso2.metrics: enabled: true This will report JMX Mbeans in the name of org.wso2.carbon.metrics . However, in this default configuration the JVM metrics will not be measured. A detail JMX configuration along with the metrics reporting level is as follows. wso2.metrics: # Enable Metrics enabled: true jmx: # Register MBean when initializing Metrics registerMBean: true # MBean Name name: org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO","title":"Reporting via JMX Mbeans"},{"location":"docs/config-guide/#reporting-via-console","text":"To enable statistics by periodically printing the metrics on console add the following configuration to the the Siddhi Configurations yaml file, and pass that during startup. # This is the main configuration for metrics wso2.metrics: # Enable Metrics enabled: false reporting: console: - # The name for the Console Reporter name: Console # Enable Console Reporter enabled: false # Polling Period in seconds. # This is the period for polling metrics from the metric registry and printing in the console pollingPeriod: 5","title":"Reporting via Console"},{"location":"docs/config-guide/#reporting-via-database","text":"To enable JDBC reporting and to periodically clean up the outdated statistics from the database, first a datasource should be created with the relevant database configurations and then the related metrics properties as given bellow should be added to in the Siddhi Configurations yaml file, and pass that during startup. The bellow sample is referring to the datasource with JNDI name jdbc/SiddhiMetricsDB , hence the datasource configuration in yaml should have jndiConfig.name as jdbc/SiddhiMetricsDB . For detailed instructions on configuring a datasource, refer Configuring Databases . . The scripts to create these tables are provided in the <SIDDHI_RUNNER_HOME>/wso2/runner/dbscripts directory. Sample configuration of reporting via database. wso2.metrics: # Enable Metrics enabled: true jdbc: # Data Source Configurations for JDBC Reporters dataSource: # Default Data Source Configuration - &JDBC01 # JNDI name of the data source to be used by the JDBC Reporter. # This data source should be defined under wso2.datasources. dataSourceName: java:comp/env/jdbc/SiddhiMetricsDB # Schedule regular deletion of metrics data older than a set number of days. # It is recommended that you enable this job to ensure your metrics tables do not get extremely large. # Deleting data older than seven days should be sufficient. scheduledCleanup: # Enable scheduled cleanup to delete Metrics data in the database. enabled: false # The scheduled job will cleanup all data older than the specified days daysToKeep: 7 # This is the period for each cleanup operation in seconds. scheduledCleanupPeriod: 86400 reporting: jdbc: - # The name for the JDBC Reporter name: JDBC # Enable JDBC Reporter enabled: true # Source of Metrics, which will be used to identify each metric in database --> # Commented to use the hostname by default # source: Siddhi # Alias referring to the Data Source configuration dataSource: *JDBC01 # Polling Period in seconds. # This is the period for polling metrics from the metric registry and updating the database with the values pollingPeriod: 60 Metrics history and reporting interval If the wso2.metrics.reporting.jdbc subsection is not enabled, the information relating to metrics history will not be persisted for future references. Also note the that the reporting will only start to update the database after the given pollingPeriod time has elapsed. Information about the parameters configured under the jdbc.dataSource subsection in the Siddhi Configuration yaml is as follows. Parameter Default Value Description dataSourceName java:comp/env/jdbc/SiddhiMetricsDB java:comp/env/<datasource JNDI name> . The JNDI name of the datasource used to store metric data. scheduledCleanup.enabled false If this is set to true, metrics data stored in the database is cleared periodically based on scheduled time interval. scheduledCleanup.daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. scheduledCleanup.scheduledCleanupPeriod 86400 The parameter specifies the time interval in seconds at which metric data should be cleaned.","title":"Reporting via Database"},{"location":"docs/config-guide/#converting-jars-to-osgi-bundles","text":"To convert jar files to OSGi bundles, first download and save the non-OSGi jar it in a preferred directory in your machine. Then from the CLI, navigate to the <SIDDHI_RUNNER_HOME>/bin directory, and issue the following command. ./jartobundle.sh <path to non OSGi jar> ../lib This converts the Jar to OSGi bundles and place it in <SIDDHI_RUNNER_HOME>/lib directory.","title":"Converting Jars to OSGi Bundles"},{"location":"docs/extensions/","text":"Siddhi Extensions \u00b6 Following are some supported Siddhi extensions; Extensions released under Apache 2.0 License \u00b6 Execution Extensions \u00b6 Name Description Latest Tested Version execution-string Provides basic string handling capabilities such as concat, length, replace all, etc. 5.0.3 execution-regex Provides basic RegEx execution capabilities. 5.0.3 execution-math Provides useful mathematical functions. 5.0.1 execution-time Provides time related functionality such as getting current time, current date, manipulating/formatting dates, etc. 5.0.2 execution-map Provides the capability to generate and manipulate map data objects. 5.0.2 execution-json Provides the capability to retrieve, insert, and modify JSON elements. 2.0.1 execution-unitconversion Converts various units such as length, mass, time and volume. 2.0.1 execution-reorder Orders out-of-order event arrivals using algorithms such as K-Slack and alpha K-Stack. 5.0.0 execution-unique Retains and process unique events based on the given parameters. 5.0.0 execution-streamingml Performs streaming machine learning (clustering, classification and regression) on event streams. 2.0.1 execution-tensorflow provides support for running pre-built TensorFlow models. 2.0.1 Input/Output Extensions \u00b6 Name Description Latest Tested Version io-http Receives and publishes events via http and https transports, calls external services, and serves incoming requests and provide synchronous responses. 2.0.6 io-nats Receives and publishes events from/to NATS. 2.0.3 io-kafka Receives and publishes events from/to Kafka. 5.0.1 io-email Receives and publishes events via email using smtp , pop3 and imap protocols. 2.0.2 io-cdc Captures change data from databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 2.0.1 io-tcp Receives and publishes events through TCP transport. 3.0.2 io-googlepubsub Receives and publishes events through Google Pub/Sub. 2.0.1 io-file Receives and publishes event data from/to files. 2.0.1 io-jms Receives and publishes events via Java Message Service (JMS), supporting Message brokers such as ActiveMQ 2.0.2 io-prometheus Consumes and expose Prometheus metrics from/to Prometheus server. 2.0.1 Data Mapping Extensions \u00b6 Name Description Latest Tested Version map-json Converts JSON messages to/from Siddhi events. 5.0.3 map-xml Converts XML messages to/from Siddhi events. 5.0.3 map-text Converts text messages to/from Siddhi events. 2.0.2 map-avro Converts AVRO messages to/from Siddhi events. 2.0.1 map-keyvalue Converts events having Key-Value maps to/from Siddhi events. 2.0.2 map-csv Converts messages with CSV format to/from Siddhi events. 2.0.2 map-binary Converts binary events that adheres to Siddhi format to/from Siddhi events. 2.0.2 Store Extensions \u00b6 Name Description Latest Tested Version store-rdbms Persist and retrieve events to/from RDBMS databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 6.0.1 store-mongodb Persist and retrieve events to/from MongoDB. 2.0.1 store-redis Persist and retrieve events to/from Redis. 3.1.0 store-elasticsearch Persist and retrieve events to/from Elasticsearch. 3.0.0 Script Extensions \u00b6 Name Description Latest Tested Version script-js Allows writing user defined JavaScript functions within Siddhi Applications to process events. 5.0.1","title":"Extensions"},{"location":"docs/extensions/#siddhi-extensions","text":"Following are some supported Siddhi extensions;","title":"Siddhi Extensions"},{"location":"docs/extensions/#extensions-released-under-apache-20-license","text":"","title":"Extensions released under Apache 2.0 License"},{"location":"docs/extensions/#execution-extensions","text":"Name Description Latest Tested Version execution-string Provides basic string handling capabilities such as concat, length, replace all, etc. 5.0.3 execution-regex Provides basic RegEx execution capabilities. 5.0.3 execution-math Provides useful mathematical functions. 5.0.1 execution-time Provides time related functionality such as getting current time, current date, manipulating/formatting dates, etc. 5.0.2 execution-map Provides the capability to generate and manipulate map data objects. 5.0.2 execution-json Provides the capability to retrieve, insert, and modify JSON elements. 2.0.1 execution-unitconversion Converts various units such as length, mass, time and volume. 2.0.1 execution-reorder Orders out-of-order event arrivals using algorithms such as K-Slack and alpha K-Stack. 5.0.0 execution-unique Retains and process unique events based on the given parameters. 5.0.0 execution-streamingml Performs streaming machine learning (clustering, classification and regression) on event streams. 2.0.1 execution-tensorflow provides support for running pre-built TensorFlow models. 2.0.1","title":"Execution Extensions"},{"location":"docs/extensions/#inputoutput-extensions","text":"Name Description Latest Tested Version io-http Receives and publishes events via http and https transports, calls external services, and serves incoming requests and provide synchronous responses. 2.0.6 io-nats Receives and publishes events from/to NATS. 2.0.3 io-kafka Receives and publishes events from/to Kafka. 5.0.1 io-email Receives and publishes events via email using smtp , pop3 and imap protocols. 2.0.2 io-cdc Captures change data from databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 2.0.1 io-tcp Receives and publishes events through TCP transport. 3.0.2 io-googlepubsub Receives and publishes events through Google Pub/Sub. 2.0.1 io-file Receives and publishes event data from/to files. 2.0.1 io-jms Receives and publishes events via Java Message Service (JMS), supporting Message brokers such as ActiveMQ 2.0.2 io-prometheus Consumes and expose Prometheus metrics from/to Prometheus server. 2.0.1","title":"Input/Output Extensions"},{"location":"docs/extensions/#data-mapping-extensions","text":"Name Description Latest Tested Version map-json Converts JSON messages to/from Siddhi events. 5.0.3 map-xml Converts XML messages to/from Siddhi events. 5.0.3 map-text Converts text messages to/from Siddhi events. 2.0.2 map-avro Converts AVRO messages to/from Siddhi events. 2.0.1 map-keyvalue Converts events having Key-Value maps to/from Siddhi events. 2.0.2 map-csv Converts messages with CSV format to/from Siddhi events. 2.0.2 map-binary Converts binary events that adheres to Siddhi format to/from Siddhi events. 2.0.2","title":"Data Mapping Extensions"},{"location":"docs/extensions/#store-extensions","text":"Name Description Latest Tested Version store-rdbms Persist and retrieve events to/from RDBMS databases such as MySQL, MS SQL, Postgresql, H2 and Oracle. 6.0.1 store-mongodb Persist and retrieve events to/from MongoDB. 2.0.1 store-redis Persist and retrieve events to/from Redis. 3.1.0 store-elasticsearch Persist and retrieve events to/from Elasticsearch. 3.0.0","title":"Store Extensions"},{"location":"docs/extensions/#script-extensions","text":"Name Description Latest Tested Version script-js Allows writing user defined JavaScript functions within Siddhi Applications to process events. 5.0.1","title":"Script Extensions"},{"location":"docs/features/","text":"Siddhi 5.2 Features \u00b6 Retrieving Events From various data sources supporting multiple message formats Mapping Events Mapping events with various data formats to Stream for processing Mapping streams to multiple data formats for publishing Processing Streams Filter Filtering stream based on conditions Window Support for sliding and batch (tumbling) and many other type of windows Aggregation Supporting Avg , Sum , Min , Max , etc For long running aggregations and aggregation over windows Ability to perform aggregate processing with Group by and filter aggregated data with Having conditions Incremental Aggregation Support for processing and retrieving long running Aggregation Supports data processing in seconds, minutes, hours, days, months, and years granularity Table and Stores For storing events for future processing and retrieving them on demand Supporting storage in in-memory, RDBMs, Solr, mongoDb, etc Join Joining two streams, two windows based on conditions Joining stream/window with table or incremental aggregation based on conditions Supports inner joins, and left, right & full outer joins Pattern Identifies event occurrence patterns among streams over time Identify non occurrence of events Supports repetitive matches of event pattern occurrences with logical conditions and time bound Sequence processing Identifies continuous sequence of events from streams Supports zero to many, one to many, and zero to one event matching conditions Partitions Grouping queries and based on keywords or value ranges for isolated parallel processing Scripting Support writing scripts like JavaScript, Scala and R within Siddhi Queries Process Based on event time Whole execution driven by the event time Publishing Events To various data sources with various message formats Supporting load balancing and failover data publishing Error handling Support errors and exceptions through error streams Automatic backoff retries to external data stores, sources and sinks. Parallel processing Support parallel processing through asynchronous multithreading at streams Snapshot and restore Support for periodic state persistence and restore capabilities to allow state restore during failures","title":"Features"},{"location":"docs/features/#siddhi-52-features","text":"Retrieving Events From various data sources supporting multiple message formats Mapping Events Mapping events with various data formats to Stream for processing Mapping streams to multiple data formats for publishing Processing Streams Filter Filtering stream based on conditions Window Support for sliding and batch (tumbling) and many other type of windows Aggregation Supporting Avg , Sum , Min , Max , etc For long running aggregations and aggregation over windows Ability to perform aggregate processing with Group by and filter aggregated data with Having conditions Incremental Aggregation Support for processing and retrieving long running Aggregation Supports data processing in seconds, minutes, hours, days, months, and years granularity Table and Stores For storing events for future processing and retrieving them on demand Supporting storage in in-memory, RDBMs, Solr, mongoDb, etc Join Joining two streams, two windows based on conditions Joining stream/window with table or incremental aggregation based on conditions Supports inner joins, and left, right & full outer joins Pattern Identifies event occurrence patterns among streams over time Identify non occurrence of events Supports repetitive matches of event pattern occurrences with logical conditions and time bound Sequence processing Identifies continuous sequence of events from streams Supports zero to many, one to many, and zero to one event matching conditions Partitions Grouping queries and based on keywords or value ranges for isolated parallel processing Scripting Support writing scripts like JavaScript, Scala and R within Siddhi Queries Process Based on event time Whole execution driven by the event time Publishing Events To various data sources with various message formats Supporting load balancing and failover data publishing Error handling Support errors and exceptions through error streams Automatic backoff retries to external data stores, sources and sinks. Parallel processing Support parallel processing through asynchronous multithreading at streams Snapshot and restore Support for periodic state persistence and restore capabilities to allow state restore during failures","title":"Siddhi 5.2 Features"},{"location":"docs/query-guide/","text":"Siddhi 5.2 Streaming SQL Guide \u00b6 Introduction \u00b6 Siddhi Streaming SQL is designed to process streams of events. It can be used to implement streaming data integration, streaming analytics, rule based and adaptive decision making use cases. It is an evolution of Complex Event Processing (CEP) and Stream Processing systems, hence it can also be used to process stateful computations, detecting of complex event patterns, and sending notifications in real-time. Siddhi Streaming SQL uses SQL like syntax, and annotations to consume events from diverse event sources with various data formats, process then using stateful and stateless operators and send outputs to multiple endpoints according to their accepted event formats. It also supports exposing rule based and adaptive decision making as service endpoints such that external programs and systems can synchronously get decision support form Siddhi. The following sections explains how to write processing logic using Siddhi Streaming SQL. Siddhi Application \u00b6 The processing logic for your program can be written using the Streaming SQL and put together as a single file with .siddhi extension. This file is called as the Siddhi Application or the SiddhiApp . SiddhiApps are named by adding @app:name('<name>') annotation on the top of the SiddhiApp file. When the annotation is not added Siddhi assigns a random UUID as the name of the SiddhiApp. Purpose SiddhiApp provides an isolated execution environment for your processing logic that allows you to deploy and execute processing logic independent of other SiddhiApp in the system. Therefore it's always recommended to have a processing logic related to single use case in a single SiddhiApp. This will help you to group processing logic and easily manage addition and removal of various use cases. The following diagram depicts some of the key Siddhi Streaming SQL elements of Siddhi Application and how event flows through the elements. Below table provides brief description of a few key elements in the Siddhi Streaming SQL Language. Elements Description Stream A logical series of events ordered in time with a uniquely identifiable name, and a defined set of typed attributes defining its schema. Event An event is a single event object associated with a stream. All events of a stream contains a timestamp and an identical set of typed attributes based on the schema of the stream they belong to. Table A structured representation of data stored with a defined schema. Stored data can be backed by In-Memory , or external data stores such as RDBMS , MongoDB , etc. The tables can be accessed and manipulated at runtime. Named Window A structured representation of data stored with a defined schema and eviction policy. Window data is stored In-Memory and automatically cleared by the named window constrain. Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Named Aggregation A structured representation of data that's incrementally aggregated and stored with a defined schema and aggregation granularity such as seconds, minutes, hours, etc. Aggregation data is stored both In-Memory and in external data stores such as RDBMS . Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Query A logical construct that processes events in streaming manner by by consuming data from one or more streams, tables, windows and aggregations, and publishes output events into a stream, table or a window. Source A construct that consumes data from external sources (such as TCP , Kafka , HTTP , etc) with various event formats such as XML , JSON , binary , etc, convert then to Siddhi events, and passes into streams for processing. Sink A construct that consumes events arriving at a stream, maps them to a predefined data format (such as XML , JSON , binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Input Handler A mechanism to programmatically inject events into streams. Stream/Query Callback A mechanism to programmatically consume output events from streams or queries. Partition A logical container that isolates the processing of queries based on the partition keys derived from the events. Inner Stream A positionable stream that connects portioned queries with each other within the partition. Grammar SiddhiApp is a collection of Siddhi Streaming SQL elements composed together as a script. Here each Siddhi element must be separated by a semicolon ; . Hight level syntax of SiddhiApp is as follows. <siddhi app> : <app annotation> * ( <stream definition> | <table definition> | ... ) + ( <query> | <partition> ) + ; Example Siddhi Application with name Temperature-Analytics defined with a stream named TempStream and a query named 5minAvgQuery . @app:name('Temperature-Analytics') define stream TempStream (deviceID long, roomNo int, temp double); @name('5minAvgQuery') from TempStream#window.time(5 min) select roomNo, avg(temp) as avgTemp group by roomNo insert into OutputStream; Stream \u00b6 A stream is a logical series of events ordered in time. Its schema is defined via the stream definition . A stream definition contains the stream name and a set of attributes with specific types and uniquely identifiable names within the stream. All events associated to the stream will have the same schema (i.e., have the same attributes in the same order). Purpose Stream groups common types of events together with a schema. This helps in various ways such as, processing all events together in queries and performing data format transformations together when they are consumed and published via sources and sinks. Syntax The syntax for defining a new stream is as follows. define stream <stream name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ); The following parameters are used to configure a stream definition. Parameter Description stream name The name of the stream created. (It is recommended to define a stream name in PascalCase .) attribute name Uniquely identifiable name of the stream attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . To use and refer stream and attribute names that do not follow [a-zA-Z_][a-zA-Z_0-9]* format enclose them in ` . E.g. `$test(0)` . To make the stream process events in multi-threading and asynchronous way use the @Async annotation as shown in Multi-threading and Asynchronous Processing configuration section. Example define stream TempStream (deviceID long, roomNo int, temp double); The above creates a stream with name TempStream having the following attributes. deviceID of type long roomNo of type int temp of type double Source \u00b6 Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. A source configuration allows to define a mapping in order to convert each incoming event from its native data format to a Siddhi event. When customizations to such mappings are not provided, Siddhi assumes that the arriving event adheres to the predefined format based on the stream definition and the configured message mapping type. Purpose Source provides a way to consume events from external systems and convert them to be processed by the associated stream. Syntax To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as follows: @source(type='<source type>', <static.key>='<value>', <static.key>='<value>', @map(type='<map type>', <static.key>='<value>', <static.key>='<value>', @attributes( <attribute1>='<attribute mapping>', <attributeN>='<attribute mapping>') ) ) define stream <stream name> (<attribute1> <type>, <attributeN> <type>); This syntax includes the following annotations. Source The type parameter of @source annotation defines the source type that receives events. The other parameters of @source annotation depends upon the selected source type, and here some of its parameters can be optional. For detailed information about the supported parameters see the documentation of the relevant source. The following is the list of source types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to consume events from other SiddhiApps running on the same JVM. HTTP Expose an HTTP service to consume messages. Kafka Subscribe to Kafka topic to consume events. TCP Expose a TCP service to consume messages. Email Consume emails via POP3 and IMAP protocols. JMS Subscribe to JMS topic or queue to consume events. File Reads files by tailing or as a whole to extract events out of them. CDC Perform change data capture on databases. Prometheus Consume data from Prometheus agent. In-memory is the only source inbuilt in Siddhi, and all other source types are implemented as extensions. Source Mapper \u00b6 Each @source configuration can have a mapping denoted by the @map annotation that defines how to convert the incoming event format to Siddhi events. The type parameter of the @map defines the map type to be used in converting the incoming events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional. For detailed information about the parameters see the documentation of the relevant mapper. Map Attributes @attributes is an optional annotation used with @map to define custom mapping. When @attributes is not provided, each mapper assumes that the incoming events adheres to its own default message format and attempt to convert the events from that format. By adding the @attributes annotation, users can selectively extract data from the incoming message and assign them to the attributes. There are two ways to configure @attributes . Define attribute names as keys, and mapping configurations as values: @attributes( <attribute1>='<mapping>', <attributeN>='<mapping>') Define the mapping configurations in the same order as the attributes defined in stream definition: @attributes( '<mapping for attribute1>', '<mapping for attributeN>') Supported Source Mapping Types The following is the list of source mapping types supported by Siddhi: Source mapping type Description PassThrough Omits data conversion on Siddhi events. JSON Converts JSON messages to Siddhi events. XML Converts XML messages to Siddhi events. TEXT Converts plain text messages to Siddhi events. Avro Converts Avro events to Siddhi events. Binary Converts Siddhi specific binary events to Siddhi events. Key Value Converts key-value HashMaps to Siddhi events. CSV Converts CSV like delimiter separated events to Siddhi events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the consumed Siddhi events directly to the streams without any data conversion. PassThrough is the only source mapper inbuilt in Siddhi, and all other source mappers are implemented as extensions. Example 1 Receive JSON messages by exposing an HTTP service, and direct them to InputStream stream for processing. Here the HTTP service will be secured with basic authentication, receives events on all network interfaces on port 8080 and context /foo . The service expects the JSON messages to be on the default data format that's supported by the JSON mapper as follows. { \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } The configuration of the HTTP source and JSON source mapper to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json')) define stream InputStream (name string, age int, country string); Example 2 Receive JSON messages by exposing an HTTP service, and direct them to StockStream stream for processing. Here the incoming JSON , as given bellow, do not adhere to the default data format that's supported by the JSON mapper. { \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"FB\" }, \"price\":55.6 } } } The configuration of the HTTP source and the custom JSON source mapping to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); The same can also be configured by omitting the attribute names as bellow. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(\"stock.company.symbol\", \"stock.price\", \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); Sink \u00b6 Sinks consumes events from streams and publish them via multiple transports to external endpoints in various data formats. A sink configuration allows users to define a mapping to convert the Siddhi events in to the required output data format (such as JSON , TEXT , XML , etc.) and publish the events to the configured endpoints. When customizations to such mappings are not provided, Siddhi converts events to the predefined event format based on the stream definition and the configured message mapper type before publishing the events. Purpose Sink provides a way to publish Siddhi events of a stream to external systems by converting events to their supported format. Syntax To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as follows: @sink(type='<sink type>', <static.key>='<value>', <dynamic.key>='{{<value>}}', @map(type='<map type>', <static.key>='<value>', <dynamic.key>='{{<value>}}', @payload('<payload mapping>') ) ) define stream <stream name> (<attribute1> <type>, <attributeN> <type>); Dynamic Properties The sink and sink mapper properties that are categorized as dynamic have the ability to absorb attribute values dynamically from the Siddhi events of their associated streams. This can be configured by enclosing the relevant attribute names in double curly braces as {{...}} , and using it within the property values. Some valid dynamic properties values are: '{{attribute1}}' 'This is {{attribute1}}' {{attribute1}} > {{attributeN}} Here the attribute names in the double curly braces will be replaced with the values from the events before they are published. This syntax includes the following annotations. Sink The type parameter of the @sink annotation defines the sink type that publishes the events. The other parameters of the @sink annotation depends upon the selected sink type, and here some of its parameters can be optional and/or dynamic. For detailed information about the supported parameters see documentation of the relevant sink. The following is a list of sink types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to publish events to other SiddhiApps running on the same JVM. Log Logs the events appearing on the streams. HTTP Publish events to an HTTP endpoint. Kafka Publish events to Kafka topic. TCP Publish events to a TCP service. Email Send emails via SMTP protocols. JMS Publish events to JMS topics or queues. File Writes events to files. Prometheus Expose data through Prometheus agent. Distributed Sink \u00b6 Distributed Sinks publish events from a defined stream to multiple endpoints using load balancing or partitioning strategies. Any sink can be used as a distributed sink. A distributed sink configuration allows users to define a common mapping to convert and send the Siddhi events for all its destination endpoints. Purpose Distributed sink provides a way to publish Siddhi events to multiple endpoints in the configured event format. Syntax To configure distributed sink add the sink configuration to a stream definition by adding the @sink annotation and add the configuration parameters that are common of all the destination endpoints inside it, along with the common parameters also add the @distribution annotation specifying the distribution strategy (i.e. roundRobin or partitioned ) and @destination annotations providing each endpoint specific configurations. The distributed sink syntax is as follows: RoundRobin Distributed Sink Publishes events to defined destinations in a round robin manner. @sink(type='<sink type>', <common.static.key>='<value>', <common.dynamic.key>='{{<value>}}', @map(type='<map type>', <static.key>='<value>', <dynamic.key>='{{<value>}}', @payload('<payload mapping>') ) @distribution(strategy='roundRobin', @destination(<destination.specific.key>='<value>'), @destination(<destination.specific.key>='<value>'))) ) define stream <stream name> (<attribute1> <type>, <attributeN> <type>); Partitioned Distributed Sink Publishes events to defined destinations by partitioning them based on the partitioning key. @sink(type='<sink type>', <common.static.key>='<value>', <common.dynamic.key>='{{<value>}}', @map(type='<map type>', <static.key>='<value>', <dynamic.key>='{{<value>}}', @payload('<payload mapping>') ) @distribution(strategy='partitioned', partitionKey='<partition key>', @destination(<destination.specific.key>='<value>'), @destination(<destination.specific.key>='<value>'))) ) define stream <stream name> (<attribute1> <type>, <attributeN> <type>); Sink Mapper \u00b6 Each @sink configuration can have a mapping denoted by the @map annotation that defines how to convert Siddhi events to outgoing messages with the defined format. The type parameter of the @map defines the map type to be used in converting the outgoing events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional and/or dynamic. For detailed information about the parameters see the documentation of the relevant mapper. Map Payload @payload is an optional annotation used with @map to define custom mapping. When the @payload annotation is not provided, each mapper maps the outgoing events to its own default event format. The @payload annotation allow users to configure mappers to produce the output payload of their choice, and by using dynamic properties within the payload they can selectively extract and add data from the published Siddhi events. There are two ways you to configure @payload annotation. Some mappers such as XML , JSON , and Test only accept one output payload: @payload( 'This is a test message from {{user}}.') Some mappers such key-value accept series of mapping values: @payload( key1='mapping_1', 'key2'='user : {{user}}') Here, the keys of payload mapping can be defined using the dot notation as a.b.c , or using any constant string value as '$abc' . Supported Sink Mapping Types The following is a list of sink mapping types supported by Siddhi: Sink mapping type Description PassThrough Omits data conversion on outgoing Siddhi events. JSON Converts Siddhi events to JSON messages. XML Converts Siddhi events to XML messages. TEXT Converts Siddhi events to plain text messages. Avro Converts Siddhi events to Avro Events. Binary Converts Siddhi events to Siddhi specific binary events. Key Value Converts Siddhi events to key-value HashMaps. CSV Converts Siddhi events to CSV like delimiter separated events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the outgoing Siddhi events directly to the sinks without any data conversion. PassThrough is the only sink mapper inbuilt in Siddhi, and all other sink mappers are implemented as extensions. Example 1 Publishes OutputStream events by converting them to JSON messages with the default format, and by sending to an HTTP endpoint http://localhost:8005/endpoint1 , using POST method, Accept header, and basic authentication having admin is both username and password. The configuration of the HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/endpoint', method='POST', headers='Accept-Date:20/02/2017', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json')) define stream OutputStream (name string, age int, country string); This will publish a JSON message on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } } Example 2 Publishes StockStream events by converting them to user defined JSON messages, and by sending to an HTTP endpoint http://localhost:8005/stocks . The configuration of the HTTP sink and custom JSON sink mapping to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/stocks', @map(type='json', validate.json='true', enclosing.element='$.Portfolio', @payload(\"\"\"{\"StockData\":{ \"Symbol\":\"{{symbol}}\", \"Price\":{{price}} }}\"\"\"))) define stream StockStream (symbol string, price float, volume long); This will publish a single event as the JSON message on the following format: { \"Portfolio\":{ \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } } } This can also publish multiple events together as a JSON message on the following format: { \"Portfolio\":[ { \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } }, { \"StockData\":{ \"Symbol\":\"FB\", \"Price\":57.0 } } ] } Example 3 Publishes events from the OutputStream stream to multiple the HTTP endpoints using a partitioning strategy. Here the events are sent to either http://localhost:8005/endpoint1 or http://localhost:8006/endpoint2 based on the partitioning key country . It uses default JSON mapping, POST method, and used admin as both the username and the password when publishing to both the endpoints. The configuration of the distributed HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', method='POST', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json'), @distribution(strategy='partitioned', partitionKey='country', @destination(publisher.url='http://localhost:8005/endpoint1'), @destination(publisher.url='http://localhost:8006/endpoint2'))) define stream OutputStream (name string, age int, country string); This will partition the outgoing events and publish all events with the same country attribute value to the same endpoint. The JSON message published will be on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } } Error Handling \u00b6 Errors in Siddhi can be handled at the Streams and in Sinks. Error Handling at Stream When errors are thrown by Siddhi elements subscribed to the stream, the error gets propagated up to the stream that delivered the event to those Siddhi elements. By default the error is logged and dropped at the stream, but this behavior can be altered by by adding @OnError annotation to the corresponding stream definition. @OnError annotation can help users to capture the error and the associated event, and handle them gracefully by sending them to a fault stream. The @OnError annotation and the required action to be specified as bellow. @OnError(action='on error action') define stream <stream name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ); The action parameter of the @OnError annotation defines the action to be executed during failure scenarios. The following actions can be specified to @OnError annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when @OnError annotation is not defined. STREAM : Creates a fault stream and redirects the event and the error to it. The created fault stream will have all the attributes defined in the base stream to capture the error causing event, and in addition it also contains _error attribute of type object to containing the error information. The fault stream can be referred by adding ! in front of the base stream name as !<stream name> . Example Handle errors in TempStream by redirecting the errors to a fault stream. The configuration of TempStream stream and @OnError annotation is as follows. @OnError(name='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); Siddhi will infer and automatically defines the fault stream of TempStream as given bellow. define stream !TempStream (deviceID long, roomNo int, temp double, _error object); The SiddhiApp extending the above the use-case by adding failure generation and error handling with the use of queries is as follows. Note: Details on writing processing logics via queries will be explained in later sections. -- Define fault stream to handle error occurred at TempStream subscribers @OnError(name='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); -- Error generation through a custom function `createError()` @name('error-generation') from TempStream#custom:createError() insert into IgnoreStream1; -- Handling error by simply logging the event and error. @name('handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream2; Error Handling at Sink There can be cases where external systems becoming unavailable or coursing errors when the events are published to them. By default sinks log and drop the events causing event losses, and this can be handled gracefully by configuring on.error parameter of the @sink annotation. The on.error parameter of the @sink annotation can be specified as bellow. @sink(type='<sink type>', on.error='<on error action>', <key>='<value>', ...) define stream <stream name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ); The following actions can be specified to on.error parameter of @sink annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when on.error parameter is not defined on the @sink annotation. WAIT : Publishing threads wait in back-off and re-trying mode, and only send the events when the connection is re-established. During this time the threads will not consume any new messages causing the systems to introduce back pressure on the systems that publishes to it. STREAM : Pushes the failed events with the corresponding error to the associated fault stream the sink belongs to. Example 1 Introduce back pressure on the threads who bring events via TempStream when the system cannot connect to Kafka. The configuration of TempStream stream and @sink Kafka annotation with on.error property is as follows. @sink(type='kafka', on.error='WAIT', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); Example 2 Send events to the fault stream of TempStream when the system cannot connect to Kafka. The configuration of TempStream stream with associated fault stream, @sink Kafka annotation with on.error property and a queries to handle the error is as follows. Note: Details on writing processing logics via queries will be explained in later sections. @OnError(name='STREAM') @sink(type='kafka', on.error='STREAM', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); -- Handling error by simply logging the event and error. @name('handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream; Query \u00b6 Query defines the processing logic in Siddhi. It consumes events from one or more streams, named-windows , tables , and/or named-aggregations , process the events in a streaming manner, and generate output events into a stream , named-window , or table . Purpose A query provides a way to process the events in the order they arrive and produce output using both stateful and stateless complex event processing and stream processing operations. Syntax The high level query syntax for defining processing logics is as follows: @name('<query name>') from <input> <projection> <output action> The following parameters are used to configure a stream definition. Parameter Description query name The name of the query. Since naming the query (i.e the @name('<query name>') annotation) is optional, when the name is not provided Siddhi assign a system generated name for the query. input Defines the means of event consumption via streams , named-windows , tables , and/or named-aggregations , and defines the processing logic using filters , windows , stream-functions , joins , patterns and sequences . projection Generates output event attributes using select , functions , aggregation-functions , and group by operations, and filters the generated the output using having , limit & offset , order by , and output rate limiting operations before sending them out. Here the projection is optional and when it is omitted all the input events will be sent to the output as it is. output action Defines output action (such as insert into , update , delete , etc) that needs to be performed by the generated events on a stream , named-window , or table Example A query consumes events from the TempStream stream and output only the roomNo and temp attributes to the RoomTempStream stream, from which another query consumes the events and sends all its attributes to AnotherRoomTempStream stream. define stream TempStream (deviceID long, roomNo int, temp double); from TempStream select roomNo, temp insert into RoomTempStream; from RoomTempStream insert into AnotherRoomTempStream; Inferred Stream Here, the RoomTempStream and AnotherRoomTempStream streams are an inferred streams, which means their stream definitions are inferred from the queries and they can be used same as any other defined stream without any restrictions. Value \u00b6 Values are typed data, that can be manipulated, transferred and stored. Values can be referred by the attributes defined in definitions such as streams, and tables. Siddhi supports values of type STRING , INT (Integer), LONG , DOUBLE , FLOAT , BOOL (Boolean) and OBJECT . The syntax of each type and their example use as a constant value is as follows, Attribute Type Format Example int <digit>+ 123 , -75 , +95 long <digit>+L 123000L , -750l , +154L float (<digit>+)?('.'<digit>*)? (E(-|+)?<digit>+)?F 123.0f , -75.0e-10F , +95.789f double (<digit>+)?('.'<digit>*)? (E(-|+)?<digit>+)?D? 123.0 , 123.0D , -75.0e-10D , +95.789d bool (true|false) true , false , TRUE , FALSE string '(<char>* !('|\"|\"\"\"|<new line>))' or \"(<char>* !(\"|\"\"\"|<new line>))\" or \"\"\"(<char>* !(\"\"\"))\"\"\" 'Any text.' , \"Text with 'single' quotes.\" , \"\"\" Text with 'single' quotes, \"double\" quotes, and new lines. \"\"\" Time Time is a special type of LONG value that denotes time using digits and their unit in the format (<digit>+ <unit>)+ . At execution, the time gets converted into milliseconds and returns a LONG value. Unit Syntax Year year | years Month month | months Week week | weeks Day day | days Hour hour | hours Minutes minute | minutes | min Seconds second | seconds | sec Milliseconds millisecond | milliseconds Example 1 hour and 25 minutes can by written as 1 hour and 25 minutes which is equal to the LONG value 5100000 . Select \u00b6 The select clause in Siddhi query defines the output event attributes of the query. Following are some basic query projection operations supported by select. Action Description Select specific attributes for projection Only select some of the input attributes as query output attributes. E.g., Select and output only roomNo and temp attributes from the TempStream stream. from TempStream select roomNo, temp insert into RoomTempStream; Select all attributes for projection Select all input attributes as query output attributes. This can be done by using asterisk ( * ) or by omitting the select clause itself. E.g., Both following queries select all attributes of TempStream input stream and output all attributes to NewTempStream stream. from TempStream select * insert into NewTempStream; or from TempStream insert into NewTempStream; Name attribute Provide a unique name for each output attribute generated by the query. This can help to rename the selected input attributes or assign an attribute name to a projection operation such as function, aggregate-function, mathematical operation, etc, using as keyword. E.g., Query that renames input attribute temp to temperature and function currentTimeMillis() as time . from TempStream select roomNo, temp as temperature, currentTimeMillis() as time insert into RoomTempStream; Constant values as attributes Creates output attributes with a constant value. Any constant value of type STRING , INT , LONG , DOUBLE , FLOAT , BOOL , and time as given in the values section can be defined. E.g., Query specifying 'C' as the constant value for the scale attribute. from TempStream select roomNo, temp, 'C' as scale insert into RoomTempStream; Mathematical and logical expressions in attributes Defines the mathematical and logical operations that need to be performed to generating output attribute values. These expressions are executed in the precedence order given below. Operator precedence Operator Distribution Example () Scope (cost + tax) * 0.05 IS NULL Null check deviceID is null NOT Logical NOT not (price > 10) * , / , % Multiplication, division, modulus temp * 9/5 + 32 + , - Addition, subtraction temp * 9/5 - 32 < , < = , > , >= Comparators: less-than, greater-than-equal, greater-than, less-than-equal totalCost >= price * quantity == , != Comparisons: equal, not equal totalCost != price * quantity IN Checks if value exist in the table roomNo in ServerRoomsTable AND Logical AND temp < 40 and humidity < 40 OR Logical OR humidity < 40 or humidity >= 60 E.g., Query converts temperature from Celsius to Fahrenheit, and identifies rooms with room number between 10 and 15 as server rooms. from TempStream select roomNo, temp * 9/5 + 32 as temp, 'F' as scale, roomNo > 10 and roomNo < 15 as isServerRoom insert into RoomTempStream; Function \u00b6 Function are pre-configured operations that can consumes zero, or more parameters and always produce a single value as result. It can be used anywhere an attribute can be used. Purpose Functions encapsulate pre-configured reusable execution logic allowing users to execute the logic anywhere just by calling the function. This also make writing SiddhiApps simple and easy to understand. Syntax The syntax of function is as follows, <function name>( <parameter>* ) Here <function name> uniquely identifies the function. The <parameter> defined input parameters the function can accept. The input parameters can be attributes, constant values, results of other functions, results of mathematical or logical expressions, or time values. The number and type of parameters a function accepts depend on the function itself. Note Functions, mathematical expressions, and logical expressions can be used in a nested manner. Example 1 Function name add accepting two input parameters, is called with an attribute named input and a constant value 75 . add(input, 75) Example 2 Function name alertAfter accepting two input parameters, is called with a time value of 1 hour and 25 minutes and a mathematical addition operation of startTime + 56 . add(1 hour and 25 minutes, startTime + 56) Inbuilt functions Following are some inbuilt Siddhi functions, for more functions refer execution extensions . Inbuilt function Description eventTimestamp Returns event's timestamp. currentTimeMillis Returns current time of SiddhiApp runtime. default Returns a default value if the parameter is null. ifThenElse Returns parameters based on a conditional parameter. UUID Generates a UUID. cast Casts parameter type. convert Converts parameter type. coalesce Returns first not null input parameter. maximum Returns the maximum value of all parameters. minimum Returns the minimum value of all parameters. instanceOfBoolean Checks if the parameter is an instance of Boolean. instanceOfDouble Checks if the parameter is an instance of Double. instanceOfFloat Checks if the parameter is an instance of Float. instanceOfInteger Checks if the parameter is an instance of Integer. instanceOfLong Checks if the parameter is an instance of Long. instanceOfString Checks if the parameter is an instance of String. createSet Creates HashSet with given input parameters. sizeOfSet Returns number of items in the HashSet, that's passed as a parameter. Example Query that converts the roomNo to string using convert function, finds the maximum temperature reading with maximum function, and adds a unique messageID using the UUID function. from TempStream select convert(roomNo, 'string') as roomNo, maximum(tempReading1, tempReading2) as temp, UUID() as messageID insert into RoomTempStream; Filter \u00b6 Filters provide a way of filtering input stream events based on a specified condition. It accepts any type of condition including a combination of functions and/or attributes that produces a Boolean result. Filters allow events to passthrough if the condition results in true , and drops if it results in a false . Purpose Filter helps to select the events that are relevant for the processing and omit the ones that are not. Syntax Filter conditions should be defined in square brackets ( [] ) next to the input stream as shown below. from <input stream>[<filter condition>] select <attribute name>, <attribute name>, ... insert into <output stream> Example Query to filter TempStream stream events, having roomNo within the range of 100-210 and temperature greater than 40 degrees, and insert them into HighTempStream stream. from TempStream[(roomNo >= 100 and roomNo < 210) and temp > 40] select roomNo, temp insert into HighTempStream; Window \u00b6 Window provides a way to capture a subset of events from an input stream and retain them for a period of time based on a specified criterion. The criterion defines when and how the events should be evicted from the windows. Such as events getting evicted from the window based on the time duration, or number of events and they events are evicted in a sliding (one by one) or tumbling (batch) manner. Within a query, each input stream can at most have only one window associated with it. Purpose Windows help to retain events based on a criterion, such that the values of those events can be aggregated, or checked if an event of interest is within the window or not. Syntax Window should be defined by using the #window prefix next to the input stream as shown below. from <input stream>#window.<window name>(<parameter>, <parameter>, ... ) select <attribute name>, <attribute name>, ... insert <ouput event type>? into <output stream> Note Filter conditions can be applied both before and/or after the window. Inbuilt windows Following are some inbuilt Siddhi windows, for more windows refer execution extensions . Inbuilt function Description time Retains events based on time in a sliding manner. timeBatch Retains events based on time in a tumbling/batch manner. length Retains events based on number of events in a sliding manner. lengthBatch Retains events based on number of events in a tumbling/batch manner. timeLength Retains events based on time and number of events in a sliding manner. session Retains events for each session based on session key. batch Retains events of last arrived event chunk. sort Retains top-k or bottom-k events based on a parameter value. cron Retains events based on cron time in a tumbling/batch manner. externalTime Retains events based on event time value passed as a parameter in a sliding manner. externalTimeBatch Retains events based on event time value passed as a parameter in a a tumbling/batch manner. delay Retains events and delays the output by the given time period in a sliding manner. Example 1 Query to find out the maximum temperature out of the last 10 events , using the window of length 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.length(10) select max(temp) as maxTemp insert into MaxTempStream; Here, the length window operates in a sliding manner where the following 3 event subsets are calculated and outputted when a list of 12 events are received in sequential order. Subset Event Range 1 1 - 10 2 2 - 11 3 3 - 12 Example 2 Query to find out the maximum temperature out of the every 10 events , using the window of lengthBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.lengthBatch(10) select max(temp) as maxTemp insert into MaxTempStream; Here, the window operates in a batch/tumbling manner where the following 3 event subsets are calculated and outputted when a list of 30 events are received in a sequential order. Subset Event Range 1 1 - 10 2 11 - 20 3 21 - 30 Example 3 Query to find out the maximum temperature out of the events arrived during last 10 minutes , using the window of time 10 minutes and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.time(10 min) select max(temp) as maxTemp insert into MaxTempStream; Here, the time window operates in a sliding manner with millisecond accuracy, where it will process events in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:00:01.001 - 1:10:01.000 3 1:00:01.033 - 1:10:01.034 Example 4 Query to find out the maximum temperature out of the events arriving every 10 minutes , using the window of timeBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.timeBatch(10 min) select max(temp) as maxTemp insert into MaxTempStream; Here, the window operates in a batch/tumbling manner where the window will process evetns in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:10:00.001 - 1:20:00.000 3 1:20:00.001 - 1:30:00.000 Event Type \u00b6 Query output depends on the current and expired event types it produces based on its internal processing state. By default all queries produce current events upon event arrival to the query. The queries containing windows additionally produce expired events when events expire from the windows. Purpose Event type helps to specify when a query should output events to the stream, such as output upon current events, expired events or upon both current and expired events. Syntax Event type should be defined in between insert and into keywords for insert queries as follows. from <input stream>#window.<window name>(<parameter>, <parameter>, ... ) select <attribute name>, <attribute name>, ... insert <event type> into <output stream> Event type should be defined next to the for keyword for delete queries as follows. from <input stream>#window.<window name>(<parameter>, <parameter>, ... ) select <attribute name>, <attribute name>, ... delete <table> (for <event type>)? on <condition> Event type should be defined next to the for keyword for update queries as follows. from <input stream>#window.<window name>(<parameter>, <parameter>, ... ) select <attribute name>, <attribute name>, ... update <table> (for <event type>)? set <table>.<attribute name> = (<attribute name>|<expression>)?, <table>.<attribute name> = (<attribute name>|<expression>)?, ... on <condition> Event type should be defined next to the for keyword for update or insert queries as follows. from <input stream>#window.<window name>(<parameter>, <parameter>, ... ) select <attribute name>, <attribute name>, ... update or insert into <table> (for <event type>)? set <table>.<attribute name> = <expression>, <table>.<attribute name> = <expression>, ... on <condition> Note Controlling query output based on the event types neither alters query execution nor its accuracy. The event types can be defined using the following keywords to manipulate query output. Event types Description current events Outputs events only when incoming events arrive to be processed by the query. This is default behavior when no specific event type is specified. expired events Outputs events only when events expires from the window. all events Outputs events when incoming events arrive to be processed by the query as well as when events expire from the window. Example Query to output only the expired events from a 1 minute time window to the DelayedTempStream stream. This can be used for delaying the events by a minute. from TempStream#window.time(1 min) select * insert expired events into DelayedTempStream Note This is just to illustrate how expired events work, it is recommended to use delay window for usecases where we need to delay events by a given time period. Aggregate Function \u00b6 Aggregate functions are pre-configured aggregation operations that can consumes zero, or more parameters from multiple events and always produce a single value as result. They can be only used in the query projection (as part of the select clause). When a query comprises a window, the aggregation will be contained to the events in the window, and when it does not have a window, the aggregation is performed from the first event the query has received. Purpose Aggregate functions encapsulate pre-configured reusable aggregate logic allowing users to aggregate values of multiple events together. When used with batch/tumbling windows this can also help to reduce the number of output events produced. Syntax Aggregate function can be used in query projection (as part of the select clause) alone or as a part of another expression. In all cases, the output produced by the query should be properly mapped to the output stream attribute using the as keyword. The syntax of aggregate function is as follows, from <input stream>#window.<window name>(<parameter>, <parameter>, ... ) select <aggregate function>(<parameter>, <parameter>, ... ) as <attribute name>, <attribute2 name>, ... insert into <output stream>; Here <aggregate function> uniquely identifies the aggregate function. The <parameter> defined input parameters the aggregate function can accept. The input parameters can be attributes, constant values, results of other functions or aggregate functions, results of mathematical or logical expressions, or time values. The number and type of parameters an aggregate function accepts depend on the function itself. Inbuilt aggregate functions Following are some inbuilt aggregation functions, for more functions refer execution extensions . Inbuilt aggregate function Description sum Calculates the sum from a set of values. count Calculates the count from a set of values. distinctCount Calculates the distinct count based on a parameter from a set of values. avg Calculates the average from a set of values. max Finds the maximum value from a set of values. max Finds the minimum value from a set of values. | maxForever | Finds the maximum value from all events throughout its lifetime irrespective of the windows. | | minForever | Finds the minimum value from all events throughout its lifetime irrespective of the windows. | | stdDev | Calculates the standard deviation from a set of values. | | and | Calculates boolean and from a set of values. | | or | Calculates boolean or from a set of values. | | unionSet | Calculates union as a Set from a set of values. | Example Query to calculate average, maximum, and minimum values on temp attribute of the TempStream stream in a sliding manner, from the events arrived over the last 10 minutes and to produce outputs avgTemp , maxTemp and minTemp respectively to the AvgTempStream output stream. from TempStream#window.time(10 min) select avg(temp) as avgTemp, max(temp) as maxTemp, min(temp) as minTemp insert into AvgTempStream; Group By \u00b6 Group By provides a way of grouping events based on one or more specified attributes to perform aggregate operations. Purpose Group By allows users to aggregate values of multiple events based on the given group-by fields. Syntax The syntax for the Group By with aggregate function is as follows. from <input stream>#window.<window name>(...) select <aggregate function>( <parameter>, <parameter>, ...) as <attribute1 name>, <attribute2 name>, ... group by <attribute1 name>, <attribute2 name>, ... insert into <output stream>; Here the group by attributes should be defined next to the group by keyword separating each attribute by a comma. Example Query to calculate the average temp per roomNo and deviceID combination, from the events arrived from TempStream stream, during the last 10 minutes time-window in a sliding manner. from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID insert into AvgTempStream; Having \u00b6 Having provide a way of filtering events based on a specified condition of the query output stream attributes. It accepts any type of condition including a combination of functions and/or attributes that produces a Boolean result. Having, allow events to passthrough if the condition results in true , and drops if it results in a false . Purpose Having helps to select the events that are relevant for the output based on the attributes those are produced by the select clause and omit the ones that are not. Syntax The syntax for the Having clause is as follows. from <input stream>#window.<window name>( ... ) select <aggregate function>( <parameter>, <parameter>, ...) as <attribute1 name>, <attribute2 name>, ... group by <attribute1 name>, <attribute2 name> ... having <condition> insert into <output stream>; Here the having <condition> should be defined next to the having keyword and having can be used with or without group by clause. Example Query to calculate the average temp per roomNo for the last 10 minutes, and alerts if the avgTemp exceeds 30 degrees. from TempStream#window.time(10 min) select roomNo, avg(temp) as avgTemp group by roomNo having avgTemp > 30 insert into AlertStream; Order By \u00b6 Order By, orders the query results in ascending and or descending order based on one or more specified attributes. When an attribute is used for order by, by default Siddhi orders the events in ascending order of that attribute's value, and by adding desc keyword, the events can be ordered in descending order. When more than one attribute is defined the attributes defined towards the left will have more precedence in ordering than the ones defined in right. Purpose Order By helps to sort the events in the outputs chunks produced by the query. Order By will be more helpful for batch windows, and queries where they output many of event together then for sliding window use cases where the output will be one or few events at a time. Syntax The syntax for the Order By clause is as follows: from <input stream>#window.<window name>( ... ) select <aggregate function>( <parameter>, <parameter>, ...) as <attribute1 name>, <attribute2 name>, ... group by <attribute1 name>, <attribute2 name> ... having <condition> order by <attribute1 name> (asc|desc)?, <attribute2 name> (asc|desc)?, ... insert into <output stream>; Here the order by attributes should be defined next to the order by keyword separating each by a comma, and optionally specifying the event ordering using asc (default) or desc keywords. Example Query to calculate the average temp per roomNo and deviceID combination on every 10 minutes batches, and order the generated output events in ascending order by avgTemp and then by descending order of roomNo (if the more than one event have the same avgTemp value). from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp, roomNo desc insert into AvgTempStream; Limit & Offset \u00b6 These provide a way to select the number of events (via limit) from the desired index (by specifying an offset) from the output event chunks produced by the query. Purpose Limit & Offset helps to output only the selected set of events from large event batches. This will be more useful with Order By clause where one can order the output for topK, bottomK, or even to paginate through the dataset by obtaining a set of events from the middle. Syntax The syntax for the Limit & Offset clauses is as follows: from <input stream>#window.<window name>( ... ) select <aggregate function>( <parameter>, <parameter>, ...) as <attribute1 name>, <attribute2 name>, ... group by <attribute1 name>, <attribute2 name> ... having <condition> order by <attribute1 name> (asc | desc)?, <attribute2 name> (<ascend/descend>)?, ... limit <positive integer>? offset <positive integer>? insert into <output stream>; Here both limit and offset are optional, when limit is omitted the query will output all the events, and when offset is omitted 0 is taken as the default offset value. Example 1 Query to calculate the average temp per roomNo and deviceID combination for every 10 minutes batches, from the events arriving at the TempStream stream, and emit only two events having the highest avgTemp value. from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp desc limit 2 insert into HighestAvgTempStream; Example 2 Query to calculate the average temp per roomNo and deviceID combination for every 10 minutes batches, for events that arriving at the TempStream stream, and emits only the third, forth and fifth events when sorted in descending order based on their avgTemp value. from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp desc limit 3 offset 2 insert into HighestAvgTempStream; Join (Stream) \u00b6 Joins allow you to get a combined result from two streams in real-time based on a specified condition. Purpose Streams are stateless. Therefore, in order to join two streams, they need to be connected to a window so that there is a pool of events that can be used for joining. Joins also accept conditions to join the appropriate events from each stream. During the joining process each incoming event of each stream is matched against all the events in the other stream's window based on the given condition, and the output events are generated for all the matching event pairs. Note Join can also be performed with stored data , aggregation or externally named windows . Syntax The syntax for a join is as follows: from <input stream>#window.<window name>(<parameter>, ... ) {unidirectional} {as <reference>} join <input stream>#window.<window name>(<parameter>, ... ) {unidirectional} {as <reference>} on <join condition> select <attribute name>, <attribute name>, ... insert into <output stream> Here, the <join condition> allows you to match the attributes from both the streams. Unidirectional join operation By default, events arriving at either stream can trigger the joining process. However, if you want to control the join execution, you can add the unidirectional keyword next to a stream in the join definition as depicted in the syntax in order to enable that stream to trigger the join operation. Here, events arriving at other stream only update the window of that stream, and this stream does not trigger the join operation. Note The unidirectional keyword cannot be applied to both the input streams because the default behaviour already allows both streams to trigger the join operation. Example Assuming that the temperature of regulators are updated every minute. Following is a Siddhi App that controls the temperature regulators if they are not already on for all the rooms with a room temperature greater than 30 degrees. define stream TempStream(deviceID long, roomNo int, temp double); define stream RegulatorStream(deviceID long, roomNo int, isOn bool); from TempStream[temp > 30.0]#window.time(1 min) as T join RegulatorStream[isOn == false]#window.length(1) as R on T.roomNo == R.roomNo select T.roomNo, R.deviceID, 'start' as action insert into RegulatorActionStream; Supported join types Following are the supported operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join both the streams. The output is generated only if there is a matching event in both the streams. Left outer join The left outer join operation allows you to join two streams to be merged based on a condition. left outer join is used as the keyword to join both the streams. Here, it returns all the events of left stream even if there are no matching events in the right stream by having null values for the attributes of the right stream. Example The following query generates output events for all events from the StockStream stream regardless of whether a matching symbol exists in the TwitterStream stream or not. from StockStream#window.time(1 min) as S left outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ; Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join both the streams. It returns all the events of the right stream even if there are no matching events in the left stream. Full outer join The full outer join combines the results of left outer join and right outer join. full outer join is used as the keyword to join both the streams. Here, output event are generated for each incoming event even if there are no matching events in the other stream. Example The following query generates output events for all the incoming events of each stream regardless of whether there is a match for the symbol attribute in the other stream or not. from StockStream#window.time(1 min) as S full outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ; Pattern \u00b6 This is a state machine implementation that allows you to detect patterns in the events that arrive over time. This can correlate events within a single stream or between multiple streams. Purpose Patterns allow you to identify trends in events over a time period. Syntax The following is the syntax for a pattern query: from (every)? <event reference>=<input stream>[<filter condition>] -> (every)? <event reference>=<input stream [<filter condition>] -> ... (within <time gap>)? select <event reference>.<attribute name>, <event reference>.<attribute name>, ... insert into <output stream> Items Description -> This is used to indicate an event that should be following another event. The subsequent event does not necessarily have to occur immediately after the preceding event. The condition to be met by the preceding event should be added before the sign, and the condition to be met by the subsequent event should be added after the sign. <event reference> This allows you to add a reference to the the matching event so that it can be accessed later for further processing. (within <time gap>)? The within clause is optional. It defines the time duration within which all the matching events should occur. every every is an optional keyword. This defines whether the event matching should be triggered for every event arrival in the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. Siddhi also supports pattern matching with counting events and matching events in a logical order such as ( and , or , and not ). These are described in detail further below in this guide. Example This query sends an alert if the temperature of a room increases by 5 degrees within 10 min. from every( e1=TempStream ) -> e2=TempStream[ e1.roomNo == roomNo and (e1.temp + 5) <= temp ] within 10 min select e1.roomNo, e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Here, the matching process begins for each event in the TempStream stream (because every is used with e1=TempStream ), and if another event arrives within 10 minutes with a value for the temp attribute that is greater than or equal to e1.temp + 5 of the event e1, an output is generated via the AlertStream . Counting Pattern \u00b6 Counting patterns allow you to match multiple events that may have been received for the same matching condition. The number of events matched per condition can be limited via condition postfixes. Syntax Each matching condition can contain a collection of events with the minimum and maximum number of events to be matched as shown in the syntax below. from (every)? <event reference>=<input stream>[<filter condition>] (<<min count>:<max count>>)? -> ... (within <time gap>)? select <event reference>([event index])?.<attribute name>, ... insert into <output stream> Postfix Description Example <n1:n2> This matches n1 to n2 events (including n1 and not more than n2 ). 1:4 matches 1 to 4 events. <n:> This matches n or more events (including n ). <2:> matches 2 or more events. <:n> This matches up to n events (excluding n ). <:5> matches up to 5 events. <n> This matches exactly n events. <5> matches exactly 5 events. Specific occurrences of the event in a collection can be retrieved by using an event index with its reference. Square brackets can be used to indicate the event index where 1 can be used as the index of the first event and last can be used as the index for the last available event in the event collection. If you provide an index greater then the last event index, the system returns null . The following are some valid examples. e1[3] refers to the 3 rd event. e1[last] refers to the last event. e1[last - 1] refers to the event before the last event. Example The following Siddhi App calculates the temperature difference between two regulator events. define stream TempStream (deviceID long, roomNo int, temp double); define stream RegulatorStream (deviceID long, roomNo int, tempSet double, isOn bool); from every( e1=RegulatorStream) -> e2=TempStream[e1.roomNo==roomNo]<1:> -> e3=RegulatorStream[e1.roomNo==roomNo] select e1.roomNo, e2[0].temp - e2[last].temp as tempDiff insert into TempDiffStream; Logical Patterns \u00b6 Logical patterns match events that arrive in temporal order and correlate them with logical relationships such as and , or and not . Syntax from (every)? (not)? <event reference>=<input stream>[<filter condition>] ((and|or) <event reference>=<input stream>[<filter condition>])? (within <time gap>)? -> ... select <event reference>([event index])?.<attribute name>, ... insert into <output stream> Keywords such as and , or , or not can be used to illustrate the logical relationship. Key Word Description and This allows both conditions of and to be matched by two events in any order. or The state succeeds if either condition of or is satisfied. Here the event reference of the other condition is null . not <condition1> and <condition2> When not is included with and , it identifies the events that match arriving before any event that match . not <condition> for <time period> When not is included with for , it allows you to identify a situation where no event that matches <condition1> arrives during the specified <time period> . e.g., from not TemperatureStream[temp > 60] for 5 sec . Here the not pattern can be followed by either an and clause or the effective period of not can be concluded after a given <time period> . Further in Siddhi more than two streams cannot be matched with logical conditions using and , or , or not clauses at this point. Detecting Non-occurring Events \u00b6 Siddhi allows you to detect non-occurring events via multiple combinations of the key words specified above as shown in the table below. In the patterns listed, P* can be either a regular event pattern, an absent event pattern or a logical pattern. Pattern Detected Scenario not A for <time period> The non-occurrence of event A within <time period> after system start up. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, to indicate that the passenger might be in danger. not A for <time period> and B After system start up, event A does not occur within time period , but event B occurs at some point in time. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, and the passenger marked that he/she is in danger at some point in time. not A for <time period 1> and not B for <time period 2> After system start up, event A doess not occur within time period 1 , and event B also does not occur within <time period 2> . e.g., Generating an alert if the driver of a taxi has not reached the destination within 30 minutes, and the passenger has not marked himself/herself to be in danger within that same time period. not A for <time period> or B After system start up, either event A does not occur within <time period> , or event B occurs at some point in time. e.g., Generating an alert if the taxi has not reached its destination within 30 minutes, or if the passenger has marked that he/she is in danger at some point in time. not A for <time period 1> or not B for <time period 2> After system start up, either event A does not occur within <time period 1> , or event B occurs within <time period 2> . e.g., Generating an alert to indicate that the driver is not on an expected route if the taxi has not reached destination A within 20 minutes, or reached destination B within 30 minutes. A \u2192 not B for <time period> Event B does not occur within <time period> after the occurrence of event A. e.g., Generating an alert if the taxi has reached its destination, but this was not followed by a payment record. P* \u2192 not A for <time period> and B After the occurrence of P*, event A does not occur within <time period> , and event B occurs at some point in time. P* \u2192 not A for <time period 1> and not B for <time period 2> After the occurrence of P*, event A does not occur within <time period 1> , and event B does not occur within <time period 2> . P* \u2192 not A for <time period> or B After the occurrence of P*, either event A does not occur within <time period> , or event B occurs at some point in time. P* \u2192 not A for <time period 1> or not B for <time period 2> After the occurrence of P*, either event A does not occur within <time period 1> , or event B does not occur within <time period 2> . not A for <time period> \u2192 B Event A does occur within <time period> after the system start up, but event B occurs after that <time period> has elapsed. not A for <time period> and B \u2192 P* Event A does not occur within <time period> , and event B occurs at some point in time. Then P* occurs after the <time period> has elapsed, and after B has occurred. not A for <time period 1> and not B for <time period 2> \u2192 P* After system start up, event A does not occur within <time period 1> , and event B does not occur within <time period 2> . However, P* occurs after both A and B. not A for <time period> or B \u2192 P* After system start up, event A does not occur within <time period> or event B occurs at some point in time. The P* occurs after <time period> has elapsed, or after B has occurred. not A for <time period 1> or not B for <time period 2> \u2192 P* After system start up, either event A does not occur within <time period 1> , or event B does not occur within <time period 2> . Then P* occurs after both <time period 1> and <time period 2> have elapsed. not A and B Event A does not occur before event B. A and not B Event B does not occur before event A. Example Following Siddhi App, sends the stop control action to the regulator when the key is removed from the hotel room. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream RoomKeyStream(deviceID long, roomNo int, action string); from every( e1=RegulatorStateChangeStream[ action == 'on' ] ) -> e2=RoomKeyStream[ e1.roomNo == roomNo and action == 'removed' ] or e3=RegulatorStateChangeStream[ e1.roomNo == roomNo and action == 'off'] select e1.roomNo, ifThenElse( e2 is null, 'none', 'stop' ) as action having action != 'none' insert into RegulatorActionStream; This Siddhi Application generates an alert if we have switch off the regulator before the temperature reaches 12 degrees. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from e1=RegulatorStateChangeStream[action == 'start'] -> not TempStream[e1.roomNo == roomNo and temp < 12] and e2=RegulatorStateChangeStream[action == 'off'] select e1.roomNo as roomNo insert into AlertStream; This Siddhi Application generates an alert if the temperature does not reduce to 12 degrees within 5 minutes of switching on the regulator. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from e1=RegulatorStateChangeStream[action == 'start'] -> not TempStream[e1.roomNo == roomNo and temp < 12] for '5 min' select e1.roomNo as roomNo insert into AlertStream; Sequence \u00b6 Sequence is a state machine implementation that allows you to detect the sequence of event occurrences over time. Here all matching events need to arrive consecutively to match the sequence condition, and there cannot be any non-matching events arriving within a matching sequence of events. This can correlate events within a single stream or between multiple streams. Purpose This allows you to detect a specified event sequence over a specified time period. Syntax The syntax for a sequence query is as follows: from (every)? <event reference>=<input stream>[<filter condition>], <event reference>=<input stream [<filter condition>], ... (within <time gap>)? select <event reference>.<attribute name>, <event reference>.<attribute name>, ... insert into <output stream> Items Description , This represents the immediate next event i.e., when an event that matches the first condition arrives, the event that arrives immediately after it should match the second condition. <event reference> This allows you to add a reference to the the matching event so that it can be accessed later for further processing. (within <time gap>)? The within clause is optional. It defines the time duration within which all the matching events should occur. every every is an optional keyword. This defines whether the matching event should be triggered for every event that arrives at the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. Example This query generates an alert if the increase in the temperature between two consecutive temperature events exceeds one degree. from every e1=TempStream, e2=TempStream[e1.temp + 1 < temp] select e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Counting Sequence Counting sequences allow you to match multiple events for the same matching condition. The number of events matched per condition can be limited via condition postfixes such as Counting Patterns , or by using the * , + , and ? operators. The matching events can also be retrieved using event indexes, similar to how it is done in Counting Patterns . Syntax Each matching condition in a sequence can contain a collection of events as shown below. from (every)? <event reference>=<input stream>[<filter condition>](+|*|?)?, <event reference>=<input stream [<filter condition>](+|*|?)?, ... (within <time gap>)? select <event reference>.<attribute name>, <event reference>.<attribute name>, ... insert into <output stream> Postfix symbol Required/Optional Description + Optional This matches one or more events to the given condition. * Optional This matches zero or more events to the given condition. ? Optional This matches zero or one events to the given condition. Example This Siddhi application identifies temperature peeks. define stream TempStream(deviceID long, roomNo int, temp double); from every e1=TempStream, e2=TempStream[e1.temp <= temp]+, e3=TempStream[e2[last].temp > temp] select e1.temp as initialTemp, e2[last].temp as peakTemp insert into PeekTempStream; Logical Sequence Logical sequences identify logical relationships using and , or and not on consecutively arriving events. Syntax The syntax for a logical sequence is as follows: from (every)? (not)? <event reference>=<input stream>[<filter condition>] ((and|or) <event reference>=<input stream>[<filter condition>])? (within <time gap>)?, ... select <event reference>([event index])?.<attribute name>, ... insert into <output stream> Keywords such as and , or , or not can be used to illustrate the logical relationship, similar to how it is done in Logical Patterns . Example This Siddhi application notifies the state when a regulator event is immediately followed by both temperature and humidity events. define stream TempStream(deviceID long, temp double); define stream HumidStream(deviceID long, humid double); define stream RegulatorStream(deviceID long, isOn bool); from every e1=RegulatorStream, e2=TempStream and e3=HumidStream select e2.temp, e3.humid insert into StateNotificationStream; Output rate limiting \u00b6 Output rate limiting allows queries to output events periodically based on a specified condition. Purpose This allows you to limit the output to avoid overloading the subsequent executions, and to remove unnecessary information. Syntax The syntax of an output rate limiting configuration is as follows: from <input stream> ... select <attribute name>, <attribute name>, ... output <rate limiting configuration> insert into <output stream> Siddhi supports three types of output rate limiting configurations as explained in the following table: Rate limiting configuration Syntax Description Based on time <output event> every <time interval> This outputs <output event> every <time interval> time interval. Based on number of events <output event> every <event interval> events This outputs <output event> for every <event interval> number of events. Snapshot based output snapshot every <time interval> This outputs all events in the window (or the last event if no window is defined in the query) for every given <time interval> time interval. Here the <output event> specifies the event(s) that should be returned as the output of the query. The possible values are as follows: * first : Only the first event processed by the query during the specified time interval/sliding window is emitted. * last : Only the last event processed by the query during the specified time interval/sliding window is emitted. * all : All the events processed by the query during the specified time interval/sliding window are emitted. When no <output event> is defined, all is used by default. Examples Returning events based on the number of events Here, events are emitted every time the specified number of events arrive. You can also specify whether to emit only the first event/last event, or all the events out of the events that arrived. In this example, the last temperature per sensor is emitted for every 10 events. from TempStreamselect select temp, deviceID group by deviceID output last every 10 events insert into LowRateTempStream; Returning events based on time Here events are emitted for every predefined time interval. You can also specify whether to emit only the first event, last event, or all events out of the events that arrived during the specified time interval. In this example, emits all temperature events every 10 seconds from TempStreamoutput output every 10 sec insert into LowRateTempStream; Returning a periodic snapshot of events This method works best with windows. When an input stream is connected to a window, snapshot rate limiting emits all the current events that have arrived and do not have corresponding expired events for every predefined time interval. If the input stream is not connected to a window, only the last current event for each predefined time interval is emitted. This query emits a snapshot of the events in a time window of 5 seconds every 1 second. from TempStream#window.time(5 sec) output snapshot every 1 sec insert into SnapshotTempStream; Partition \u00b6 Partitions divide streams and queries into isolated groups in order to process them in parallel and in isolation. A partition can contain one or more queries and there can be multiple instances where the same queries and streams are replicated for each partition. Each partition is tagged with a partition key. Those partitions only process the events that match the corresponding partition key. Purpose Partitions allow you to process the events groups in isolation so that event processing can be performed using the same set of queries for each group. Partition key generation A partition key can be generated in the following two methods: Partition by value This is created by generating unique values using input stream attributes. Syntax partition with ( <expression> of <stream name>, <expression> of <stream name>, ... ) begin <query> <query> ... end; Example This query calculates the maximum temperature recorded within the last 10 events per deviceID . partition with ( deviceID of TempStream ) begin from TempStream#window.length(10) select roomNo, deviceID, max(temp) as maxTemp insert into DeviceTempStream; end; Partition by range This is created by mapping each partition key to a range condition of the input streams numerical attribute. Syntax partition with ( <condition> as <partition key> or <condition> as <partition key> or ... of <stream name>, ... ) begin <query> <query> ... end; Example This query calculates the average temperature for the last 10 minutes per office area. partition with ( roomNo >= 1030 as 'serverRoom' or roomNo < 1030 and roomNo >= 330 as 'officeRoom' or roomNo < 330 as 'lobby' of TempStream) begin from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp insert into AreaTempStream end; Inner Stream \u00b6 Queries inside a partition block can use inner streams to communicate with each other while preserving partition isolation. Inner streams are denoted by a \"#\" placed before the stream name, and these streams cannot be accessed outside a partition block. Purpose Inner streams allow you to connect queries within the partition block so that the output of a query can be used as an input only by another query within the same partition. Therefore, you do not need to repartition the streams if they are communicating within the partition. Example This partition calculates the average temperature of every 10 events for each sensor, and sends an output to the DeviceTempIncreasingStream stream if the consecutive average temperature values increase by more than 5 degrees. partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every (e1=#AvgTempStream),e2=#AvgTempStream[e1.avgTemp + 5 < avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end; Purge Partition \u00b6 Based on the partition key used for the partition, multiple instances of streams and queries will be generated. When an extremely large number of unique partition keys are used there is a possibility of very high instances of streams and queries getting generated and eventually system going out of memory. In order to overcome this, users can define a purge interval to clean partitions that will not be used anymore. Purpose @purge allows you to clean the partition instances that will not be used anymore. Syntax The syntax of partition purge configuration is as follows: @purge(enable='true', interval='<purge interval>', idle.period='<idle period of partition instance>') partition with ( <partition key> of <input stream> ) begin from <input stream> ... select <attribute name>, <attribute name>, ... insert into <output stream> end; Partition purge configuration Description Purge interval The periodic time interval to purge the purgeable partition instances. Idle period of partition instance The period, a particular partition instance (for a given partition key) needs to be idle before it becomes purgeable. Examples Mark partition instances eligible for purging, if there are no events from a particular deviceID for 15 seconds, and periodically purge those partition instances every 1 second. @purge(enable='true', interval='1 sec', idle.period='15 sec') partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every (e1=#AvgTempStream),e2=#AvgTempStream[e1.avgTemp + 5 < avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end; Table \u00b6 A table is a stored version of an stream or a table of events. Its schema is defined via the table definition that is similar to a stream definition. These events are by default stored in-memory , but Siddhi also provides store extensions to work with data/events stored in various data stores through the table abstraction. Purpose Tables allow Siddhi to work with stored events. By defining a schema for tables Siddhi enables them to be processed by queries using their defined attributes with the streaming data. You can also interactively query the state of the stored events in the table. Syntax The syntax for a new table definition is as follows: define table <table name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ); The following parameters are configured in a table definition: Parameter Description table name The name of the table defined. ( PascalCase is used for table name as a convention.) attribute name The schema of the table is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . Example The following defines a table named RoomTypeTable with roomNo and type attributes of data types int and string respectively. define table RoomTypeTable ( roomNo int, type string ); Primary Keys Tables can be configured with primary keys to avoid the duplication of data. Primary keys are configured by including the @PrimaryKey( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have only one @PrimaryKey annotation. The number of attributes supported differ based on the table implementations. When more than one attribute is used for the primary key, the uniqueness of the events stored in the table is determined based on the combination of values for those attributes. Examples This query creates an event table with the symbol attribute as the primary key. Therefore each entry in this table must have a unique value for symbol attribute. @PrimaryKey('symbol') define table StockTable (symbol string, price float, volume long); Indexes Indexes allow tables to be searched/modified much faster. Indexes are configured by including the @Index( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have 0-1 @Index annotations. Support for the @Index annotation and the number of attributes supported differ based on the table implementations. When more then one attribute is used for index, each one of them is used to index the table for fast access of the data. Indexes can be configured together with primary keys. Examples This query creates an indexed event table named RoomTypeTable with the roomNo attribute as the index key. @Index('roomNo') define table RoomTypeTable (roomNo int, type string); Store \u00b6 Store is a table that refers to data/events stored in data stores outside of Siddhi such as RDBMS, Cassandra, etc. Store is defined via the @store annotation, and the store schema is defined via a table definition associated with it. Purpose Store allows Siddhi to search, retrieve and manipulate data stored in external data stores through Siddhi queries. Syntax The syntax for a defining store and it's associated table definition is as follows: @store(type='store_type', static.option.key1='static_option_value1', static.option.keyN='static_option_valueN') define table TableName (attribute1 Type1, attributeN TypeN); Example The following defines a RDBMS data store pointing to a MySQL database with name hotel hosted in loacalhost:3306 having a table RoomTypeTable with columns roomNo of INTEGER and type of VARCHAR(255) mapped to Siddhi data types int and string respectively. @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/hotel\", username=\"siddhi\", password=\"123\", jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table RoomTypeTable ( roomNo int, type string ); Supported Store Types The following is a list of currently supported store types: RDBMS (MySQL, Oracle, SQL Server, PostgreSQL, DB2, H2) MongoDB Caching in Memory Store tables are persisted in high i/o latency storage. Hence, it is beneficial to maintain a cache of store tables in memory which has low latency. Siddhi supports caching of store tables through @cache annotation. It should be used within @store annotation in a nested fashion as shown below. @store(type='store_type', static.option.key1='static_option_value1', static.option.keyN='static_option_valueN', @cache(size=10, cache.policy=FIFO)) define table TableName (attribute1 Type1, attributeN TypeN); In the above example we have defined a cache with a maximum size of 10 rows with first-in first-out cache policy. The following table contains the cache parameters. Parameter Mandatory/Optional Default Value Description size Mandatory - maximum number of rows to be cached cache.policy Optional FIFO policy to free up cache when cache miss occurs. There are 3 allowed policies. 1. FIFO - First-In, First-Out 2. LRU - Least Recently Used 3. LFU - Least Frequently Used retention.period Optional - If user specifies this parameter then cache expiry is enabled. For example if this is 5 min, rows older than 5 mins will be removed and in some cases reloaded from store purge.interval optional equal to retention period When cache expiry is enabled, a thread will be created for every purge.interval which will check for expired rows and remove them. The following is an example of caching with expiry. @store(type='store_type', static.option.key1='static_option_value1', static.option.keyN='static_option_valueN', @cache(size=10, retention.period=5 min, purge.interval=1 min)) define table TableName (attribute1 Type1, attributeN TypeN); The above query will define and create a store table of given type and a cache with a max size of 10. A thread will be created every 1 minute which will check the entire cache table for rows added earlier than 5 minutes and expire them. Cache Behaviour Cache behaviour changes profoundly based on the size of store table relative to maximum cache size defined. Since memory is a limited resource we don't allow cache to grow more than the user specified maximum size. Case 1 \\ When store table is smaller than maximum cache size defined we keep the entire content of store table in memory in cache table. All types of queries are routed to cache and cache results are directly sent out to the user. Every time the expiry thread finds that cache events were loaded earlier than retention period entire cache table will be deleted and reloaded from store. In addition, when siddhi app starts, the entire store table, if it exists, will be loaded into cache. Case 2 \\ When store table is bigger than maximum cache size only the queries satisfying the following 2 conditions are sent to cache. 1. the query contains all the primary keys of the table 2. the query contains only == type of comparison. Only for the above types of queries we can establish if the cache is hit or missed. Subject to these conditions if the cache is hit the results from cache is sent out. If the cache is missed then store is checked. If the above conditions are not met by a query it is directly sent to the store table. In addition, please note that if the store table is pre existing when siddhi app is started and it is bigger than max cache size, cache preloading will take only upto max size and put it in cache. For example if store table has 50 entries when the siddhi app is defined with cache size of 10, only the first 10 rows will be cached. When cache miss occurs we look for the answer in the store table. If there is a result from the store table it is added to cache. One element from cache is removed using the user given cache policy prior to adding. When it comes to cache expiry, since not all rows are loaded at once in this case there may be some expired rows and some unexpired rows at any time. So for every purge interval a thread will be generated which looks for rows that were loaded earlier than retention period and delete only those rows. No reloading is done. Operators on Table (and Store) The following operators can be performed on tables (and stores). Insert \u00b6 This allows events to be inserted into tables. This is similar to inserting events into streams. Warning If the table is defined with primary keys, and if you insert duplicate data, primary key constrain violations can occur. In such cases use the update or insert into operation. Syntax from <input stream> select <attribute name>, <attribute name>, ... insert into <table> Similar to streams, you need to use the current events , expired events or the all events keyword between insert and into keywords in order to insert only the specific event types. For more information, see Event Type Example This query inserts all the events from the TempStream stream to the TempTable table. from TempStream select * insert into TempTable; Join (Table) \u00b6 This allows a stream to retrieve information from a table in a streaming manner. Note Joins can also be performed with two streams , aggregation or against externally named windows . Syntax from <input stream> join <table> on <condition> select (<input stream>|<table>).<attribute name>, (<input stream>|<table>).<attribute name>, ... insert into <output stream> Note A table can only be joint with a stream. Two tables cannot be joint because there must be at least one active entity to trigger the join operation. Example This Siddhi App performs a join to retrieve the room type from RoomTypeTable table based on the room number, so that it can filter the events related to server-room s. define table RoomTypeTable (roomNo int, type string); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream join RoomTypeTable on RoomTypeTable.roomNo == TempStream.roomNo select deviceID, RoomTypeTable.type as roomType, type, temp having roomType == 'server-room' insert into ServerRoomTempStream; Supported join types Table join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the table. The output is generated only if there is a matching event in both the stream and the table. Left outer join The left outer join operation allows you to join a stream on left side with a table on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right table by having null values for the attributes of the right table. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a table on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left table. Delete \u00b6 To delete selected events that are stored in a table. Syntax from <input stream> select <attribute name>, <attribute name>, ... delete <table> (for <event type>)? on <condition> The condition element specifies the basis on which events are selected to be deleted. When specifying the condition, table attributes should be referred to with the table name. To execute delete for specific event types, use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see Event Type Note Table attributes must be always referred to with the table name as follows: <table name>.<attibute name> Example In this example, the script deletes a record in the RoomTypeTable table if it has a value for the roomNo attribute that matches the value for the roomNumber attribute of an event in the DeleteStream stream. define table RoomTypeTable (roomNo int, type string); define stream DeleteStream (roomNumber int); from DeleteStream delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber; Update \u00b6 This operator updates selected event attributes stored in a table based on a condition. Syntax from <input stream> select <attribute name>, <attribute name>, ... update <table> (for <event type>)? set <table>.<attribute name> = (<attribute name>|<expression>)?, <table>.<attribute name> = (<attribute name>|<expression>)?, ... on <condition> The condition element specifies the basis on which events are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. To execute an update for specific event types use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see Event Type . Note Table attributes must be always referred to with the table name as shown below: <table name>.<attibute name> . Example This Siddhi application updates the room occupancy in the RoomOccupancyTable table for each room number based on new arrivals and exits from the UpdateStream stream. define table RoomOccupancyTable (roomNo int, people int); define stream UpdateStream (roomNumber int, arrival int, exit int); from UpdateStream select * update RoomOccupancyTable set RoomOccupancyTable.people = RoomOccupancyTable.people + arrival - exit on RoomOccupancyTable.roomNo == roomNumber; Update or Insert \u00b6 This allows you update if the event attributes already exist in the table based on a condition, or else insert the entry as a new attribute. Syntax from <input stream> select <attribute name>, <attribute name>, ... update or insert into <table> (for <event type>)? set <table>.<attribute name> = <expression>, <table>.<attribute name> = <expression>, ... on <condition> The condition element specifies the basis on which events are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note When the attribute to the right is a table attribute, the operations supported differ based on the database type. To execute update upon specific event types use the current events , expired events or the all events keyword with for as shown in the syntax. To understand more see Event Type . Note Table attributes should be always referred to with the table name as <table name>.<attibute name> . Example The following query update for events in the UpdateTable event table that have room numbers that match the same in the UpdateStream stream. When such events are found in the event table, they are updated. When a room number available in the stream is not found in the event table, it is inserted from the stream. define table RoomAssigneeTable (roomNo int, type string, assignee string); define stream RoomAssigneeStream (roomNumber int, type string, assignee string); from RoomAssigneeStream select roomNumber as roomNo, type, assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo; In \u00b6 This allows the stream to check whether the expected value exists in the table as a part of a conditional operation. Syntax from <input stream>[<condition> in <table>] select <attribute name>, <attribute name>, ... insert into <output stream> The condition element specifies the basis on which events are selected to be compared. When constructing the condition , the table attribute must be always referred to with the table name as shown below: <table>.<attibute name> . Example This Siddhi application filters only room numbers that are listed in the ServerRoomTable table. define table ServerRoomTable (roomNo int); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream[ServerRoomTable.roomNo == roomNo in ServerRoomTable] insert into ServerRoomTempStream; Named Aggregation \u00b6 Named aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. This not only allows you to calculate aggregations with varied time granularity, but also allows you to access them in an interactive manner for reports, dashboards, and for further processing. Its schema is defined via the aggregation definition . Purpose Named aggregation allows you to retrieve the aggregate values for different time durations. That is, it allows you to obtain aggregates such as sum , count , avg , min , max , count and distinctCount of stream attributes for durations such as sec , min , hour , etc. This is of considerable importance in many Analytics scenarios because aggregate values are often needed for several time periods. Furthermore, this ensures that the aggregations are not lost due to unexpected system failures because aggregates can be stored in different persistence stores . Syntax @store(type=\"<store type>\", ...) @purge(enable=\"<true or false>\",interval=<purging interval>,@retentionPeriod(<granularity> = <retention period>, ...) ) define aggregation <aggregator name> from <input stream> select <attribute name>, <aggregate function>(<attribute name>) as <attribute name>, ... group by <attribute name> aggregate by <timestamp attribute> every <time periods> ; The above syntax includes the following: Item Description @store This annotation is used to refer to the data store where the calculated aggregate results are stored. This annotation is optional. When no annotation is provided, the data is stored in the in-memory store. @purge This annotation is used to configure purging in aggregation granularities. If this annotation is not provided, the default purging mentioned above is applied. If you want to disable automatic data purging, you can use this annotation as follows: '@purge(enable=false) /You should disable data purging if the aggregation query in included in the Siddhi application for read-only purposes. @retentionPeriod This annotation is used to specify the length of time the data needs to be retained when carrying out data purging. If this annotation is not provided, the default retention period is applied. <aggregator name> This specifies a unique name for the aggregation so that it can be referred when accessing aggregate results. <input stream> The stream that feeds the aggregation. Note! this stream should be already defined. group by <attribute name> The group by clause is optional. If it is included in a Siddhi application, aggregate values are calculated per each group by attribute. If it is not used, all the events are aggregated together. by <timestamp attribute> This clause is optional. This defines the attribute that should be used as the timestamp. If this clause is not used, the event time is used by default. The timestamp could be given as either a string or a long value. If it is a long value, the unix timestamp in milliseconds is expected (e.g. 1496289950000 ). If it is a string value, the supported formats are <yyyy>-<MM>-<dd> <HH>:<mm>:<ss> (if time is in GMT) and <yyyy>-<MM>-<dd> <HH>:<mm>:<ss> <Z> (if time is not in GMT), here the ISO 8601 UTC offset must be provided for <Z> . (e.g., +05:30 , -11:00 ). <time periods> Time periods can be specified as a range where the minimum and the maximum value are separated by three dots, or as comma-separated values. e.g., A range can be specified as sec...year where aggregation is done per second, minute, hour, day, month and year. Comma-separated values can be specified as min, hour. Skipping time durations (e.g., min, day where the hour duration is skipped) when specifying comma-separated values is supported only from v4.1.1 onwards Aggregation's granularity data holders are automatically purged every 15 minutes. When carrying out data purging, the retention period you have specified for each granularity in the named aggregation query is taken into account. The retention period defined for a granularity needs to be greater than or equal to its minimum retention period as specified in the table below. If no valid retention period is defined for a granularity, the default retention period (as specified in the table below) is applied. Granularity Default retention Minimum retention second 120 seconds 120 seconds minute 24 hours 120 minutes hour 30 days 25 hours day 1 year 32 days month All 13 month year All none Note Aggregation is carried out at calendar start times for each granularity with the GMT timezone Note The same aggregation can be defined in multiple Siddhi apps for joining, however, only one siddhi app should carry out the processing (i.e. the aggregation input stream should only feed events to one aggregation definition). Example This Siddhi Application defines an aggregation named TradeAggregation to calculate the average and sum for the price attribute of events arriving at the TradeStream stream. These aggregates are calculated per every time granularity in the second-year range. define stream TradeStream (symbol string, price double, volume long, timestamp long); @purge(enable='true', interval='10 sec',@retentionPeriod(sec='120 sec',min='24 hours',hours='30 days',days='1 year',months='all',years='all')) define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; Distributed Aggregation \u00b6 Distributed Aggregation allows you to partially process aggregations in different shards. This allows Siddhi app in one shard to be responsible only for processing a part of the aggregation. However for this, all aggregations must be based on a common physical database(@store). Syntax @store(type=\"<store type>\", ...) @PartitionById define aggregation <aggregator name> from <input stream> select <attribute name>, <aggregate function>(<attribute name>) as <attribute name>, ... group by <attribute name> aggregate by <timestamp attribute> every <time periods> ; Following table includes the annotation to be used to enable distributed aggregation, Item Description @PartitionById If the annotation is given, then the distributed aggregation is enabled. Further this can be disabled by using enable element, @PartitionById(enable='false') . Further, following system properties are also available, System Property Description Possible Values Optional Default Value shardId The id of the shard one of the distributed aggregation is running in. This should be unique to a single shard Any string No partitionById This allows user to enable/disable distributed aggregation for all aggregations running in one siddhi manager .(Available from v4.3.3) true/false Yesio false Note ShardIds should not be changed after the first configuration in order to keep data consistency. Join (Aggregation) \u00b6 This allows a stream to retrieve calculated aggregate values from the aggregation. Note A join can also be performed with two streams , with a table and a stream, or with a stream against externally named windows . Syntax A join with aggregation is similer to the join with table , but with additional within and per clauses. from <input stream> join <aggrigation> on <join condition> within <time range> per <time granularity> select <attribute name>, <attribute name>, ... insert into <output stream>; Apart from constructs of table join this includes the following. Please note that the 'on' condition is optional : Item Description within <time range> This allows you to specify the time interval for which the aggregate values need to be retrieved. This can be specified by providing the start and end time separated by a comma as string or long values, or by using the wildcard string specifying the data range. For details refer examples. per <time granularity> This specifies the time granularity by which the aggregate values must be grouped and returned. e.g., If you specify days , the retrieved aggregate values are grouped for each day within the selected time interval. within and per clauses also accept attribute values from the stream. The timestamp of the aggregations can be accessed through the AGG_TIMESTAMP attribute. Example Following aggregation definition will be used for the examples. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select AGG_TIMESTAMP, symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) define stream StockStream (symbol string, value int); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; This query retrieves hourly aggregations within the day 2014-02-15 . define stream StockStream (symbol string, value int); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within \"2014-02-15 **:**:** +05:30\" per \"hours\" select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; This query retrieves all aggregations per perValue stream attribute within the time period between timestamps 1496200000000 and 1596434876000 . define stream StockStream (symbol string, value int, perValue string); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within 1496200000000L, 1596434876000L per S.perValue select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; Supported join types Aggregation join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the aggregation. The output is generated only if there is a matching event in the stream and the aggregation. Left outer join The left outer join operation allows you to join a stream on left side with a aggregation on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right aggregation by having null values for the attributes of the right aggregation. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a aggregation on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left aggregation. Named Window \u00b6 A named window is a window that can be shared across multiple queries. Events can be inserted to a named window from one or more queries and it can produce output events based on the named window type. Syntax The syntax for a named window is as follows: define window <window name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ) <window type>(<parameter>, <parameter>, \u2026) <event type>; The following parameters are configured in a table definition: Parameter Description window name The name of the window defined. ( PascalCase is used for window names as a convention.) attribute name The schema of the window is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . <window type>(<parameter>, ...) The window type associated with the window and its parameters. output <event type> This is optional. Keywords such as current events , expired events and all events (the default) can be used to specify when the window output should be exposed. For more information, see Event Type . Examples Returning all output when events arrive and when events expire from the window. In this query, the event type is not specified. Therefore, it returns both current and expired events as the output. define window SensorWindow (name string, value float, roomNo int, deviceID string) timeBatch(1 second); Returning an output only when events expire from the window. In this query, the event type of the window is expired events . Therefore, it only returns the events that have expired from the window as the output. define window SensorWindow (name string, value float, roomNo int, deviceID string) timeBatch(1 second) output expired events; Operators on Named Windows The following operators can be performed on named windows. Insert \u00b6 This allows events to be inserted into windows. This is similar to inserting events into streams. Syntax from <input stream> select <attribute name>, <attribute name>, ... insert into <window> To insert only events of a specific event type, add the current events , expired events or the all events keyword between insert and into keywords (similar to how it is done for streams). For more information, see Event Type . Example This query inserts all events from the TempStream stream to the OneMinTempWindow window. define stream TempStream(tempId string, temp double); define window OneMinTempWindow(tempId string, temp double) time(1 min); from TempStream select * insert into OneMinTempWindow; Join (Window) \u00b6 To allow a stream to retrieve information from a window based on a condition. Note A join can also be performed with two streams , aggregation or with tables tables . Syntax from <input stream> join <window> on <condition> select (<input stream>|<window>).<attribute name>, (<input stream>|<window>).<attribute name>, ... insert into <output stream> Example This Siddhi Application performs a join count the number of temperature events having more then 40 degrees within the last 2 minutes. define window TwoMinTempWindow (roomNo int, temp double) time(2 min); define stream CheckStream (requestId string); from CheckStream as C join TwoMinTempWindow as T on T.temp > 40 select requestId, count(T.temp) as count insert into HighTempCountStream; Supported join types Window join supports following operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join two windows or a stream with a window. The output is generated only if there is a matching event in both stream/window. Left outer join The left outer join operation allows you to join two windows or a stream with a window to be merged based on a condition. Here, it returns all the events of left stream/window even if there are no matching events in the right stream/window by having null values for the attributes of the right stream/window. Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join two windows or a stream with a window. It returns all the events of the right stream/window even if there are no matching events in the left stream/window. Full outer join The full outer join combines the results of left outer join and right outer join . full outer join is used as the keyword to join two windows or a stream with a window. Here, output event are generated for each incoming event even if there are no matching events in the other stream/window. From \u00b6 A window can be an input to a query, similar to streams. Note !!! When window is used as an input to a query, another window cannot be applied on top of this. Syntax from <window> select <attribute name>, <attribute name>, ... insert into <output stream> Example This Siddhi Application calculates the maximum temperature within the last 5 minutes. define window FiveMinTempWindow (roomNo int, temp double) time(5 min); from FiveMinTempWindow select max(temp) as maxValue, roomNo insert into MaxSensorReadingStream; Trigger \u00b6 Triggers allow events to be periodically generated. Trigger definition can be used to define a trigger. A trigger also works like a stream with a predefined schema. Purpose For some use cases the system should be able to periodically generate events based on a specified time interval to perform some periodic executions. A trigger can be performed for a 'start' operation, for a given <time interval> , or for a given '<cron expression>' . Syntax The syntax for a trigger definition is as follows. define trigger <trigger name> at ('start'| every <time interval>| '<cron expression>'); Similar to streams, triggers can be used as inputs. They adhere to the following stream definition and produce the triggered_time attribute of the long type. define stream <trigger name> (triggered_time long); The following types of triggeres are currently supported: Trigger type Description 'start' An event is triggered when Siddhi is started. every <time interval> An event is triggered periodically at the given time interval. '<cron expression>' An event is triggered periodically based on the given cron expression. For configuration details, see quartz-scheduler . Examples Triggering events regularly at specific time intervals The following query triggers events every 5 minutes. define trigger FiveMinTriggerStream at every 5 min; Triggering events at a specific time on specified days The following query triggers an event at 10.15 AM on every weekdays. define trigger FiveMinTriggerStream at '0 15 10 ? * MON-FRI'; Script \u00b6 Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Function definitions can be used to define these scripts. Function parameters are passed into the function logic as Object[] and with the name data . Purpose Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Syntax The syntax for a Script definition is as follows. define function <function name>[<language name>] return <return type> { <operation of the function> }; The following parameters are configured when defining a script. Parameter Description function name The name of the function ( camelCase is used for the function name) as a convention. language name The name of the programming language used to define the script, such as javascript , r and scala . return type The attribute type of the function\u2019s return. This can be int , long , float , double , string , bool or object . Here the function implementer should be responsible for returning the output attribute on the defined return type for proper functionality. operation of the function Here, the execution logic of the function is added. This logic should be written in the language specified under the language name , and it should return the output in the data type specified via the return type parameter. Examples This query performs concatenation using JavaScript, and returns the output as a string. define function concatFn[javascript] return string { var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var responce = str1 + str2 + str3; return responce; }; define stream TempStream(deviceID long, roomNo int, temp double); from TempStream select concatFn(roomNo,'-',deviceID) as id, temp insert into DeviceTempStream; Store Query \u00b6 Siddhi store queries are a set of on-demand queries that can be used to perform operations on Siddhi tables, windows, and aggregators. Purpose Store queries allow you to execute the following operations on Siddhi tables, windows, and aggregators without the intervention of streams. Queries supported for tables: SELECT INSERT DELETE UPDATE UPDATE OR INSERT Queries supported for windows and aggregators: SELECT This is be done by submitting the store query to the Siddhi application runtime using its query() method. In order to execute store queries, the Siddhi application of the Siddhi application runtime you are using, should have a store defined, which contains the table that needs to be queried. Example If you need to query the table named RoomTypeTable the it should have been defined in the Siddhi application. In order to execute a store query on RoomTypeTable , you need to submit the store query using query() method of SiddhiAppRuntime instance as below. siddhiAppRuntime.query(<store query>); (Table/Window) Select \u00b6 The SELECT store query retrieves records from the specified table or window, based on the given condition. Syntax from <table/window> <on condition>? select <attribute name>, <attribute name>, ... <group by>? <having>? <order by>? <limit>? Example This query retrieves room numbers and types of the rooms starting from room no 10. from roomTypeTable on roomNo >= 10; select roomNo, type (Aggregation) Select \u00b6 The SELECT store query retrieves records from the specified aggregation, based on the given condition, time range, and granularity. Syntax from <aggregation> <on condition>? within <time range> per <time granularity> select <attribute name>, <attribute name>, ... <group by>? <having>? <order by>? <limit>? Example Following aggregation definition will be used for the examples. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) from TradeAggregation within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select symbol, total, avgPrice ; This query retrieves hourly aggregations of \"FB\" symbol within the day 2014-02-15 . from TradeAggregation on symbol == \"FB\" within \"2014-02-15 **:**:** +05:30\" per \"hours\" select symbol, total, avgPrice; Insert \u00b6 This allows you to insert a new record to the table with the attribute values you define in the select section. Syntax select <attribute name>, <attribute name>, ... insert into <table>; Example This store query inserts a new record to the table RoomOccupancyTable , with the specified attribute values. select 10 as roomNo, 2 as people insert into RoomOccupancyTable Delete \u00b6 The DELETE store query deletes selected records from a specified table. Syntax <select>? delete <table> on <conditional expresssion> The condition element specifies the basis on which records are selected to be deleted. Note Table attributes must always be referred to with the table name as shown below: <table name>.<attibute name> . Example In this example, query deletes a record in the table named RoomTypeTable if it has value for the roomNo attribute that matches the value for the roomNumber attribute of the selection which has 10 as the actual value. select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber; delete RoomTypeTable on RoomTypeTable.roomNo == 10; Update \u00b6 The UPDATE store query updates selected attributes stored in a specific table, based on a given condition. Syntax select <attribute name>, <attribute name>, ...? update <table> set <table>.<attribute name> = (<attribute name>|<expression>)?, <table>.<attribute name> = (<attribute name>|<expression>)?, ... on <condition> The condition element specifies the basis on which records are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: <table name>.<attibute name> . Example The following query updates the room occupancy by increasing the value of people by 1, in the RoomOccupancyTable table for each room number greater than 10. select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber; update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + 1 on RoomTypeTable.roomNo == 10; Update or Insert \u00b6 This allows you to update selected attributes if a record that meets the given conditions already exists in the specified table. If a matching record does not exist, the entry is inserted as a new record. Syntax select <attribute name>, <attribute name>, ... update or insert into <table> set <table>.<attribute name> = <expression>, <table>.<attribute name> = <expression>, ... on <condition> The condition element specifies the basis on which records are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: <table name>.<attibute name> . Example The following query tries to update the records in the RoomAssigneeTable table that have room numbers that match the same in the selection. If such records are not found, it inserts a new record based on the values provided in the selection. select 10 as roomNo, \"single\" as type, \"abc\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo; Extensions \u00b6 Siddhi supports an extension architecture to enhance its functionality by incorporating other libraries in a seamless manner. Purpose Extensions are supported because, Siddhi core cannot have all the functionality that's needed for all the use cases, mostly use cases require different type of functionality, and for some cases there can be gaps and you need to write the functionality by yourself. All extensions have a namespace. This is used to identify the relevant extensions together, and to let you specifically call the extension. Syntax Extensions follow the following syntax; <namespace>:<function name>(<parameter>, <parameter>, ... ) The following parameters are configured when referring a script function. Parameter Description namespace Allows Siddhi to identify the extension without conflict function name The name of the function referred. parameter The function input parameter for function execution. Extension Types Siddhi supports following extension types: Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute. This can be used to manipulate existing event attributes to generate new attributes like any Function operation. This is implemented by extending io.siddhi.core.executor.function.FunctionExecutor . Example : math:sin(x) Here, the sin function of math extension returns the sin value for the x parameter. Aggregate Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute with aggregated results. This can be used in conjunction with a window in order to find the aggregated results based on the given window like any Aggregate Function operation. This is implemented by extending io.siddhi.core.query.selector.attribute.aggregator.AttributeAggregatorExecutor . Example : custom:std(x) Here, the std aggregate function of custom extension returns the standard deviation of the x value based on its assigned window query. Window This allows events to be collected, generated, dropped and expired anytime without altering the event format based on the given input parameters, similar to any other Window operator. This is implemented by extending io.siddhi.core.query.processor.stream.window.WindowProcessor . Example : custom:unique(key) Here, the unique window of the custom extension retains one event for each unique key parameter. Stream Function This allows events to be generated or dropped only during event arrival and altered by adding one or more attributes to it. This is implemented by extending io.siddhi.core.query.processor.stream.function.StreamFunctionProcessor . Example : custom:pol2cart(theta,rho) Here, the pol2cart function of the custom extension returns all the events by calculating the cartesian coordinates x & y and adding them as new attributes to the events. Stream Processor This allows events to be collected, generated, dropped and expired anytime by altering the event format by adding one or more attributes to it based on the given input parameters. Implemented by extending io.siddhi.core.query.processor.stream.StreamProcessor . Example : custom:perMinResults(<parameter>, <parameter>, ...) Here, the perMinResults function of the custom extension returns all events by adding one or more attributes to the events based on the conversion logic. Altered events are output every minute regardless of event arrivals. Sink Sinks provide a way to publish Siddhi events to external systems in the preferred data format. Sinks publish events from the streams via multiple transports to external endpoints in various data formats. Implemented by extending io.siddhi.core.stream.output.sink.Sink . Example : @sink(type='sink_type', static_option_key1='static_option_value1') To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as above Source Source allows Siddhi to consume events from external systems , and map the events to adhere to the associated stream. Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. Implemented by extending io.siddhi.core.stream.input.source.Source . Example : @source(type='source_type', static.option.key1='static_option_value1') To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as above Store You can use Store extension type to work with data/events stored in various data stores through the table abstraction . You can find more information about these extension types under the heading 'Extension types' in this document. Implemented by extending io.siddhi.core.table.record.AbstractRecordTable . Script Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Implemented by extending io.siddhi.core.function.Script . Source Mapper Each @source configuration has a mapping denoted by the @map annotation that converts the incoming messages format to Siddhi events .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SourceMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Sink Mapper Each @sink configuration has a mapping denoted by the @map annotation that converts the outgoing Siddhi events to configured messages format .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SinkMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Example A window extension created with namespace foo and function name unique can be referred as follows: from StockExchangeStream[price >= 20]#window.foo:unique(symbol) select symbol, price insert into StockQuote Available Extensions Siddhi currently has several pre written extensions that are available here We value your contribution on improving Siddhi and its extensions further. Writing Custom Extensions \u00b6 Custom extensions can be written in order to cater use case specific logic that are not available in Siddhi out of the box or as an existing extension. There are five types of Siddhi extensions that you can write to cater your specific use cases. These extension types and the related maven archetypes are given below. You can use these archetypes to generate Maven projects for each extension type. Follow the procedure for the required archetype, based on your project: Note When using the generated archetype please make sure you complete the @Extension annotation with proper values. This annotation will be used to identify and document the extension, hence your extension will not work without @Extension annotation. siddhi-execution Siddhi-execution provides following extension types: Function Aggregate Function Stream Function Stream Processor Window You can use one or more from above mentioned extension types and implement according to your requirement. For more information about these extension types, see Extension Types . To install and implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DgroupId=io.siddhi.extension.execution -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfFunction Name of the custom function to be created Y - _nameSpaceOfFunction Namespace of the function, used to grouped similar custom functions Y - groupIdPostfix Namespace of the function is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-execution- classNameOfAggregateFunction Class name of the Aggregate Function N $ classNameOfFunction Class name of the Function N $ classNameOfStreamFunction Class name of the Stream Function N $ classNameOfStreamProcessor Class name of the Stream Processor N $ classNameOfWindow Class name of the Window N $ To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-io Siddhi-io provides following extension types: Sink Source You can use one or more from above mentioned extension types and implement according to your requirement. siddhi-io is generally used to work with IO operations as follows: * The Source extension type gets inputs to your Siddhi application. * The Sink extension publishes outputs from your Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DgroupId=io.siddhi.extension.io -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _IOType Type of IO for which Siddhi-io extension is written Y - groupIdPostfix Type of the IO is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-io- classNameOfSink Class name of the Sink N classNameOfSource Class name of the Source N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-map Siddhi-map provides following extension types, Sink Mapper Source Mapper You can use one or more from above mentioned extension types and implement according to your requirement as follows. The Source Mapper maps events to a predefined data format (such as XML, JSON, binary, etc), and publishes them to external endpoints (such as E-mail, TCP, Kafka, HTTP, etc). The Sink Mapper also maps events to a predefined data format, but it does it at the time of publishing events from a Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-map extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DgroupId=io.siddhi.extension.map -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _mapType Type of Mapper for which Siddhi-map extension is written Y - groupIdPostfix Type of the Map is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-map- classNameOfSinkMapper Class name of the Sink Mapper N classNameOfSourceMapper Class name of the Source Mapper N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-script Siddhi-script provides the Script extension type. The script extension type allows you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. For more information about these extension types, see Extension Types . To implement the siddhi-script extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DgroupId=io.siddhi.extension.script -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfScript Name of Custom Script for which Siddhi-script extension is written Y - groupIdPostfix Name of the Script is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-script- classNameOfScript Class name of the Script N Eval To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-store Siddhi-store provides the Store extension type. The Store extension type allows you to work with data/events stored in various data stores through the table abstraction. For more information about these extension types, see Extension Types . To implement the siddhi-store extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DgroupId=io.siddhi.extension.store -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _storeType Type of Store for which Siddhi-store extension is written Y - groupIdPostfix Type of the Store is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-store- className Class name of the Store N To confirm that all property values are correct, type Y in the console. If not, press N . Configuring and Monitoring Siddhi Applications \u00b6 Multi-threading and Asynchronous Processing \u00b6 When @Async annotation is added to the Streams it enable the Streams to introduce asynchronous and multi-threading behaviour. @Async(buffer.size='256', workers='2', batch.size.max='5') define stream <stream name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ); The following elements are configured with this annotation. Annotation Description Default Value buffer.size The size of the event buffer that will be used to handover the execution to other threads. - workers Number of worker threads that will be be used to process the buffered events. 1 batch.size.max The maximum number of events that will be processed together by a worker thread at a given time. buffer.size Statistics \u00b6 Use @app:statistics app level annotation to evaluate the performance of an application, you can enable the statistics of a Siddhi application to be published. This is done via the @app:statistics annotation that can be added to a Siddhi application as shown in the following example. @app:statistics(reporter = 'console') The following elements are configured with this annotation. Annotation Description Default Value reporter The interface in which statistics for the Siddhi application are published. Possible values are as follows: console jmx console interval The time interval (in seconds) at which the statistics for the Siddhi application are reported. 60 include If this parameter is added, only the types of metrics you specify are included in the reporting. The required metric types can be specified as a comma-separated list. It is also possible to use wild cards All ( . ) The metrics are reported in the following format. io.siddhi.SiddhiApps.<SiddhiAppName>.Siddhi.<Component Type>.<Component Name>. <Metrics name> The following table lists the types of metrics supported for different Siddhi application component types. Component Type Metrics Type Stream Throughput The size of the buffer if parallel processing is enabled via the @async annotation. Trigger Throughput (Trigger and Stream) Source Throughput Sink Throughput Mapper Latency Input/output throughput Table Memory Throughput (For all operations) Throughput (For all operations) Query Memory Latency Window Throughput (For all operations) Latency (For all operation) Partition Throughput (For all operations) Latency (For all operation) e.g., the following is a Siddhi application that includes the @app annotation to report performance statistics. @App:name('TestMetrics') @App:Statistics(reporter = 'console') define stream TestStream (message string); @info(name='logQuery') from TestSream#log(\"Message:\") insert into TempSream; Statistics are reported for this Siddhi application as shown in the extract below. Click to view the extract 11/26/17 8:01:20 PM ============================================================ -- Gauges ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.memory value = 5760 io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.size value = 0 -- Meters ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Sources.TestStream.http.throughput count = 0 mean rate = 0.00 events/second 1-minute rate = 0.00 events/second 5-minute rate = 0.00 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TempSream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second -- Timers ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.latency count = 2 mean rate = 0.11 calls/second 1-minute rate = 0.34 calls/second 5-minute rate = 0.39 calls/second 15-minute rate = 0.40 calls/second min = 0.61 milliseconds max = 1.08 milliseconds mean = 0.84 milliseconds stddev = 0.23 milliseconds median = 0.61 milliseconds 75% < = 1.08 milliseconds 95% < = 1.08 milliseconds 98% < = 1.08 milliseconds 99% < = 1.08 milliseconds 99.9% < = 1.08 milliseconds Event Playback \u00b6 When @app:playback annotation is added to the app, the timestamp of the event (specified via an attribute) is treated as the current time. This results in events being processed faster. The following elements are configured with this annotation. Annotation Description idle.time If no events are received during a time interval specified (in milliseconds) via this element, the Siddhi system time is incremented by a number of seconds specified via the increment element. increment The number of seconds by which the Siddhi system time must be incremented if no events are received during the time interval specified via the idle.time element. e.g., In the following example, the Siddhi system time is incremented by two seconds if no events arrive for a time interval of 100 milliseconds. @app:playback(idle.time = '100 millisecond', increment = '2 sec')","title":"Query Guide"},{"location":"docs/query-guide/#siddhi-52-streaming-sql-guide","text":"","title":"Siddhi 5.2 Streaming SQL Guide"},{"location":"docs/query-guide/#introduction","text":"Siddhi Streaming SQL is designed to process streams of events. It can be used to implement streaming data integration, streaming analytics, rule based and adaptive decision making use cases. It is an evolution of Complex Event Processing (CEP) and Stream Processing systems, hence it can also be used to process stateful computations, detecting of complex event patterns, and sending notifications in real-time. Siddhi Streaming SQL uses SQL like syntax, and annotations to consume events from diverse event sources with various data formats, process then using stateful and stateless operators and send outputs to multiple endpoints according to their accepted event formats. It also supports exposing rule based and adaptive decision making as service endpoints such that external programs and systems can synchronously get decision support form Siddhi. The following sections explains how to write processing logic using Siddhi Streaming SQL.","title":"Introduction"},{"location":"docs/query-guide/#siddhi-application","text":"The processing logic for your program can be written using the Streaming SQL and put together as a single file with .siddhi extension. This file is called as the Siddhi Application or the SiddhiApp . SiddhiApps are named by adding @app:name('<name>') annotation on the top of the SiddhiApp file. When the annotation is not added Siddhi assigns a random UUID as the name of the SiddhiApp. Purpose SiddhiApp provides an isolated execution environment for your processing logic that allows you to deploy and execute processing logic independent of other SiddhiApp in the system. Therefore it's always recommended to have a processing logic related to single use case in a single SiddhiApp. This will help you to group processing logic and easily manage addition and removal of various use cases. The following diagram depicts some of the key Siddhi Streaming SQL elements of Siddhi Application and how event flows through the elements. Below table provides brief description of a few key elements in the Siddhi Streaming SQL Language. Elements Description Stream A logical series of events ordered in time with a uniquely identifiable name, and a defined set of typed attributes defining its schema. Event An event is a single event object associated with a stream. All events of a stream contains a timestamp and an identical set of typed attributes based on the schema of the stream they belong to. Table A structured representation of data stored with a defined schema. Stored data can be backed by In-Memory , or external data stores such as RDBMS , MongoDB , etc. The tables can be accessed and manipulated at runtime. Named Window A structured representation of data stored with a defined schema and eviction policy. Window data is stored In-Memory and automatically cleared by the named window constrain. Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Named Aggregation A structured representation of data that's incrementally aggregated and stored with a defined schema and aggregation granularity such as seconds, minutes, hours, etc. Aggregation data is stored both In-Memory and in external data stores such as RDBMS . Other siddhi elements can only query the values in windows at runtime but they cannot modify them. Query A logical construct that processes events in streaming manner by by consuming data from one or more streams, tables, windows and aggregations, and publishes output events into a stream, table or a window. Source A construct that consumes data from external sources (such as TCP , Kafka , HTTP , etc) with various event formats such as XML , JSON , binary , etc, convert then to Siddhi events, and passes into streams for processing. Sink A construct that consumes events arriving at a stream, maps them to a predefined data format (such as XML , JSON , binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Input Handler A mechanism to programmatically inject events into streams. Stream/Query Callback A mechanism to programmatically consume output events from streams or queries. Partition A logical container that isolates the processing of queries based on the partition keys derived from the events. Inner Stream A positionable stream that connects portioned queries with each other within the partition. Grammar SiddhiApp is a collection of Siddhi Streaming SQL elements composed together as a script. Here each Siddhi element must be separated by a semicolon ; . Hight level syntax of SiddhiApp is as follows. <siddhi app> : <app annotation> * ( <stream definition> | <table definition> | ... ) + ( <query> | <partition> ) + ; Example Siddhi Application with name Temperature-Analytics defined with a stream named TempStream and a query named 5minAvgQuery . @app:name('Temperature-Analytics') define stream TempStream (deviceID long, roomNo int, temp double); @name('5minAvgQuery') from TempStream#window.time(5 min) select roomNo, avg(temp) as avgTemp group by roomNo insert into OutputStream;","title":"Siddhi Application"},{"location":"docs/query-guide/#stream","text":"A stream is a logical series of events ordered in time. Its schema is defined via the stream definition . A stream definition contains the stream name and a set of attributes with specific types and uniquely identifiable names within the stream. All events associated to the stream will have the same schema (i.e., have the same attributes in the same order). Purpose Stream groups common types of events together with a schema. This helps in various ways such as, processing all events together in queries and performing data format transformations together when they are consumed and published via sources and sinks. Syntax The syntax for defining a new stream is as follows. define stream <stream name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ); The following parameters are used to configure a stream definition. Parameter Description stream name The name of the stream created. (It is recommended to define a stream name in PascalCase .) attribute name Uniquely identifiable name of the stream attribute. (It is recommended to define attribute names in camelCase .) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . To use and refer stream and attribute names that do not follow [a-zA-Z_][a-zA-Z_0-9]* format enclose them in ` . E.g. `$test(0)` . To make the stream process events in multi-threading and asynchronous way use the @Async annotation as shown in Multi-threading and Asynchronous Processing configuration section. Example define stream TempStream (deviceID long, roomNo int, temp double); The above creates a stream with name TempStream having the following attributes. deviceID of type long roomNo of type int temp of type double","title":"Stream"},{"location":"docs/query-guide/#source","text":"Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. A source configuration allows to define a mapping in order to convert each incoming event from its native data format to a Siddhi event. When customizations to such mappings are not provided, Siddhi assumes that the arriving event adheres to the predefined format based on the stream definition and the configured message mapping type. Purpose Source provides a way to consume events from external systems and convert them to be processed by the associated stream. Syntax To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as follows: @source(type='<source type>', <static.key>='<value>', <static.key>='<value>', @map(type='<map type>', <static.key>='<value>', <static.key>='<value>', @attributes( <attribute1>='<attribute mapping>', <attributeN>='<attribute mapping>') ) ) define stream <stream name> (<attribute1> <type>, <attributeN> <type>); This syntax includes the following annotations. Source The type parameter of @source annotation defines the source type that receives events. The other parameters of @source annotation depends upon the selected source type, and here some of its parameters can be optional. For detailed information about the supported parameters see the documentation of the relevant source. The following is the list of source types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to consume events from other SiddhiApps running on the same JVM. HTTP Expose an HTTP service to consume messages. Kafka Subscribe to Kafka topic to consume events. TCP Expose a TCP service to consume messages. Email Consume emails via POP3 and IMAP protocols. JMS Subscribe to JMS topic or queue to consume events. File Reads files by tailing or as a whole to extract events out of them. CDC Perform change data capture on databases. Prometheus Consume data from Prometheus agent. In-memory is the only source inbuilt in Siddhi, and all other source types are implemented as extensions.","title":"Source"},{"location":"docs/query-guide/#source-mapper","text":"Each @source configuration can have a mapping denoted by the @map annotation that defines how to convert the incoming event format to Siddhi events. The type parameter of the @map defines the map type to be used in converting the incoming events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional. For detailed information about the parameters see the documentation of the relevant mapper. Map Attributes @attributes is an optional annotation used with @map to define custom mapping. When @attributes is not provided, each mapper assumes that the incoming events adheres to its own default message format and attempt to convert the events from that format. By adding the @attributes annotation, users can selectively extract data from the incoming message and assign them to the attributes. There are two ways to configure @attributes . Define attribute names as keys, and mapping configurations as values: @attributes( <attribute1>='<mapping>', <attributeN>='<mapping>') Define the mapping configurations in the same order as the attributes defined in stream definition: @attributes( '<mapping for attribute1>', '<mapping for attributeN>') Supported Source Mapping Types The following is the list of source mapping types supported by Siddhi: Source mapping type Description PassThrough Omits data conversion on Siddhi events. JSON Converts JSON messages to Siddhi events. XML Converts XML messages to Siddhi events. TEXT Converts plain text messages to Siddhi events. Avro Converts Avro events to Siddhi events. Binary Converts Siddhi specific binary events to Siddhi events. Key Value Converts key-value HashMaps to Siddhi events. CSV Converts CSV like delimiter separated events to Siddhi events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the consumed Siddhi events directly to the streams without any data conversion. PassThrough is the only source mapper inbuilt in Siddhi, and all other source mappers are implemented as extensions. Example 1 Receive JSON messages by exposing an HTTP service, and direct them to InputStream stream for processing. Here the HTTP service will be secured with basic authentication, receives events on all network interfaces on port 8080 and context /foo . The service expects the JSON messages to be on the default data format that's supported by the JSON mapper as follows. { \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } The configuration of the HTTP source and JSON source mapper to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json')) define stream InputStream (name string, age int, country string); Example 2 Receive JSON messages by exposing an HTTP service, and direct them to StockStream stream for processing. Here the incoming JSON , as given bellow, do not adhere to the default data format that's supported by the JSON mapper. { \"portfolio\":{ \"stock\":{ \"volume\":100, \"company\":{ \"symbol\":\"FB\" }, \"price\":55.6 } } } The configuration of the HTTP source and the custom JSON source mapping to achieve the above is as follows. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); The same can also be configured by omitting the attribute names as bellow. @source(type='http', receiver.url='http://0.0.0.0:8080/foo', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(\"stock.company.symbol\", \"stock.price\", \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long);","title":"Source Mapper"},{"location":"docs/query-guide/#sink","text":"Sinks consumes events from streams and publish them via multiple transports to external endpoints in various data formats. A sink configuration allows users to define a mapping to convert the Siddhi events in to the required output data format (such as JSON , TEXT , XML , etc.) and publish the events to the configured endpoints. When customizations to such mappings are not provided, Siddhi converts events to the predefined event format based on the stream definition and the configured message mapper type before publishing the events. Purpose Sink provides a way to publish Siddhi events of a stream to external systems by converting events to their supported format. Syntax To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as follows: @sink(type='<sink type>', <static.key>='<value>', <dynamic.key>='{{<value>}}', @map(type='<map type>', <static.key>='<value>', <dynamic.key>='{{<value>}}', @payload('<payload mapping>') ) ) define stream <stream name> (<attribute1> <type>, <attributeN> <type>); Dynamic Properties The sink and sink mapper properties that are categorized as dynamic have the ability to absorb attribute values dynamically from the Siddhi events of their associated streams. This can be configured by enclosing the relevant attribute names in double curly braces as {{...}} , and using it within the property values. Some valid dynamic properties values are: '{{attribute1}}' 'This is {{attribute1}}' {{attribute1}} > {{attributeN}} Here the attribute names in the double curly braces will be replaced with the values from the events before they are published. This syntax includes the following annotations. Sink The type parameter of the @sink annotation defines the sink type that publishes the events. The other parameters of the @sink annotation depends upon the selected sink type, and here some of its parameters can be optional and/or dynamic. For detailed information about the supported parameters see documentation of the relevant sink. The following is a list of sink types supported by Siddhi: Source type Description In-memory Allow SiddhiApp to publish events to other SiddhiApps running on the same JVM. Log Logs the events appearing on the streams. HTTP Publish events to an HTTP endpoint. Kafka Publish events to Kafka topic. TCP Publish events to a TCP service. Email Send emails via SMTP protocols. JMS Publish events to JMS topics or queues. File Writes events to files. Prometheus Expose data through Prometheus agent.","title":"Sink"},{"location":"docs/query-guide/#distributed-sink","text":"Distributed Sinks publish events from a defined stream to multiple endpoints using load balancing or partitioning strategies. Any sink can be used as a distributed sink. A distributed sink configuration allows users to define a common mapping to convert and send the Siddhi events for all its destination endpoints. Purpose Distributed sink provides a way to publish Siddhi events to multiple endpoints in the configured event format. Syntax To configure distributed sink add the sink configuration to a stream definition by adding the @sink annotation and add the configuration parameters that are common of all the destination endpoints inside it, along with the common parameters also add the @distribution annotation specifying the distribution strategy (i.e. roundRobin or partitioned ) and @destination annotations providing each endpoint specific configurations. The distributed sink syntax is as follows: RoundRobin Distributed Sink Publishes events to defined destinations in a round robin manner. @sink(type='<sink type>', <common.static.key>='<value>', <common.dynamic.key>='{{<value>}}', @map(type='<map type>', <static.key>='<value>', <dynamic.key>='{{<value>}}', @payload('<payload mapping>') ) @distribution(strategy='roundRobin', @destination(<destination.specific.key>='<value>'), @destination(<destination.specific.key>='<value>'))) ) define stream <stream name> (<attribute1> <type>, <attributeN> <type>); Partitioned Distributed Sink Publishes events to defined destinations by partitioning them based on the partitioning key. @sink(type='<sink type>', <common.static.key>='<value>', <common.dynamic.key>='{{<value>}}', @map(type='<map type>', <static.key>='<value>', <dynamic.key>='{{<value>}}', @payload('<payload mapping>') ) @distribution(strategy='partitioned', partitionKey='<partition key>', @destination(<destination.specific.key>='<value>'), @destination(<destination.specific.key>='<value>'))) ) define stream <stream name> (<attribute1> <type>, <attributeN> <type>);","title":"Distributed Sink"},{"location":"docs/query-guide/#sink-mapper","text":"Each @sink configuration can have a mapping denoted by the @map annotation that defines how to convert Siddhi events to outgoing messages with the defined format. The type parameter of the @map defines the map type to be used in converting the outgoing events. The other parameters of @map annotation depends on the mapper selected, and some of its parameters can be optional and/or dynamic. For detailed information about the parameters see the documentation of the relevant mapper. Map Payload @payload is an optional annotation used with @map to define custom mapping. When the @payload annotation is not provided, each mapper maps the outgoing events to its own default event format. The @payload annotation allow users to configure mappers to produce the output payload of their choice, and by using dynamic properties within the payload they can selectively extract and add data from the published Siddhi events. There are two ways you to configure @payload annotation. Some mappers such as XML , JSON , and Test only accept one output payload: @payload( 'This is a test message from {{user}}.') Some mappers such key-value accept series of mapping values: @payload( key1='mapping_1', 'key2'='user : {{user}}') Here, the keys of payload mapping can be defined using the dot notation as a.b.c , or using any constant string value as '$abc' . Supported Sink Mapping Types The following is a list of sink mapping types supported by Siddhi: Sink mapping type Description PassThrough Omits data conversion on outgoing Siddhi events. JSON Converts Siddhi events to JSON messages. XML Converts Siddhi events to XML messages. TEXT Converts Siddhi events to plain text messages. Avro Converts Siddhi events to Avro Events. Binary Converts Siddhi events to Siddhi specific binary events. Key Value Converts Siddhi events to key-value HashMaps. CSV Converts Siddhi events to CSV like delimiter separated events. Tip When the @map annotation is not provided @map(type='passThrough') is used as default, that passes the outgoing Siddhi events directly to the sinks without any data conversion. PassThrough is the only sink mapper inbuilt in Siddhi, and all other sink mappers are implemented as extensions. Example 1 Publishes OutputStream events by converting them to JSON messages with the default format, and by sending to an HTTP endpoint http://localhost:8005/endpoint1 , using POST method, Accept header, and basic authentication having admin is both username and password. The configuration of the HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/endpoint', method='POST', headers='Accept-Date:20/02/2017', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json')) define stream OutputStream (name string, age int, country string); This will publish a JSON message on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } } Example 2 Publishes StockStream events by converting them to user defined JSON messages, and by sending to an HTTP endpoint http://localhost:8005/stocks . The configuration of the HTTP sink and custom JSON sink mapping to achieve the above is as follows. @sink(type='http', publisher.url='http://localhost:8005/stocks', @map(type='json', validate.json='true', enclosing.element='$.Portfolio', @payload(\"\"\"{\"StockData\":{ \"Symbol\":\"{{symbol}}\", \"Price\":{{price}} }}\"\"\"))) define stream StockStream (symbol string, price float, volume long); This will publish a single event as the JSON message on the following format: { \"Portfolio\":{ \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } } } This can also publish multiple events together as a JSON message on the following format: { \"Portfolio\":[ { \"StockData\":{ \"Symbol\":\"GOOG\", \"Price\":55.6 } }, { \"StockData\":{ \"Symbol\":\"FB\", \"Price\":57.0 } } ] } Example 3 Publishes events from the OutputStream stream to multiple the HTTP endpoints using a partitioning strategy. Here the events are sent to either http://localhost:8005/endpoint1 or http://localhost:8006/endpoint2 based on the partitioning key country . It uses default JSON mapping, POST method, and used admin as both the username and the password when publishing to both the endpoints. The configuration of the distributed HTTP sink and JSON sink mapper to achieve the above is as follows. @sink(type='http', method='POST', basic.auth.enabled='true', basic.auth.username='admin', basic.auth.password='admin', @map(type='json'), @distribution(strategy='partitioned', partitionKey='country', @destination(publisher.url='http://localhost:8005/endpoint1'), @destination(publisher.url='http://localhost:8006/endpoint2'))) define stream OutputStream (name string, age int, country string); This will partition the outgoing events and publish all events with the same country attribute value to the same endpoint. The JSON message published will be on the following format: { \"event\":{ \"name\":\"Paul\", \"age\":20, \"country\":\"UK\" } }","title":"Sink Mapper"},{"location":"docs/query-guide/#error-handling","text":"Errors in Siddhi can be handled at the Streams and in Sinks. Error Handling at Stream When errors are thrown by Siddhi elements subscribed to the stream, the error gets propagated up to the stream that delivered the event to those Siddhi elements. By default the error is logged and dropped at the stream, but this behavior can be altered by by adding @OnError annotation to the corresponding stream definition. @OnError annotation can help users to capture the error and the associated event, and handle them gracefully by sending them to a fault stream. The @OnError annotation and the required action to be specified as bellow. @OnError(action='on error action') define stream <stream name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ); The action parameter of the @OnError annotation defines the action to be executed during failure scenarios. The following actions can be specified to @OnError annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when @OnError annotation is not defined. STREAM : Creates a fault stream and redirects the event and the error to it. The created fault stream will have all the attributes defined in the base stream to capture the error causing event, and in addition it also contains _error attribute of type object to containing the error information. The fault stream can be referred by adding ! in front of the base stream name as !<stream name> . Example Handle errors in TempStream by redirecting the errors to a fault stream. The configuration of TempStream stream and @OnError annotation is as follows. @OnError(name='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); Siddhi will infer and automatically defines the fault stream of TempStream as given bellow. define stream !TempStream (deviceID long, roomNo int, temp double, _error object); The SiddhiApp extending the above the use-case by adding failure generation and error handling with the use of queries is as follows. Note: Details on writing processing logics via queries will be explained in later sections. -- Define fault stream to handle error occurred at TempStream subscribers @OnError(name='STREAM') define stream TempStream (deviceID long, roomNo int, temp double); -- Error generation through a custom function `createError()` @name('error-generation') from TempStream#custom:createError() insert into IgnoreStream1; -- Handling error by simply logging the event and error. @name('handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream2; Error Handling at Sink There can be cases where external systems becoming unavailable or coursing errors when the events are published to them. By default sinks log and drop the events causing event losses, and this can be handled gracefully by configuring on.error parameter of the @sink annotation. The on.error parameter of the @sink annotation can be specified as bellow. @sink(type='<sink type>', on.error='<on error action>', <key>='<value>', ...) define stream <stream name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ); The following actions can be specified to on.error parameter of @sink annotation to handle erroneous scenarios. LOG : Logs the event with the error, and drops the event. This is the default action performed even when on.error parameter is not defined on the @sink annotation. WAIT : Publishing threads wait in back-off and re-trying mode, and only send the events when the connection is re-established. During this time the threads will not consume any new messages causing the systems to introduce back pressure on the systems that publishes to it. STREAM : Pushes the failed events with the corresponding error to the associated fault stream the sink belongs to. Example 1 Introduce back pressure on the threads who bring events via TempStream when the system cannot connect to Kafka. The configuration of TempStream stream and @sink Kafka annotation with on.error property is as follows. @sink(type='kafka', on.error='WAIT', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); Example 2 Send events to the fault stream of TempStream when the system cannot connect to Kafka. The configuration of TempStream stream with associated fault stream, @sink Kafka annotation with on.error property and a queries to handle the error is as follows. Note: Details on writing processing logics via queries will be explained in later sections. @OnError(name='STREAM') @sink(type='kafka', on.error='STREAM', topic='{{roomNo}}', bootstrap.servers='localhost:9092', @map(type='xml')) define stream TempStream (deviceID long, roomNo int, temp double); -- Handling error by simply logging the event and error. @name('handle-error') from !TempStream#log(\"Error Occurred!\") select deviceID, roomNo, temp, _error insert into IgnoreStream;","title":"Error Handling"},{"location":"docs/query-guide/#query","text":"Query defines the processing logic in Siddhi. It consumes events from one or more streams, named-windows , tables , and/or named-aggregations , process the events in a streaming manner, and generate output events into a stream , named-window , or table . Purpose A query provides a way to process the events in the order they arrive and produce output using both stateful and stateless complex event processing and stream processing operations. Syntax The high level query syntax for defining processing logics is as follows: @name('<query name>') from <input> <projection> <output action> The following parameters are used to configure a stream definition. Parameter Description query name The name of the query. Since naming the query (i.e the @name('<query name>') annotation) is optional, when the name is not provided Siddhi assign a system generated name for the query. input Defines the means of event consumption via streams , named-windows , tables , and/or named-aggregations , and defines the processing logic using filters , windows , stream-functions , joins , patterns and sequences . projection Generates output event attributes using select , functions , aggregation-functions , and group by operations, and filters the generated the output using having , limit & offset , order by , and output rate limiting operations before sending them out. Here the projection is optional and when it is omitted all the input events will be sent to the output as it is. output action Defines output action (such as insert into , update , delete , etc) that needs to be performed by the generated events on a stream , named-window , or table Example A query consumes events from the TempStream stream and output only the roomNo and temp attributes to the RoomTempStream stream, from which another query consumes the events and sends all its attributes to AnotherRoomTempStream stream. define stream TempStream (deviceID long, roomNo int, temp double); from TempStream select roomNo, temp insert into RoomTempStream; from RoomTempStream insert into AnotherRoomTempStream; Inferred Stream Here, the RoomTempStream and AnotherRoomTempStream streams are an inferred streams, which means their stream definitions are inferred from the queries and they can be used same as any other defined stream without any restrictions.","title":"Query"},{"location":"docs/query-guide/#value","text":"Values are typed data, that can be manipulated, transferred and stored. Values can be referred by the attributes defined in definitions such as streams, and tables. Siddhi supports values of type STRING , INT (Integer), LONG , DOUBLE , FLOAT , BOOL (Boolean) and OBJECT . The syntax of each type and their example use as a constant value is as follows, Attribute Type Format Example int <digit>+ 123 , -75 , +95 long <digit>+L 123000L , -750l , +154L float (<digit>+)?('.'<digit>*)? (E(-|+)?<digit>+)?F 123.0f , -75.0e-10F , +95.789f double (<digit>+)?('.'<digit>*)? (E(-|+)?<digit>+)?D? 123.0 , 123.0D , -75.0e-10D , +95.789d bool (true|false) true , false , TRUE , FALSE string '(<char>* !('|\"|\"\"\"|<new line>))' or \"(<char>* !(\"|\"\"\"|<new line>))\" or \"\"\"(<char>* !(\"\"\"))\"\"\" 'Any text.' , \"Text with 'single' quotes.\" , \"\"\" Text with 'single' quotes, \"double\" quotes, and new lines. \"\"\" Time Time is a special type of LONG value that denotes time using digits and their unit in the format (<digit>+ <unit>)+ . At execution, the time gets converted into milliseconds and returns a LONG value. Unit Syntax Year year | years Month month | months Week week | weeks Day day | days Hour hour | hours Minutes minute | minutes | min Seconds second | seconds | sec Milliseconds millisecond | milliseconds Example 1 hour and 25 minutes can by written as 1 hour and 25 minutes which is equal to the LONG value 5100000 .","title":"Value"},{"location":"docs/query-guide/#select","text":"The select clause in Siddhi query defines the output event attributes of the query. Following are some basic query projection operations supported by select. Action Description Select specific attributes for projection Only select some of the input attributes as query output attributes. E.g., Select and output only roomNo and temp attributes from the TempStream stream. from TempStream select roomNo, temp insert into RoomTempStream; Select all attributes for projection Select all input attributes as query output attributes. This can be done by using asterisk ( * ) or by omitting the select clause itself. E.g., Both following queries select all attributes of TempStream input stream and output all attributes to NewTempStream stream. from TempStream select * insert into NewTempStream; or from TempStream insert into NewTempStream; Name attribute Provide a unique name for each output attribute generated by the query. This can help to rename the selected input attributes or assign an attribute name to a projection operation such as function, aggregate-function, mathematical operation, etc, using as keyword. E.g., Query that renames input attribute temp to temperature and function currentTimeMillis() as time . from TempStream select roomNo, temp as temperature, currentTimeMillis() as time insert into RoomTempStream; Constant values as attributes Creates output attributes with a constant value. Any constant value of type STRING , INT , LONG , DOUBLE , FLOAT , BOOL , and time as given in the values section can be defined. E.g., Query specifying 'C' as the constant value for the scale attribute. from TempStream select roomNo, temp, 'C' as scale insert into RoomTempStream; Mathematical and logical expressions in attributes Defines the mathematical and logical operations that need to be performed to generating output attribute values. These expressions are executed in the precedence order given below. Operator precedence Operator Distribution Example () Scope (cost + tax) * 0.05 IS NULL Null check deviceID is null NOT Logical NOT not (price > 10) * , / , % Multiplication, division, modulus temp * 9/5 + 32 + , - Addition, subtraction temp * 9/5 - 32 < , < = , > , >= Comparators: less-than, greater-than-equal, greater-than, less-than-equal totalCost >= price * quantity == , != Comparisons: equal, not equal totalCost != price * quantity IN Checks if value exist in the table roomNo in ServerRoomsTable AND Logical AND temp < 40 and humidity < 40 OR Logical OR humidity < 40 or humidity >= 60 E.g., Query converts temperature from Celsius to Fahrenheit, and identifies rooms with room number between 10 and 15 as server rooms. from TempStream select roomNo, temp * 9/5 + 32 as temp, 'F' as scale, roomNo > 10 and roomNo < 15 as isServerRoom insert into RoomTempStream;","title":"Select"},{"location":"docs/query-guide/#function","text":"Function are pre-configured operations that can consumes zero, or more parameters and always produce a single value as result. It can be used anywhere an attribute can be used. Purpose Functions encapsulate pre-configured reusable execution logic allowing users to execute the logic anywhere just by calling the function. This also make writing SiddhiApps simple and easy to understand. Syntax The syntax of function is as follows, <function name>( <parameter>* ) Here <function name> uniquely identifies the function. The <parameter> defined input parameters the function can accept. The input parameters can be attributes, constant values, results of other functions, results of mathematical or logical expressions, or time values. The number and type of parameters a function accepts depend on the function itself. Note Functions, mathematical expressions, and logical expressions can be used in a nested manner. Example 1 Function name add accepting two input parameters, is called with an attribute named input and a constant value 75 . add(input, 75) Example 2 Function name alertAfter accepting two input parameters, is called with a time value of 1 hour and 25 minutes and a mathematical addition operation of startTime + 56 . add(1 hour and 25 minutes, startTime + 56) Inbuilt functions Following are some inbuilt Siddhi functions, for more functions refer execution extensions . Inbuilt function Description eventTimestamp Returns event's timestamp. currentTimeMillis Returns current time of SiddhiApp runtime. default Returns a default value if the parameter is null. ifThenElse Returns parameters based on a conditional parameter. UUID Generates a UUID. cast Casts parameter type. convert Converts parameter type. coalesce Returns first not null input parameter. maximum Returns the maximum value of all parameters. minimum Returns the minimum value of all parameters. instanceOfBoolean Checks if the parameter is an instance of Boolean. instanceOfDouble Checks if the parameter is an instance of Double. instanceOfFloat Checks if the parameter is an instance of Float. instanceOfInteger Checks if the parameter is an instance of Integer. instanceOfLong Checks if the parameter is an instance of Long. instanceOfString Checks if the parameter is an instance of String. createSet Creates HashSet with given input parameters. sizeOfSet Returns number of items in the HashSet, that's passed as a parameter. Example Query that converts the roomNo to string using convert function, finds the maximum temperature reading with maximum function, and adds a unique messageID using the UUID function. from TempStream select convert(roomNo, 'string') as roomNo, maximum(tempReading1, tempReading2) as temp, UUID() as messageID insert into RoomTempStream;","title":"Function"},{"location":"docs/query-guide/#filter","text":"Filters provide a way of filtering input stream events based on a specified condition. It accepts any type of condition including a combination of functions and/or attributes that produces a Boolean result. Filters allow events to passthrough if the condition results in true , and drops if it results in a false . Purpose Filter helps to select the events that are relevant for the processing and omit the ones that are not. Syntax Filter conditions should be defined in square brackets ( [] ) next to the input stream as shown below. from <input stream>[<filter condition>] select <attribute name>, <attribute name>, ... insert into <output stream> Example Query to filter TempStream stream events, having roomNo within the range of 100-210 and temperature greater than 40 degrees, and insert them into HighTempStream stream. from TempStream[(roomNo >= 100 and roomNo < 210) and temp > 40] select roomNo, temp insert into HighTempStream;","title":"Filter"},{"location":"docs/query-guide/#window","text":"Window provides a way to capture a subset of events from an input stream and retain them for a period of time based on a specified criterion. The criterion defines when and how the events should be evicted from the windows. Such as events getting evicted from the window based on the time duration, or number of events and they events are evicted in a sliding (one by one) or tumbling (batch) manner. Within a query, each input stream can at most have only one window associated with it. Purpose Windows help to retain events based on a criterion, such that the values of those events can be aggregated, or checked if an event of interest is within the window or not. Syntax Window should be defined by using the #window prefix next to the input stream as shown below. from <input stream>#window.<window name>(<parameter>, <parameter>, ... ) select <attribute name>, <attribute name>, ... insert <ouput event type>? into <output stream> Note Filter conditions can be applied both before and/or after the window. Inbuilt windows Following are some inbuilt Siddhi windows, for more windows refer execution extensions . Inbuilt function Description time Retains events based on time in a sliding manner. timeBatch Retains events based on time in a tumbling/batch manner. length Retains events based on number of events in a sliding manner. lengthBatch Retains events based on number of events in a tumbling/batch manner. timeLength Retains events based on time and number of events in a sliding manner. session Retains events for each session based on session key. batch Retains events of last arrived event chunk. sort Retains top-k or bottom-k events based on a parameter value. cron Retains events based on cron time in a tumbling/batch manner. externalTime Retains events based on event time value passed as a parameter in a sliding manner. externalTimeBatch Retains events based on event time value passed as a parameter in a a tumbling/batch manner. delay Retains events and delays the output by the given time period in a sliding manner. Example 1 Query to find out the maximum temperature out of the last 10 events , using the window of length 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.length(10) select max(temp) as maxTemp insert into MaxTempStream; Here, the length window operates in a sliding manner where the following 3 event subsets are calculated and outputted when a list of 12 events are received in sequential order. Subset Event Range 1 1 - 10 2 2 - 11 3 3 - 12 Example 2 Query to find out the maximum temperature out of the every 10 events , using the window of lengthBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.lengthBatch(10) select max(temp) as maxTemp insert into MaxTempStream; Here, the window operates in a batch/tumbling manner where the following 3 event subsets are calculated and outputted when a list of 30 events are received in a sequential order. Subset Event Range 1 1 - 10 2 11 - 20 3 21 - 30 Example 3 Query to find out the maximum temperature out of the events arrived during last 10 minutes , using the window of time 10 minutes and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.time(10 min) select max(temp) as maxTemp insert into MaxTempStream; Here, the time window operates in a sliding manner with millisecond accuracy, where it will process events in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:00:01.001 - 1:10:01.000 3 1:00:01.033 - 1:10:01.034 Example 4 Query to find out the maximum temperature out of the events arriving every 10 minutes , using the window of timeBatch 10 and max() aggregation function, from the TempStream stream and insert the results into the MaxTempStream stream. from TempStream#window.timeBatch(10 min) select max(temp) as maxTemp insert into MaxTempStream; Here, the window operates in a batch/tumbling manner where the window will process evetns in the following 3 time durations and output aggregated events when a list of events are received in a sequential order. Subset Time Range (in ms) 1 1:00:00.001 - 1:10:00.000 2 1:10:00.001 - 1:20:00.000 3 1:20:00.001 - 1:30:00.000","title":"Window"},{"location":"docs/query-guide/#event-type","text":"Query output depends on the current and expired event types it produces based on its internal processing state. By default all queries produce current events upon event arrival to the query. The queries containing windows additionally produce expired events when events expire from the windows. Purpose Event type helps to specify when a query should output events to the stream, such as output upon current events, expired events or upon both current and expired events. Syntax Event type should be defined in between insert and into keywords for insert queries as follows. from <input stream>#window.<window name>(<parameter>, <parameter>, ... ) select <attribute name>, <attribute name>, ... insert <event type> into <output stream> Event type should be defined next to the for keyword for delete queries as follows. from <input stream>#window.<window name>(<parameter>, <parameter>, ... ) select <attribute name>, <attribute name>, ... delete <table> (for <event type>)? on <condition> Event type should be defined next to the for keyword for update queries as follows. from <input stream>#window.<window name>(<parameter>, <parameter>, ... ) select <attribute name>, <attribute name>, ... update <table> (for <event type>)? set <table>.<attribute name> = (<attribute name>|<expression>)?, <table>.<attribute name> = (<attribute name>|<expression>)?, ... on <condition> Event type should be defined next to the for keyword for update or insert queries as follows. from <input stream>#window.<window name>(<parameter>, <parameter>, ... ) select <attribute name>, <attribute name>, ... update or insert into <table> (for <event type>)? set <table>.<attribute name> = <expression>, <table>.<attribute name> = <expression>, ... on <condition> Note Controlling query output based on the event types neither alters query execution nor its accuracy. The event types can be defined using the following keywords to manipulate query output. Event types Description current events Outputs events only when incoming events arrive to be processed by the query. This is default behavior when no specific event type is specified. expired events Outputs events only when events expires from the window. all events Outputs events when incoming events arrive to be processed by the query as well as when events expire from the window. Example Query to output only the expired events from a 1 minute time window to the DelayedTempStream stream. This can be used for delaying the events by a minute. from TempStream#window.time(1 min) select * insert expired events into DelayedTempStream Note This is just to illustrate how expired events work, it is recommended to use delay window for usecases where we need to delay events by a given time period.","title":"Event Type"},{"location":"docs/query-guide/#aggregate-function","text":"Aggregate functions are pre-configured aggregation operations that can consumes zero, or more parameters from multiple events and always produce a single value as result. They can be only used in the query projection (as part of the select clause). When a query comprises a window, the aggregation will be contained to the events in the window, and when it does not have a window, the aggregation is performed from the first event the query has received. Purpose Aggregate functions encapsulate pre-configured reusable aggregate logic allowing users to aggregate values of multiple events together. When used with batch/tumbling windows this can also help to reduce the number of output events produced. Syntax Aggregate function can be used in query projection (as part of the select clause) alone or as a part of another expression. In all cases, the output produced by the query should be properly mapped to the output stream attribute using the as keyword. The syntax of aggregate function is as follows, from <input stream>#window.<window name>(<parameter>, <parameter>, ... ) select <aggregate function>(<parameter>, <parameter>, ... ) as <attribute name>, <attribute2 name>, ... insert into <output stream>; Here <aggregate function> uniquely identifies the aggregate function. The <parameter> defined input parameters the aggregate function can accept. The input parameters can be attributes, constant values, results of other functions or aggregate functions, results of mathematical or logical expressions, or time values. The number and type of parameters an aggregate function accepts depend on the function itself. Inbuilt aggregate functions Following are some inbuilt aggregation functions, for more functions refer execution extensions . Inbuilt aggregate function Description sum Calculates the sum from a set of values. count Calculates the count from a set of values. distinctCount Calculates the distinct count based on a parameter from a set of values. avg Calculates the average from a set of values. max Finds the maximum value from a set of values. max Finds the minimum value from a set of values. | maxForever | Finds the maximum value from all events throughout its lifetime irrespective of the windows. | | minForever | Finds the minimum value from all events throughout its lifetime irrespective of the windows. | | stdDev | Calculates the standard deviation from a set of values. | | and | Calculates boolean and from a set of values. | | or | Calculates boolean or from a set of values. | | unionSet | Calculates union as a Set from a set of values. | Example Query to calculate average, maximum, and minimum values on temp attribute of the TempStream stream in a sliding manner, from the events arrived over the last 10 minutes and to produce outputs avgTemp , maxTemp and minTemp respectively to the AvgTempStream output stream. from TempStream#window.time(10 min) select avg(temp) as avgTemp, max(temp) as maxTemp, min(temp) as minTemp insert into AvgTempStream;","title":"Aggregate Function"},{"location":"docs/query-guide/#group-by","text":"Group By provides a way of grouping events based on one or more specified attributes to perform aggregate operations. Purpose Group By allows users to aggregate values of multiple events based on the given group-by fields. Syntax The syntax for the Group By with aggregate function is as follows. from <input stream>#window.<window name>(...) select <aggregate function>( <parameter>, <parameter>, ...) as <attribute1 name>, <attribute2 name>, ... group by <attribute1 name>, <attribute2 name>, ... insert into <output stream>; Here the group by attributes should be defined next to the group by keyword separating each attribute by a comma. Example Query to calculate the average temp per roomNo and deviceID combination, from the events arrived from TempStream stream, during the last 10 minutes time-window in a sliding manner. from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID insert into AvgTempStream;","title":"Group By"},{"location":"docs/query-guide/#having","text":"Having provide a way of filtering events based on a specified condition of the query output stream attributes. It accepts any type of condition including a combination of functions and/or attributes that produces a Boolean result. Having, allow events to passthrough if the condition results in true , and drops if it results in a false . Purpose Having helps to select the events that are relevant for the output based on the attributes those are produced by the select clause and omit the ones that are not. Syntax The syntax for the Having clause is as follows. from <input stream>#window.<window name>( ... ) select <aggregate function>( <parameter>, <parameter>, ...) as <attribute1 name>, <attribute2 name>, ... group by <attribute1 name>, <attribute2 name> ... having <condition> insert into <output stream>; Here the having <condition> should be defined next to the having keyword and having can be used with or without group by clause. Example Query to calculate the average temp per roomNo for the last 10 minutes, and alerts if the avgTemp exceeds 30 degrees. from TempStream#window.time(10 min) select roomNo, avg(temp) as avgTemp group by roomNo having avgTemp > 30 insert into AlertStream;","title":"Having"},{"location":"docs/query-guide/#order-by","text":"Order By, orders the query results in ascending and or descending order based on one or more specified attributes. When an attribute is used for order by, by default Siddhi orders the events in ascending order of that attribute's value, and by adding desc keyword, the events can be ordered in descending order. When more than one attribute is defined the attributes defined towards the left will have more precedence in ordering than the ones defined in right. Purpose Order By helps to sort the events in the outputs chunks produced by the query. Order By will be more helpful for batch windows, and queries where they output many of event together then for sliding window use cases where the output will be one or few events at a time. Syntax The syntax for the Order By clause is as follows: from <input stream>#window.<window name>( ... ) select <aggregate function>( <parameter>, <parameter>, ...) as <attribute1 name>, <attribute2 name>, ... group by <attribute1 name>, <attribute2 name> ... having <condition> order by <attribute1 name> (asc|desc)?, <attribute2 name> (asc|desc)?, ... insert into <output stream>; Here the order by attributes should be defined next to the order by keyword separating each by a comma, and optionally specifying the event ordering using asc (default) or desc keywords. Example Query to calculate the average temp per roomNo and deviceID combination on every 10 minutes batches, and order the generated output events in ascending order by avgTemp and then by descending order of roomNo (if the more than one event have the same avgTemp value). from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp, roomNo desc insert into AvgTempStream;","title":"Order By"},{"location":"docs/query-guide/#limit-offset","text":"These provide a way to select the number of events (via limit) from the desired index (by specifying an offset) from the output event chunks produced by the query. Purpose Limit & Offset helps to output only the selected set of events from large event batches. This will be more useful with Order By clause where one can order the output for topK, bottomK, or even to paginate through the dataset by obtaining a set of events from the middle. Syntax The syntax for the Limit & Offset clauses is as follows: from <input stream>#window.<window name>( ... ) select <aggregate function>( <parameter>, <parameter>, ...) as <attribute1 name>, <attribute2 name>, ... group by <attribute1 name>, <attribute2 name> ... having <condition> order by <attribute1 name> (asc | desc)?, <attribute2 name> (<ascend/descend>)?, ... limit <positive integer>? offset <positive integer>? insert into <output stream>; Here both limit and offset are optional, when limit is omitted the query will output all the events, and when offset is omitted 0 is taken as the default offset value. Example 1 Query to calculate the average temp per roomNo and deviceID combination for every 10 minutes batches, from the events arriving at the TempStream stream, and emit only two events having the highest avgTemp value. from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp desc limit 2 insert into HighestAvgTempStream; Example 2 Query to calculate the average temp per roomNo and deviceID combination for every 10 minutes batches, for events that arriving at the TempStream stream, and emits only the third, forth and fifth events when sorted in descending order based on their avgTemp value. from TempStream#window.timeBatch(10 min) select roomNo, deviceID, avg(temp) as avgTemp group by roomNo, deviceID order by avgTemp desc limit 3 offset 2 insert into HighestAvgTempStream;","title":"Limit &amp; Offset"},{"location":"docs/query-guide/#join-stream","text":"Joins allow you to get a combined result from two streams in real-time based on a specified condition. Purpose Streams are stateless. Therefore, in order to join two streams, they need to be connected to a window so that there is a pool of events that can be used for joining. Joins also accept conditions to join the appropriate events from each stream. During the joining process each incoming event of each stream is matched against all the events in the other stream's window based on the given condition, and the output events are generated for all the matching event pairs. Note Join can also be performed with stored data , aggregation or externally named windows . Syntax The syntax for a join is as follows: from <input stream>#window.<window name>(<parameter>, ... ) {unidirectional} {as <reference>} join <input stream>#window.<window name>(<parameter>, ... ) {unidirectional} {as <reference>} on <join condition> select <attribute name>, <attribute name>, ... insert into <output stream> Here, the <join condition> allows you to match the attributes from both the streams. Unidirectional join operation By default, events arriving at either stream can trigger the joining process. However, if you want to control the join execution, you can add the unidirectional keyword next to a stream in the join definition as depicted in the syntax in order to enable that stream to trigger the join operation. Here, events arriving at other stream only update the window of that stream, and this stream does not trigger the join operation. Note The unidirectional keyword cannot be applied to both the input streams because the default behaviour already allows both streams to trigger the join operation. Example Assuming that the temperature of regulators are updated every minute. Following is a Siddhi App that controls the temperature regulators if they are not already on for all the rooms with a room temperature greater than 30 degrees. define stream TempStream(deviceID long, roomNo int, temp double); define stream RegulatorStream(deviceID long, roomNo int, isOn bool); from TempStream[temp > 30.0]#window.time(1 min) as T join RegulatorStream[isOn == false]#window.length(1) as R on T.roomNo == R.roomNo select T.roomNo, R.deviceID, 'start' as action insert into RegulatorActionStream; Supported join types Following are the supported operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join both the streams. The output is generated only if there is a matching event in both the streams. Left outer join The left outer join operation allows you to join two streams to be merged based on a condition. left outer join is used as the keyword to join both the streams. Here, it returns all the events of left stream even if there are no matching events in the right stream by having null values for the attributes of the right stream. Example The following query generates output events for all events from the StockStream stream regardless of whether a matching symbol exists in the TwitterStream stream or not. from StockStream#window.time(1 min) as S left outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ; Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join both the streams. It returns all the events of the right stream even if there are no matching events in the left stream. Full outer join The full outer join combines the results of left outer join and right outer join. full outer join is used as the keyword to join both the streams. Here, output event are generated for each incoming event even if there are no matching events in the other stream. Example The following query generates output events for all the incoming events of each stream regardless of whether there is a match for the symbol attribute in the other stream or not. from StockStream#window.time(1 min) as S full outer join TwitterStream#window.length(1) as T on S.symbol== T.symbol select S.symbol as symbol, T.tweet, S.price insert into outputStream ;","title":"Join (Stream)"},{"location":"docs/query-guide/#pattern","text":"This is a state machine implementation that allows you to detect patterns in the events that arrive over time. This can correlate events within a single stream or between multiple streams. Purpose Patterns allow you to identify trends in events over a time period. Syntax The following is the syntax for a pattern query: from (every)? <event reference>=<input stream>[<filter condition>] -> (every)? <event reference>=<input stream [<filter condition>] -> ... (within <time gap>)? select <event reference>.<attribute name>, <event reference>.<attribute name>, ... insert into <output stream> Items Description -> This is used to indicate an event that should be following another event. The subsequent event does not necessarily have to occur immediately after the preceding event. The condition to be met by the preceding event should be added before the sign, and the condition to be met by the subsequent event should be added after the sign. <event reference> This allows you to add a reference to the the matching event so that it can be accessed later for further processing. (within <time gap>)? The within clause is optional. It defines the time duration within which all the matching events should occur. every every is an optional keyword. This defines whether the event matching should be triggered for every event arrival in the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. Siddhi also supports pattern matching with counting events and matching events in a logical order such as ( and , or , and not ). These are described in detail further below in this guide. Example This query sends an alert if the temperature of a room increases by 5 degrees within 10 min. from every( e1=TempStream ) -> e2=TempStream[ e1.roomNo == roomNo and (e1.temp + 5) <= temp ] within 10 min select e1.roomNo, e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Here, the matching process begins for each event in the TempStream stream (because every is used with e1=TempStream ), and if another event arrives within 10 minutes with a value for the temp attribute that is greater than or equal to e1.temp + 5 of the event e1, an output is generated via the AlertStream .","title":"Pattern"},{"location":"docs/query-guide/#counting-pattern","text":"Counting patterns allow you to match multiple events that may have been received for the same matching condition. The number of events matched per condition can be limited via condition postfixes. Syntax Each matching condition can contain a collection of events with the minimum and maximum number of events to be matched as shown in the syntax below. from (every)? <event reference>=<input stream>[<filter condition>] (<<min count>:<max count>>)? -> ... (within <time gap>)? select <event reference>([event index])?.<attribute name>, ... insert into <output stream> Postfix Description Example <n1:n2> This matches n1 to n2 events (including n1 and not more than n2 ). 1:4 matches 1 to 4 events. <n:> This matches n or more events (including n ). <2:> matches 2 or more events. <:n> This matches up to n events (excluding n ). <:5> matches up to 5 events. <n> This matches exactly n events. <5> matches exactly 5 events. Specific occurrences of the event in a collection can be retrieved by using an event index with its reference. Square brackets can be used to indicate the event index where 1 can be used as the index of the first event and last can be used as the index for the last available event in the event collection. If you provide an index greater then the last event index, the system returns null . The following are some valid examples. e1[3] refers to the 3 rd event. e1[last] refers to the last event. e1[last - 1] refers to the event before the last event. Example The following Siddhi App calculates the temperature difference between two regulator events. define stream TempStream (deviceID long, roomNo int, temp double); define stream RegulatorStream (deviceID long, roomNo int, tempSet double, isOn bool); from every( e1=RegulatorStream) -> e2=TempStream[e1.roomNo==roomNo]<1:> -> e3=RegulatorStream[e1.roomNo==roomNo] select e1.roomNo, e2[0].temp - e2[last].temp as tempDiff insert into TempDiffStream;","title":"Counting Pattern"},{"location":"docs/query-guide/#logical-patterns","text":"Logical patterns match events that arrive in temporal order and correlate them with logical relationships such as and , or and not . Syntax from (every)? (not)? <event reference>=<input stream>[<filter condition>] ((and|or) <event reference>=<input stream>[<filter condition>])? (within <time gap>)? -> ... select <event reference>([event index])?.<attribute name>, ... insert into <output stream> Keywords such as and , or , or not can be used to illustrate the logical relationship. Key Word Description and This allows both conditions of and to be matched by two events in any order. or The state succeeds if either condition of or is satisfied. Here the event reference of the other condition is null . not <condition1> and <condition2> When not is included with and , it identifies the events that match arriving before any event that match . not <condition> for <time period> When not is included with for , it allows you to identify a situation where no event that matches <condition1> arrives during the specified <time period> . e.g., from not TemperatureStream[temp > 60] for 5 sec . Here the not pattern can be followed by either an and clause or the effective period of not can be concluded after a given <time period> . Further in Siddhi more than two streams cannot be matched with logical conditions using and , or , or not clauses at this point.","title":"Logical Patterns"},{"location":"docs/query-guide/#detecting-non-occurring-events","text":"Siddhi allows you to detect non-occurring events via multiple combinations of the key words specified above as shown in the table below. In the patterns listed, P* can be either a regular event pattern, an absent event pattern or a logical pattern. Pattern Detected Scenario not A for <time period> The non-occurrence of event A within <time period> after system start up. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, to indicate that the passenger might be in danger. not A for <time period> and B After system start up, event A does not occur within time period , but event B occurs at some point in time. e.g., Generating an alert if a taxi has not reached its destination within 30 minutes, and the passenger marked that he/she is in danger at some point in time. not A for <time period 1> and not B for <time period 2> After system start up, event A doess not occur within time period 1 , and event B also does not occur within <time period 2> . e.g., Generating an alert if the driver of a taxi has not reached the destination within 30 minutes, and the passenger has not marked himself/herself to be in danger within that same time period. not A for <time period> or B After system start up, either event A does not occur within <time period> , or event B occurs at some point in time. e.g., Generating an alert if the taxi has not reached its destination within 30 minutes, or if the passenger has marked that he/she is in danger at some point in time. not A for <time period 1> or not B for <time period 2> After system start up, either event A does not occur within <time period 1> , or event B occurs within <time period 2> . e.g., Generating an alert to indicate that the driver is not on an expected route if the taxi has not reached destination A within 20 minutes, or reached destination B within 30 minutes. A \u2192 not B for <time period> Event B does not occur within <time period> after the occurrence of event A. e.g., Generating an alert if the taxi has reached its destination, but this was not followed by a payment record. P* \u2192 not A for <time period> and B After the occurrence of P*, event A does not occur within <time period> , and event B occurs at some point in time. P* \u2192 not A for <time period 1> and not B for <time period 2> After the occurrence of P*, event A does not occur within <time period 1> , and event B does not occur within <time period 2> . P* \u2192 not A for <time period> or B After the occurrence of P*, either event A does not occur within <time period> , or event B occurs at some point in time. P* \u2192 not A for <time period 1> or not B for <time period 2> After the occurrence of P*, either event A does not occur within <time period 1> , or event B does not occur within <time period 2> . not A for <time period> \u2192 B Event A does occur within <time period> after the system start up, but event B occurs after that <time period> has elapsed. not A for <time period> and B \u2192 P* Event A does not occur within <time period> , and event B occurs at some point in time. Then P* occurs after the <time period> has elapsed, and after B has occurred. not A for <time period 1> and not B for <time period 2> \u2192 P* After system start up, event A does not occur within <time period 1> , and event B does not occur within <time period 2> . However, P* occurs after both A and B. not A for <time period> or B \u2192 P* After system start up, event A does not occur within <time period> or event B occurs at some point in time. The P* occurs after <time period> has elapsed, or after B has occurred. not A for <time period 1> or not B for <time period 2> \u2192 P* After system start up, either event A does not occur within <time period 1> , or event B does not occur within <time period 2> . Then P* occurs after both <time period 1> and <time period 2> have elapsed. not A and B Event A does not occur before event B. A and not B Event B does not occur before event A. Example Following Siddhi App, sends the stop control action to the regulator when the key is removed from the hotel room. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream RoomKeyStream(deviceID long, roomNo int, action string); from every( e1=RegulatorStateChangeStream[ action == 'on' ] ) -> e2=RoomKeyStream[ e1.roomNo == roomNo and action == 'removed' ] or e3=RegulatorStateChangeStream[ e1.roomNo == roomNo and action == 'off'] select e1.roomNo, ifThenElse( e2 is null, 'none', 'stop' ) as action having action != 'none' insert into RegulatorActionStream; This Siddhi Application generates an alert if we have switch off the regulator before the temperature reaches 12 degrees. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from e1=RegulatorStateChangeStream[action == 'start'] -> not TempStream[e1.roomNo == roomNo and temp < 12] and e2=RegulatorStateChangeStream[action == 'off'] select e1.roomNo as roomNo insert into AlertStream; This Siddhi Application generates an alert if the temperature does not reduce to 12 degrees within 5 minutes of switching on the regulator. define stream RegulatorStateChangeStream(deviceID long, roomNo int, tempSet double, action string); define stream TempStream (deviceID long, roomNo int, temp double); from e1=RegulatorStateChangeStream[action == 'start'] -> not TempStream[e1.roomNo == roomNo and temp < 12] for '5 min' select e1.roomNo as roomNo insert into AlertStream;","title":"Detecting Non-occurring Events"},{"location":"docs/query-guide/#sequence","text":"Sequence is a state machine implementation that allows you to detect the sequence of event occurrences over time. Here all matching events need to arrive consecutively to match the sequence condition, and there cannot be any non-matching events arriving within a matching sequence of events. This can correlate events within a single stream or between multiple streams. Purpose This allows you to detect a specified event sequence over a specified time period. Syntax The syntax for a sequence query is as follows: from (every)? <event reference>=<input stream>[<filter condition>], <event reference>=<input stream [<filter condition>], ... (within <time gap>)? select <event reference>.<attribute name>, <event reference>.<attribute name>, ... insert into <output stream> Items Description , This represents the immediate next event i.e., when an event that matches the first condition arrives, the event that arrives immediately after it should match the second condition. <event reference> This allows you to add a reference to the the matching event so that it can be accessed later for further processing. (within <time gap>)? The within clause is optional. It defines the time duration within which all the matching events should occur. every every is an optional keyword. This defines whether the matching event should be triggered for every event that arrives at the specified stream with the matching condition. When this keyword is not used, the matching is carried out only once. Example This query generates an alert if the increase in the temperature between two consecutive temperature events exceeds one degree. from every e1=TempStream, e2=TempStream[e1.temp + 1 < temp] select e1.temp as initialTemp, e2.temp as finalTemp insert into AlertStream; Counting Sequence Counting sequences allow you to match multiple events for the same matching condition. The number of events matched per condition can be limited via condition postfixes such as Counting Patterns , or by using the * , + , and ? operators. The matching events can also be retrieved using event indexes, similar to how it is done in Counting Patterns . Syntax Each matching condition in a sequence can contain a collection of events as shown below. from (every)? <event reference>=<input stream>[<filter condition>](+|*|?)?, <event reference>=<input stream [<filter condition>](+|*|?)?, ... (within <time gap>)? select <event reference>.<attribute name>, <event reference>.<attribute name>, ... insert into <output stream> Postfix symbol Required/Optional Description + Optional This matches one or more events to the given condition. * Optional This matches zero or more events to the given condition. ? Optional This matches zero or one events to the given condition. Example This Siddhi application identifies temperature peeks. define stream TempStream(deviceID long, roomNo int, temp double); from every e1=TempStream, e2=TempStream[e1.temp <= temp]+, e3=TempStream[e2[last].temp > temp] select e1.temp as initialTemp, e2[last].temp as peakTemp insert into PeekTempStream; Logical Sequence Logical sequences identify logical relationships using and , or and not on consecutively arriving events. Syntax The syntax for a logical sequence is as follows: from (every)? (not)? <event reference>=<input stream>[<filter condition>] ((and|or) <event reference>=<input stream>[<filter condition>])? (within <time gap>)?, ... select <event reference>([event index])?.<attribute name>, ... insert into <output stream> Keywords such as and , or , or not can be used to illustrate the logical relationship, similar to how it is done in Logical Patterns . Example This Siddhi application notifies the state when a regulator event is immediately followed by both temperature and humidity events. define stream TempStream(deviceID long, temp double); define stream HumidStream(deviceID long, humid double); define stream RegulatorStream(deviceID long, isOn bool); from every e1=RegulatorStream, e2=TempStream and e3=HumidStream select e2.temp, e3.humid insert into StateNotificationStream;","title":"Sequence"},{"location":"docs/query-guide/#output-rate-limiting","text":"Output rate limiting allows queries to output events periodically based on a specified condition. Purpose This allows you to limit the output to avoid overloading the subsequent executions, and to remove unnecessary information. Syntax The syntax of an output rate limiting configuration is as follows: from <input stream> ... select <attribute name>, <attribute name>, ... output <rate limiting configuration> insert into <output stream> Siddhi supports three types of output rate limiting configurations as explained in the following table: Rate limiting configuration Syntax Description Based on time <output event> every <time interval> This outputs <output event> every <time interval> time interval. Based on number of events <output event> every <event interval> events This outputs <output event> for every <event interval> number of events. Snapshot based output snapshot every <time interval> This outputs all events in the window (or the last event if no window is defined in the query) for every given <time interval> time interval. Here the <output event> specifies the event(s) that should be returned as the output of the query. The possible values are as follows: * first : Only the first event processed by the query during the specified time interval/sliding window is emitted. * last : Only the last event processed by the query during the specified time interval/sliding window is emitted. * all : All the events processed by the query during the specified time interval/sliding window are emitted. When no <output event> is defined, all is used by default. Examples Returning events based on the number of events Here, events are emitted every time the specified number of events arrive. You can also specify whether to emit only the first event/last event, or all the events out of the events that arrived. In this example, the last temperature per sensor is emitted for every 10 events. from TempStreamselect select temp, deviceID group by deviceID output last every 10 events insert into LowRateTempStream; Returning events based on time Here events are emitted for every predefined time interval. You can also specify whether to emit only the first event, last event, or all events out of the events that arrived during the specified time interval. In this example, emits all temperature events every 10 seconds from TempStreamoutput output every 10 sec insert into LowRateTempStream; Returning a periodic snapshot of events This method works best with windows. When an input stream is connected to a window, snapshot rate limiting emits all the current events that have arrived and do not have corresponding expired events for every predefined time interval. If the input stream is not connected to a window, only the last current event for each predefined time interval is emitted. This query emits a snapshot of the events in a time window of 5 seconds every 1 second. from TempStream#window.time(5 sec) output snapshot every 1 sec insert into SnapshotTempStream;","title":"Output rate limiting"},{"location":"docs/query-guide/#partition","text":"Partitions divide streams and queries into isolated groups in order to process them in parallel and in isolation. A partition can contain one or more queries and there can be multiple instances where the same queries and streams are replicated for each partition. Each partition is tagged with a partition key. Those partitions only process the events that match the corresponding partition key. Purpose Partitions allow you to process the events groups in isolation so that event processing can be performed using the same set of queries for each group. Partition key generation A partition key can be generated in the following two methods: Partition by value This is created by generating unique values using input stream attributes. Syntax partition with ( <expression> of <stream name>, <expression> of <stream name>, ... ) begin <query> <query> ... end; Example This query calculates the maximum temperature recorded within the last 10 events per deviceID . partition with ( deviceID of TempStream ) begin from TempStream#window.length(10) select roomNo, deviceID, max(temp) as maxTemp insert into DeviceTempStream; end; Partition by range This is created by mapping each partition key to a range condition of the input streams numerical attribute. Syntax partition with ( <condition> as <partition key> or <condition> as <partition key> or ... of <stream name>, ... ) begin <query> <query> ... end; Example This query calculates the average temperature for the last 10 minutes per office area. partition with ( roomNo >= 1030 as 'serverRoom' or roomNo < 1030 and roomNo >= 330 as 'officeRoom' or roomNo < 330 as 'lobby' of TempStream) begin from TempStream#window.time(10 min) select roomNo, deviceID, avg(temp) as avgTemp insert into AreaTempStream end;","title":"Partition"},{"location":"docs/query-guide/#inner-stream","text":"Queries inside a partition block can use inner streams to communicate with each other while preserving partition isolation. Inner streams are denoted by a \"#\" placed before the stream name, and these streams cannot be accessed outside a partition block. Purpose Inner streams allow you to connect queries within the partition block so that the output of a query can be used as an input only by another query within the same partition. Therefore, you do not need to repartition the streams if they are communicating within the partition. Example This partition calculates the average temperature of every 10 events for each sensor, and sends an output to the DeviceTempIncreasingStream stream if the consecutive average temperature values increase by more than 5 degrees. partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every (e1=#AvgTempStream),e2=#AvgTempStream[e1.avgTemp + 5 < avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end;","title":"Inner Stream"},{"location":"docs/query-guide/#purge-partition","text":"Based on the partition key used for the partition, multiple instances of streams and queries will be generated. When an extremely large number of unique partition keys are used there is a possibility of very high instances of streams and queries getting generated and eventually system going out of memory. In order to overcome this, users can define a purge interval to clean partitions that will not be used anymore. Purpose @purge allows you to clean the partition instances that will not be used anymore. Syntax The syntax of partition purge configuration is as follows: @purge(enable='true', interval='<purge interval>', idle.period='<idle period of partition instance>') partition with ( <partition key> of <input stream> ) begin from <input stream> ... select <attribute name>, <attribute name>, ... insert into <output stream> end; Partition purge configuration Description Purge interval The periodic time interval to purge the purgeable partition instances. Idle period of partition instance The period, a particular partition instance (for a given partition key) needs to be idle before it becomes purgeable. Examples Mark partition instances eligible for purging, if there are no events from a particular deviceID for 15 seconds, and periodically purge those partition instances every 1 second. @purge(enable='true', interval='1 sec', idle.period='15 sec') partition with ( deviceID of TempStream ) begin from TempStream#window.lengthBatch(10) select roomNo, deviceID, avg(temp) as avgTemp insert into #AvgTempStream from every (e1=#AvgTempStream),e2=#AvgTempStream[e1.avgTemp + 5 < avgTemp] select e1.deviceID, e1.avgTemp as initialAvgTemp, e2.avgTemp as finalAvgTemp insert into DeviceTempIncreasingStream end;","title":"Purge Partition"},{"location":"docs/query-guide/#table","text":"A table is a stored version of an stream or a table of events. Its schema is defined via the table definition that is similar to a stream definition. These events are by default stored in-memory , but Siddhi also provides store extensions to work with data/events stored in various data stores through the table abstraction. Purpose Tables allow Siddhi to work with stored events. By defining a schema for tables Siddhi enables them to be processed by queries using their defined attributes with the streaming data. You can also interactively query the state of the stored events in the table. Syntax The syntax for a new table definition is as follows: define table <table name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ); The following parameters are configured in a table definition: Parameter Description table name The name of the table defined. ( PascalCase is used for table name as a convention.) attribute name The schema of the table is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . Example The following defines a table named RoomTypeTable with roomNo and type attributes of data types int and string respectively. define table RoomTypeTable ( roomNo int, type string ); Primary Keys Tables can be configured with primary keys to avoid the duplication of data. Primary keys are configured by including the @PrimaryKey( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have only one @PrimaryKey annotation. The number of attributes supported differ based on the table implementations. When more than one attribute is used for the primary key, the uniqueness of the events stored in the table is determined based on the combination of values for those attributes. Examples This query creates an event table with the symbol attribute as the primary key. Therefore each entry in this table must have a unique value for symbol attribute. @PrimaryKey('symbol') define table StockTable (symbol string, price float, volume long); Indexes Indexes allow tables to be searched/modified much faster. Indexes are configured by including the @Index( 'key1', 'key2' ) annotation to the table definition. Each event table configuration can have 0-1 @Index annotations. Support for the @Index annotation and the number of attributes supported differ based on the table implementations. When more then one attribute is used for index, each one of them is used to index the table for fast access of the data. Indexes can be configured together with primary keys. Examples This query creates an indexed event table named RoomTypeTable with the roomNo attribute as the index key. @Index('roomNo') define table RoomTypeTable (roomNo int, type string);","title":"Table"},{"location":"docs/query-guide/#store","text":"Store is a table that refers to data/events stored in data stores outside of Siddhi such as RDBMS, Cassandra, etc. Store is defined via the @store annotation, and the store schema is defined via a table definition associated with it. Purpose Store allows Siddhi to search, retrieve and manipulate data stored in external data stores through Siddhi queries. Syntax The syntax for a defining store and it's associated table definition is as follows: @store(type='store_type', static.option.key1='static_option_value1', static.option.keyN='static_option_valueN') define table TableName (attribute1 Type1, attributeN TypeN); Example The following defines a RDBMS data store pointing to a MySQL database with name hotel hosted in loacalhost:3306 having a table RoomTypeTable with columns roomNo of INTEGER and type of VARCHAR(255) mapped to Siddhi data types int and string respectively. @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/hotel\", username=\"siddhi\", password=\"123\", jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table RoomTypeTable ( roomNo int, type string ); Supported Store Types The following is a list of currently supported store types: RDBMS (MySQL, Oracle, SQL Server, PostgreSQL, DB2, H2) MongoDB Caching in Memory Store tables are persisted in high i/o latency storage. Hence, it is beneficial to maintain a cache of store tables in memory which has low latency. Siddhi supports caching of store tables through @cache annotation. It should be used within @store annotation in a nested fashion as shown below. @store(type='store_type', static.option.key1='static_option_value1', static.option.keyN='static_option_valueN', @cache(size=10, cache.policy=FIFO)) define table TableName (attribute1 Type1, attributeN TypeN); In the above example we have defined a cache with a maximum size of 10 rows with first-in first-out cache policy. The following table contains the cache parameters. Parameter Mandatory/Optional Default Value Description size Mandatory - maximum number of rows to be cached cache.policy Optional FIFO policy to free up cache when cache miss occurs. There are 3 allowed policies. 1. FIFO - First-In, First-Out 2. LRU - Least Recently Used 3. LFU - Least Frequently Used retention.period Optional - If user specifies this parameter then cache expiry is enabled. For example if this is 5 min, rows older than 5 mins will be removed and in some cases reloaded from store purge.interval optional equal to retention period When cache expiry is enabled, a thread will be created for every purge.interval which will check for expired rows and remove them. The following is an example of caching with expiry. @store(type='store_type', static.option.key1='static_option_value1', static.option.keyN='static_option_valueN', @cache(size=10, retention.period=5 min, purge.interval=1 min)) define table TableName (attribute1 Type1, attributeN TypeN); The above query will define and create a store table of given type and a cache with a max size of 10. A thread will be created every 1 minute which will check the entire cache table for rows added earlier than 5 minutes and expire them. Cache Behaviour Cache behaviour changes profoundly based on the size of store table relative to maximum cache size defined. Since memory is a limited resource we don't allow cache to grow more than the user specified maximum size. Case 1 \\ When store table is smaller than maximum cache size defined we keep the entire content of store table in memory in cache table. All types of queries are routed to cache and cache results are directly sent out to the user. Every time the expiry thread finds that cache events were loaded earlier than retention period entire cache table will be deleted and reloaded from store. In addition, when siddhi app starts, the entire store table, if it exists, will be loaded into cache. Case 2 \\ When store table is bigger than maximum cache size only the queries satisfying the following 2 conditions are sent to cache. 1. the query contains all the primary keys of the table 2. the query contains only == type of comparison. Only for the above types of queries we can establish if the cache is hit or missed. Subject to these conditions if the cache is hit the results from cache is sent out. If the cache is missed then store is checked. If the above conditions are not met by a query it is directly sent to the store table. In addition, please note that if the store table is pre existing when siddhi app is started and it is bigger than max cache size, cache preloading will take only upto max size and put it in cache. For example if store table has 50 entries when the siddhi app is defined with cache size of 10, only the first 10 rows will be cached. When cache miss occurs we look for the answer in the store table. If there is a result from the store table it is added to cache. One element from cache is removed using the user given cache policy prior to adding. When it comes to cache expiry, since not all rows are loaded at once in this case there may be some expired rows and some unexpired rows at any time. So for every purge interval a thread will be generated which looks for rows that were loaded earlier than retention period and delete only those rows. No reloading is done. Operators on Table (and Store) The following operators can be performed on tables (and stores).","title":"Store"},{"location":"docs/query-guide/#insert","text":"This allows events to be inserted into tables. This is similar to inserting events into streams. Warning If the table is defined with primary keys, and if you insert duplicate data, primary key constrain violations can occur. In such cases use the update or insert into operation. Syntax from <input stream> select <attribute name>, <attribute name>, ... insert into <table> Similar to streams, you need to use the current events , expired events or the all events keyword between insert and into keywords in order to insert only the specific event types. For more information, see Event Type Example This query inserts all the events from the TempStream stream to the TempTable table. from TempStream select * insert into TempTable;","title":"Insert"},{"location":"docs/query-guide/#join-table","text":"This allows a stream to retrieve information from a table in a streaming manner. Note Joins can also be performed with two streams , aggregation or against externally named windows . Syntax from <input stream> join <table> on <condition> select (<input stream>|<table>).<attribute name>, (<input stream>|<table>).<attribute name>, ... insert into <output stream> Note A table can only be joint with a stream. Two tables cannot be joint because there must be at least one active entity to trigger the join operation. Example This Siddhi App performs a join to retrieve the room type from RoomTypeTable table based on the room number, so that it can filter the events related to server-room s. define table RoomTypeTable (roomNo int, type string); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream join RoomTypeTable on RoomTypeTable.roomNo == TempStream.roomNo select deviceID, RoomTypeTable.type as roomType, type, temp having roomType == 'server-room' insert into ServerRoomTempStream; Supported join types Table join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the table. The output is generated only if there is a matching event in both the stream and the table. Left outer join The left outer join operation allows you to join a stream on left side with a table on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right table by having null values for the attributes of the right table. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a table on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left table.","title":"Join (Table)"},{"location":"docs/query-guide/#delete","text":"To delete selected events that are stored in a table. Syntax from <input stream> select <attribute name>, <attribute name>, ... delete <table> (for <event type>)? on <condition> The condition element specifies the basis on which events are selected to be deleted. When specifying the condition, table attributes should be referred to with the table name. To execute delete for specific event types, use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see Event Type Note Table attributes must be always referred to with the table name as follows: <table name>.<attibute name> Example In this example, the script deletes a record in the RoomTypeTable table if it has a value for the roomNo attribute that matches the value for the roomNumber attribute of an event in the DeleteStream stream. define table RoomTypeTable (roomNo int, type string); define stream DeleteStream (roomNumber int); from DeleteStream delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber;","title":"Delete"},{"location":"docs/query-guide/#update","text":"This operator updates selected event attributes stored in a table based on a condition. Syntax from <input stream> select <attribute name>, <attribute name>, ... update <table> (for <event type>)? set <table>.<attribute name> = (<attribute name>|<expression>)?, <table>.<attribute name> = (<attribute name>|<expression>)?, ... on <condition> The condition element specifies the basis on which events are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. To execute an update for specific event types use the current events , expired events or the all events keyword with for as shown in the syntax. For more information, see Event Type . Note Table attributes must be always referred to with the table name as shown below: <table name>.<attibute name> . Example This Siddhi application updates the room occupancy in the RoomOccupancyTable table for each room number based on new arrivals and exits from the UpdateStream stream. define table RoomOccupancyTable (roomNo int, people int); define stream UpdateStream (roomNumber int, arrival int, exit int); from UpdateStream select * update RoomOccupancyTable set RoomOccupancyTable.people = RoomOccupancyTable.people + arrival - exit on RoomOccupancyTable.roomNo == roomNumber;","title":"Update"},{"location":"docs/query-guide/#update-or-insert","text":"This allows you update if the event attributes already exist in the table based on a condition, or else insert the entry as a new attribute. Syntax from <input stream> select <attribute name>, <attribute name>, ... update or insert into <table> (for <event type>)? set <table>.<attribute name> = <expression>, <table>.<attribute name> = <expression>, ... on <condition> The condition element specifies the basis on which events are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note When the attribute to the right is a table attribute, the operations supported differ based on the database type. To execute update upon specific event types use the current events , expired events or the all events keyword with for as shown in the syntax. To understand more see Event Type . Note Table attributes should be always referred to with the table name as <table name>.<attibute name> . Example The following query update for events in the UpdateTable event table that have room numbers that match the same in the UpdateStream stream. When such events are found in the event table, they are updated. When a room number available in the stream is not found in the event table, it is inserted from the stream. define table RoomAssigneeTable (roomNo int, type string, assignee string); define stream RoomAssigneeStream (roomNumber int, type string, assignee string); from RoomAssigneeStream select roomNumber as roomNo, type, assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo;","title":"Update or Insert"},{"location":"docs/query-guide/#in","text":"This allows the stream to check whether the expected value exists in the table as a part of a conditional operation. Syntax from <input stream>[<condition> in <table>] select <attribute name>, <attribute name>, ... insert into <output stream> The condition element specifies the basis on which events are selected to be compared. When constructing the condition , the table attribute must be always referred to with the table name as shown below: <table>.<attibute name> . Example This Siddhi application filters only room numbers that are listed in the ServerRoomTable table. define table ServerRoomTable (roomNo int); define stream TempStream (deviceID long, roomNo int, temp double); from TempStream[ServerRoomTable.roomNo == roomNo in ServerRoomTable] insert into ServerRoomTempStream;","title":"In"},{"location":"docs/query-guide/#named-aggregation","text":"Named aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. This not only allows you to calculate aggregations with varied time granularity, but also allows you to access them in an interactive manner for reports, dashboards, and for further processing. Its schema is defined via the aggregation definition . Purpose Named aggregation allows you to retrieve the aggregate values for different time durations. That is, it allows you to obtain aggregates such as sum , count , avg , min , max , count and distinctCount of stream attributes for durations such as sec , min , hour , etc. This is of considerable importance in many Analytics scenarios because aggregate values are often needed for several time periods. Furthermore, this ensures that the aggregations are not lost due to unexpected system failures because aggregates can be stored in different persistence stores . Syntax @store(type=\"<store type>\", ...) @purge(enable=\"<true or false>\",interval=<purging interval>,@retentionPeriod(<granularity> = <retention period>, ...) ) define aggregation <aggregator name> from <input stream> select <attribute name>, <aggregate function>(<attribute name>) as <attribute name>, ... group by <attribute name> aggregate by <timestamp attribute> every <time periods> ; The above syntax includes the following: Item Description @store This annotation is used to refer to the data store where the calculated aggregate results are stored. This annotation is optional. When no annotation is provided, the data is stored in the in-memory store. @purge This annotation is used to configure purging in aggregation granularities. If this annotation is not provided, the default purging mentioned above is applied. If you want to disable automatic data purging, you can use this annotation as follows: '@purge(enable=false) /You should disable data purging if the aggregation query in included in the Siddhi application for read-only purposes. @retentionPeriod This annotation is used to specify the length of time the data needs to be retained when carrying out data purging. If this annotation is not provided, the default retention period is applied. <aggregator name> This specifies a unique name for the aggregation so that it can be referred when accessing aggregate results. <input stream> The stream that feeds the aggregation. Note! this stream should be already defined. group by <attribute name> The group by clause is optional. If it is included in a Siddhi application, aggregate values are calculated per each group by attribute. If it is not used, all the events are aggregated together. by <timestamp attribute> This clause is optional. This defines the attribute that should be used as the timestamp. If this clause is not used, the event time is used by default. The timestamp could be given as either a string or a long value. If it is a long value, the unix timestamp in milliseconds is expected (e.g. 1496289950000 ). If it is a string value, the supported formats are <yyyy>-<MM>-<dd> <HH>:<mm>:<ss> (if time is in GMT) and <yyyy>-<MM>-<dd> <HH>:<mm>:<ss> <Z> (if time is not in GMT), here the ISO 8601 UTC offset must be provided for <Z> . (e.g., +05:30 , -11:00 ). <time periods> Time periods can be specified as a range where the minimum and the maximum value are separated by three dots, or as comma-separated values. e.g., A range can be specified as sec...year where aggregation is done per second, minute, hour, day, month and year. Comma-separated values can be specified as min, hour. Skipping time durations (e.g., min, day where the hour duration is skipped) when specifying comma-separated values is supported only from v4.1.1 onwards Aggregation's granularity data holders are automatically purged every 15 minutes. When carrying out data purging, the retention period you have specified for each granularity in the named aggregation query is taken into account. The retention period defined for a granularity needs to be greater than or equal to its minimum retention period as specified in the table below. If no valid retention period is defined for a granularity, the default retention period (as specified in the table below) is applied. Granularity Default retention Minimum retention second 120 seconds 120 seconds minute 24 hours 120 minutes hour 30 days 25 hours day 1 year 32 days month All 13 month year All none Note Aggregation is carried out at calendar start times for each granularity with the GMT timezone Note The same aggregation can be defined in multiple Siddhi apps for joining, however, only one siddhi app should carry out the processing (i.e. the aggregation input stream should only feed events to one aggregation definition). Example This Siddhi Application defines an aggregation named TradeAggregation to calculate the average and sum for the price attribute of events arriving at the TradeStream stream. These aggregates are calculated per every time granularity in the second-year range. define stream TradeStream (symbol string, price double, volume long, timestamp long); @purge(enable='true', interval='10 sec',@retentionPeriod(sec='120 sec',min='24 hours',hours='30 days',days='1 year',months='all',years='all')) define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year;","title":"Named Aggregation"},{"location":"docs/query-guide/#distributed-aggregation","text":"Distributed Aggregation allows you to partially process aggregations in different shards. This allows Siddhi app in one shard to be responsible only for processing a part of the aggregation. However for this, all aggregations must be based on a common physical database(@store). Syntax @store(type=\"<store type>\", ...) @PartitionById define aggregation <aggregator name> from <input stream> select <attribute name>, <aggregate function>(<attribute name>) as <attribute name>, ... group by <attribute name> aggregate by <timestamp attribute> every <time periods> ; Following table includes the annotation to be used to enable distributed aggregation, Item Description @PartitionById If the annotation is given, then the distributed aggregation is enabled. Further this can be disabled by using enable element, @PartitionById(enable='false') . Further, following system properties are also available, System Property Description Possible Values Optional Default Value shardId The id of the shard one of the distributed aggregation is running in. This should be unique to a single shard Any string No partitionById This allows user to enable/disable distributed aggregation for all aggregations running in one siddhi manager .(Available from v4.3.3) true/false Yesio false Note ShardIds should not be changed after the first configuration in order to keep data consistency.","title":"Distributed Aggregation"},{"location":"docs/query-guide/#join-aggregation","text":"This allows a stream to retrieve calculated aggregate values from the aggregation. Note A join can also be performed with two streams , with a table and a stream, or with a stream against externally named windows . Syntax A join with aggregation is similer to the join with table , but with additional within and per clauses. from <input stream> join <aggrigation> on <join condition> within <time range> per <time granularity> select <attribute name>, <attribute name>, ... insert into <output stream>; Apart from constructs of table join this includes the following. Please note that the 'on' condition is optional : Item Description within <time range> This allows you to specify the time interval for which the aggregate values need to be retrieved. This can be specified by providing the start and end time separated by a comma as string or long values, or by using the wildcard string specifying the data range. For details refer examples. per <time granularity> This specifies the time granularity by which the aggregate values must be grouped and returned. e.g., If you specify days , the retrieved aggregate values are grouped for each day within the selected time interval. within and per clauses also accept attribute values from the stream. The timestamp of the aggregations can be accessed through the AGG_TIMESTAMP attribute. Example Following aggregation definition will be used for the examples. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select AGG_TIMESTAMP, symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) define stream StockStream (symbol string, value int); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; This query retrieves hourly aggregations within the day 2014-02-15 . define stream StockStream (symbol string, value int); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within \"2014-02-15 **:**:** +05:30\" per \"hours\" select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; This query retrieves all aggregations per perValue stream attribute within the time period between timestamps 1496200000000 and 1596434876000 . define stream StockStream (symbol string, value int, perValue string); from StockStream as S join TradeAggregation as T on S.symbol == T.symbol within 1496200000000L, 1596434876000L per S.perValue select S.symbol, T.total, T.avgPrice insert into AggregateStockStream; Supported join types Aggregation join supports following join operations. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join the stream with the aggregation. The output is generated only if there is a matching event in the stream and the aggregation. Left outer join The left outer join operation allows you to join a stream on left side with a aggregation on the right side based on a condition. Here, it returns all the events of left stream even if there are no matching events in the right aggregation by having null values for the attributes of the right aggregation. Right outer join This is similar to a left outer join . right outer join is used as the keyword to join a stream on right side with a aggregation on the left side based on a condition. It returns all the events of the right stream even if there are no matching events in the left aggregation.","title":"Join (Aggregation)"},{"location":"docs/query-guide/#named-window","text":"A named window is a window that can be shared across multiple queries. Events can be inserted to a named window from one or more queries and it can produce output events based on the named window type. Syntax The syntax for a named window is as follows: define window <window name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ) <window type>(<parameter>, <parameter>, \u2026) <event type>; The following parameters are configured in a table definition: Parameter Description window name The name of the window defined. ( PascalCase is used for window names as a convention.) attribute name The schema of the window is defined by its attributes with uniquely identifiable attribute names ( camelCase is used for attribute names as a convention.) attribute type The type of each attribute defined in the schema. This can be STRING , INT , LONG , DOUBLE , FLOAT , BOOL or OBJECT . <window type>(<parameter>, ...) The window type associated with the window and its parameters. output <event type> This is optional. Keywords such as current events , expired events and all events (the default) can be used to specify when the window output should be exposed. For more information, see Event Type . Examples Returning all output when events arrive and when events expire from the window. In this query, the event type is not specified. Therefore, it returns both current and expired events as the output. define window SensorWindow (name string, value float, roomNo int, deviceID string) timeBatch(1 second); Returning an output only when events expire from the window. In this query, the event type of the window is expired events . Therefore, it only returns the events that have expired from the window as the output. define window SensorWindow (name string, value float, roomNo int, deviceID string) timeBatch(1 second) output expired events; Operators on Named Windows The following operators can be performed on named windows.","title":"Named Window"},{"location":"docs/query-guide/#insert_1","text":"This allows events to be inserted into windows. This is similar to inserting events into streams. Syntax from <input stream> select <attribute name>, <attribute name>, ... insert into <window> To insert only events of a specific event type, add the current events , expired events or the all events keyword between insert and into keywords (similar to how it is done for streams). For more information, see Event Type . Example This query inserts all events from the TempStream stream to the OneMinTempWindow window. define stream TempStream(tempId string, temp double); define window OneMinTempWindow(tempId string, temp double) time(1 min); from TempStream select * insert into OneMinTempWindow;","title":"Insert"},{"location":"docs/query-guide/#join-window","text":"To allow a stream to retrieve information from a window based on a condition. Note A join can also be performed with two streams , aggregation or with tables tables . Syntax from <input stream> join <window> on <condition> select (<input stream>|<window>).<attribute name>, (<input stream>|<window>).<attribute name>, ... insert into <output stream> Example This Siddhi Application performs a join count the number of temperature events having more then 40 degrees within the last 2 minutes. define window TwoMinTempWindow (roomNo int, temp double) time(2 min); define stream CheckStream (requestId string); from CheckStream as C join TwoMinTempWindow as T on T.temp > 40 select requestId, count(T.temp) as count insert into HighTempCountStream; Supported join types Window join supports following operations of a join clause. Inner join (join) This is the default behaviour of a join operation. join is used as the keyword to join two windows or a stream with a window. The output is generated only if there is a matching event in both stream/window. Left outer join The left outer join operation allows you to join two windows or a stream with a window to be merged based on a condition. Here, it returns all the events of left stream/window even if there are no matching events in the right stream/window by having null values for the attributes of the right stream/window. Right outer join This is similar to a left outer join. Right outer join is used as the keyword to join two windows or a stream with a window. It returns all the events of the right stream/window even if there are no matching events in the left stream/window. Full outer join The full outer join combines the results of left outer join and right outer join . full outer join is used as the keyword to join two windows or a stream with a window. Here, output event are generated for each incoming event even if there are no matching events in the other stream/window.","title":"Join (Window)"},{"location":"docs/query-guide/#from","text":"A window can be an input to a query, similar to streams. Note !!! When window is used as an input to a query, another window cannot be applied on top of this. Syntax from <window> select <attribute name>, <attribute name>, ... insert into <output stream> Example This Siddhi Application calculates the maximum temperature within the last 5 minutes. define window FiveMinTempWindow (roomNo int, temp double) time(5 min); from FiveMinTempWindow select max(temp) as maxValue, roomNo insert into MaxSensorReadingStream;","title":"From"},{"location":"docs/query-guide/#trigger","text":"Triggers allow events to be periodically generated. Trigger definition can be used to define a trigger. A trigger also works like a stream with a predefined schema. Purpose For some use cases the system should be able to periodically generate events based on a specified time interval to perform some periodic executions. A trigger can be performed for a 'start' operation, for a given <time interval> , or for a given '<cron expression>' . Syntax The syntax for a trigger definition is as follows. define trigger <trigger name> at ('start'| every <time interval>| '<cron expression>'); Similar to streams, triggers can be used as inputs. They adhere to the following stream definition and produce the triggered_time attribute of the long type. define stream <trigger name> (triggered_time long); The following types of triggeres are currently supported: Trigger type Description 'start' An event is triggered when Siddhi is started. every <time interval> An event is triggered periodically at the given time interval. '<cron expression>' An event is triggered periodically based on the given cron expression. For configuration details, see quartz-scheduler . Examples Triggering events regularly at specific time intervals The following query triggers events every 5 minutes. define trigger FiveMinTriggerStream at every 5 min; Triggering events at a specific time on specified days The following query triggers an event at 10.15 AM on every weekdays. define trigger FiveMinTriggerStream at '0 15 10 ? * MON-FRI';","title":"Trigger"},{"location":"docs/query-guide/#script","text":"Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Function definitions can be used to define these scripts. Function parameters are passed into the function logic as Object[] and with the name data . Purpose Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Syntax The syntax for a Script definition is as follows. define function <function name>[<language name>] return <return type> { <operation of the function> }; The following parameters are configured when defining a script. Parameter Description function name The name of the function ( camelCase is used for the function name) as a convention. language name The name of the programming language used to define the script, such as javascript , r and scala . return type The attribute type of the function\u2019s return. This can be int , long , float , double , string , bool or object . Here the function implementer should be responsible for returning the output attribute on the defined return type for proper functionality. operation of the function Here, the execution logic of the function is added. This logic should be written in the language specified under the language name , and it should return the output in the data type specified via the return type parameter. Examples This query performs concatenation using JavaScript, and returns the output as a string. define function concatFn[javascript] return string { var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var responce = str1 + str2 + str3; return responce; }; define stream TempStream(deviceID long, roomNo int, temp double); from TempStream select concatFn(roomNo,'-',deviceID) as id, temp insert into DeviceTempStream;","title":"Script"},{"location":"docs/query-guide/#store-query","text":"Siddhi store queries are a set of on-demand queries that can be used to perform operations on Siddhi tables, windows, and aggregators. Purpose Store queries allow you to execute the following operations on Siddhi tables, windows, and aggregators without the intervention of streams. Queries supported for tables: SELECT INSERT DELETE UPDATE UPDATE OR INSERT Queries supported for windows and aggregators: SELECT This is be done by submitting the store query to the Siddhi application runtime using its query() method. In order to execute store queries, the Siddhi application of the Siddhi application runtime you are using, should have a store defined, which contains the table that needs to be queried. Example If you need to query the table named RoomTypeTable the it should have been defined in the Siddhi application. In order to execute a store query on RoomTypeTable , you need to submit the store query using query() method of SiddhiAppRuntime instance as below. siddhiAppRuntime.query(<store query>);","title":"Store Query"},{"location":"docs/query-guide/#tablewindow-select","text":"The SELECT store query retrieves records from the specified table or window, based on the given condition. Syntax from <table/window> <on condition>? select <attribute name>, <attribute name>, ... <group by>? <having>? <order by>? <limit>? Example This query retrieves room numbers and types of the rooms starting from room no 10. from roomTypeTable on roomNo >= 10; select roomNo, type","title":"(Table/Window) Select"},{"location":"docs/query-guide/#aggregation-select","text":"The SELECT store query retrieves records from the specified aggregation, based on the given condition, time range, and granularity. Syntax from <aggregation> <on condition>? within <time range> per <time granularity> select <attribute name>, <attribute name>, ... <group by>? <having>? <order by>? <limit>? Example Following aggregation definition will be used for the examples. define stream TradeStream (symbol string, price double, volume long, timestamp long); define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(price) as total group by symbol aggregate by timestamp every sec ... year; This query retrieves daily aggregations within the time range \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" (Please note that +05:30 can be omitted if timezone is GMT) from TradeAggregation within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select symbol, total, avgPrice ; This query retrieves hourly aggregations of \"FB\" symbol within the day 2014-02-15 . from TradeAggregation on symbol == \"FB\" within \"2014-02-15 **:**:** +05:30\" per \"hours\" select symbol, total, avgPrice;","title":"(Aggregation) Select"},{"location":"docs/query-guide/#insert_2","text":"This allows you to insert a new record to the table with the attribute values you define in the select section. Syntax select <attribute name>, <attribute name>, ... insert into <table>; Example This store query inserts a new record to the table RoomOccupancyTable , with the specified attribute values. select 10 as roomNo, 2 as people insert into RoomOccupancyTable","title":"Insert"},{"location":"docs/query-guide/#delete_1","text":"The DELETE store query deletes selected records from a specified table. Syntax <select>? delete <table> on <conditional expresssion> The condition element specifies the basis on which records are selected to be deleted. Note Table attributes must always be referred to with the table name as shown below: <table name>.<attibute name> . Example In this example, query deletes a record in the table named RoomTypeTable if it has value for the roomNo attribute that matches the value for the roomNumber attribute of the selection which has 10 as the actual value. select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber; delete RoomTypeTable on RoomTypeTable.roomNo == 10;","title":"Delete"},{"location":"docs/query-guide/#update_1","text":"The UPDATE store query updates selected attributes stored in a specific table, based on a given condition. Syntax select <attribute name>, <attribute name>, ...? update <table> set <table>.<attribute name> = (<attribute name>|<expression>)?, <table>.<attribute name> = (<attribute name>|<expression>)?, ... on <condition> The condition element specifies the basis on which records are selected to be updated. When specifying the condition , table attributes must be referred to with the table name. You can use the set keyword to update selected attributes from the table. Here, for each assignment, the attribute specified in the left must be the table attribute, and the one specified in the right can be a stream/table attribute a mathematical operation, or other. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: <table name>.<attibute name> . Example The following query updates the room occupancy by increasing the value of people by 1, in the RoomOccupancyTable table for each room number greater than 10. select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber; update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + 1 on RoomTypeTable.roomNo == 10;","title":"Update"},{"location":"docs/query-guide/#update-or-insert_1","text":"This allows you to update selected attributes if a record that meets the given conditions already exists in the specified table. If a matching record does not exist, the entry is inserted as a new record. Syntax select <attribute name>, <attribute name>, ... update or insert into <table> set <table>.<attribute name> = <expression>, <table>.<attribute name> = <expression>, ... on <condition> The condition element specifies the basis on which records are selected for update. When specifying the condition , table attributes should be referred to with the table name. If a record that matches the condition does not already exist in the table, the arriving event is inserted into the table. The set clause is only used when an update is performed during the insert/update operation. When set clause is used, the attribute to the left is always a table attribute, and the attribute to the right can be a stream/table attribute, mathematical operation or other. The attribute to the left (i.e., the attribute in the event table) is updated with the value of the attribute to the right if the given condition is met. When the set clause is not provided, all the attributes in the table are updated. Note Table attributes must always be referred to with the table name as shown below: <table name>.<attibute name> . Example The following query tries to update the records in the RoomAssigneeTable table that have room numbers that match the same in the selection. If such records are not found, it inserts a new record based on the values provided in the selection. select 10 as roomNo, \"single\" as type, \"abc\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo;","title":"Update or Insert"},{"location":"docs/query-guide/#extensions","text":"Siddhi supports an extension architecture to enhance its functionality by incorporating other libraries in a seamless manner. Purpose Extensions are supported because, Siddhi core cannot have all the functionality that's needed for all the use cases, mostly use cases require different type of functionality, and for some cases there can be gaps and you need to write the functionality by yourself. All extensions have a namespace. This is used to identify the relevant extensions together, and to let you specifically call the extension. Syntax Extensions follow the following syntax; <namespace>:<function name>(<parameter>, <parameter>, ... ) The following parameters are configured when referring a script function. Parameter Description namespace Allows Siddhi to identify the extension without conflict function name The name of the function referred. parameter The function input parameter for function execution. Extension Types Siddhi supports following extension types: Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute. This can be used to manipulate existing event attributes to generate new attributes like any Function operation. This is implemented by extending io.siddhi.core.executor.function.FunctionExecutor . Example : math:sin(x) Here, the sin function of math extension returns the sin value for the x parameter. Aggregate Function For each event, it consumes zero or more parameters as input parameters and returns a single attribute with aggregated results. This can be used in conjunction with a window in order to find the aggregated results based on the given window like any Aggregate Function operation. This is implemented by extending io.siddhi.core.query.selector.attribute.aggregator.AttributeAggregatorExecutor . Example : custom:std(x) Here, the std aggregate function of custom extension returns the standard deviation of the x value based on its assigned window query. Window This allows events to be collected, generated, dropped and expired anytime without altering the event format based on the given input parameters, similar to any other Window operator. This is implemented by extending io.siddhi.core.query.processor.stream.window.WindowProcessor . Example : custom:unique(key) Here, the unique window of the custom extension retains one event for each unique key parameter. Stream Function This allows events to be generated or dropped only during event arrival and altered by adding one or more attributes to it. This is implemented by extending io.siddhi.core.query.processor.stream.function.StreamFunctionProcessor . Example : custom:pol2cart(theta,rho) Here, the pol2cart function of the custom extension returns all the events by calculating the cartesian coordinates x & y and adding them as new attributes to the events. Stream Processor This allows events to be collected, generated, dropped and expired anytime by altering the event format by adding one or more attributes to it based on the given input parameters. Implemented by extending io.siddhi.core.query.processor.stream.StreamProcessor . Example : custom:perMinResults(<parameter>, <parameter>, ...) Here, the perMinResults function of the custom extension returns all events by adding one or more attributes to the events based on the conversion logic. Altered events are output every minute regardless of event arrivals. Sink Sinks provide a way to publish Siddhi events to external systems in the preferred data format. Sinks publish events from the streams via multiple transports to external endpoints in various data formats. Implemented by extending io.siddhi.core.stream.output.sink.Sink . Example : @sink(type='sink_type', static_option_key1='static_option_value1') To configure a stream to publish events via a sink, add the sink configuration to a stream definition by adding the @sink annotation with the required parameter values. The sink syntax is as above Source Source allows Siddhi to consume events from external systems , and map the events to adhere to the associated stream. Sources receive events via multiple transports and in various data formats, and direct them into streams for processing. Implemented by extending io.siddhi.core.stream.input.source.Source . Example : @source(type='source_type', static.option.key1='static_option_value1') To configure a stream that consumes events via a source, add the source configuration to a stream definition by adding the @source annotation with the required parameter values. The source syntax is as above Store You can use Store extension type to work with data/events stored in various data stores through the table abstraction . You can find more information about these extension types under the heading 'Extension types' in this document. Implemented by extending io.siddhi.core.table.record.AbstractRecordTable . Script Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. Implemented by extending io.siddhi.core.function.Script . Source Mapper Each @source configuration has a mapping denoted by the @map annotation that converts the incoming messages format to Siddhi events .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SourceMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Sink Mapper Each @sink configuration has a mapping denoted by the @map annotation that converts the outgoing Siddhi events to configured messages format .The type parameter of the @map defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional. Implemented by extending io.siddhi.core.stream.output.sink.SinkMapper . Example : @map(type='map_type', static_option_key1='static_option_value1') Example A window extension created with namespace foo and function name unique can be referred as follows: from StockExchangeStream[price >= 20]#window.foo:unique(symbol) select symbol, price insert into StockQuote Available Extensions Siddhi currently has several pre written extensions that are available here We value your contribution on improving Siddhi and its extensions further.","title":"Extensions"},{"location":"docs/query-guide/#writing-custom-extensions","text":"Custom extensions can be written in order to cater use case specific logic that are not available in Siddhi out of the box or as an existing extension. There are five types of Siddhi extensions that you can write to cater your specific use cases. These extension types and the related maven archetypes are given below. You can use these archetypes to generate Maven projects for each extension type. Follow the procedure for the required archetype, based on your project: Note When using the generated archetype please make sure you complete the @Extension annotation with proper values. This annotation will be used to identify and document the extension, hence your extension will not work without @Extension annotation. siddhi-execution Siddhi-execution provides following extension types: Function Aggregate Function Stream Function Stream Processor Window You can use one or more from above mentioned extension types and implement according to your requirement. For more information about these extension types, see Extension Types . To install and implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DgroupId=io.siddhi.extension.execution -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfFunction Name of the custom function to be created Y - _nameSpaceOfFunction Namespace of the function, used to grouped similar custom functions Y - groupIdPostfix Namespace of the function is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-execution- classNameOfAggregateFunction Class name of the Aggregate Function N $ classNameOfFunction Class name of the Function N $ classNameOfStreamFunction Class name of the Stream Function N $ classNameOfStreamProcessor Class name of the Stream Processor N $ classNameOfWindow Class name of the Window N $ To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-io Siddhi-io provides following extension types: Sink Source You can use one or more from above mentioned extension types and implement according to your requirement. siddhi-io is generally used to work with IO operations as follows: * The Source extension type gets inputs to your Siddhi application. * The Sink extension publishes outputs from your Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DgroupId=io.siddhi.extension.io -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _IOType Type of IO for which Siddhi-io extension is written Y - groupIdPostfix Type of the IO is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-io- classNameOfSink Class name of the Sink N classNameOfSource Class name of the Source N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-map Siddhi-map provides following extension types, Sink Mapper Source Mapper You can use one or more from above mentioned extension types and implement according to your requirement as follows. The Source Mapper maps events to a predefined data format (such as XML, JSON, binary, etc), and publishes them to external endpoints (such as E-mail, TCP, Kafka, HTTP, etc). The Sink Mapper also maps events to a predefined data format, but it does it at the time of publishing events from a Siddhi application. For more information about these extension types, see Extension Types . To implement the siddhi-map extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DgroupId=io.siddhi.extension.map -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _mapType Type of Mapper for which Siddhi-map extension is written Y - groupIdPostfix Type of the Map is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-map- classNameOfSinkMapper Class name of the Sink Mapper N classNameOfSourceMapper Class name of the Source Mapper N To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-script Siddhi-script provides the Script extension type. The script extension type allows you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. For more information about these extension types, see Extension Types . To implement the siddhi-script extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DgroupId=io.siddhi.extension.script -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _nameOfScript Name of Custom Script for which Siddhi-script extension is written Y - groupIdPostfix Name of the Script is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-script- classNameOfScript Class name of the Script N Eval To confirm that all property values are correct, type Y in the console. If not, press N . siddhi-store Siddhi-store provides the Store extension type. The Store extension type allows you to work with data/events stored in various data stores through the table abstraction. For more information about these extension types, see Extension Types . To implement the siddhi-store extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=io.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DgroupId=io.siddhi.extension.store -Dversion=1.0.0-SNAPSHOT Enter the mandatory properties prompted, please see the description for all properties below. Properties Description Mandatory Default Value _storeType Type of Store for which Siddhi-store extension is written Y - groupIdPostfix Type of the Store is added as postfix to the groupId as a convention N artifactId Artifact Id of the project N siddhi-store- className Class name of the Store N To confirm that all property values are correct, type Y in the console. If not, press N .","title":"Writing Custom Extensions"},{"location":"docs/query-guide/#configuring-and-monitoring-siddhi-applications","text":"","title":"Configuring and Monitoring Siddhi Applications"},{"location":"docs/query-guide/#multi-threading-and-asynchronous-processing","text":"When @Async annotation is added to the Streams it enable the Streams to introduce asynchronous and multi-threading behaviour. @Async(buffer.size='256', workers='2', batch.size.max='5') define stream <stream name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ); The following elements are configured with this annotation. Annotation Description Default Value buffer.size The size of the event buffer that will be used to handover the execution to other threads. - workers Number of worker threads that will be be used to process the buffered events. 1 batch.size.max The maximum number of events that will be processed together by a worker thread at a given time. buffer.size","title":"Multi-threading and Asynchronous Processing"},{"location":"docs/query-guide/#statistics","text":"Use @app:statistics app level annotation to evaluate the performance of an application, you can enable the statistics of a Siddhi application to be published. This is done via the @app:statistics annotation that can be added to a Siddhi application as shown in the following example. @app:statistics(reporter = 'console') The following elements are configured with this annotation. Annotation Description Default Value reporter The interface in which statistics for the Siddhi application are published. Possible values are as follows: console jmx console interval The time interval (in seconds) at which the statistics for the Siddhi application are reported. 60 include If this parameter is added, only the types of metrics you specify are included in the reporting. The required metric types can be specified as a comma-separated list. It is also possible to use wild cards All ( . ) The metrics are reported in the following format. io.siddhi.SiddhiApps.<SiddhiAppName>.Siddhi.<Component Type>.<Component Name>. <Metrics name> The following table lists the types of metrics supported for different Siddhi application component types. Component Type Metrics Type Stream Throughput The size of the buffer if parallel processing is enabled via the @async annotation. Trigger Throughput (Trigger and Stream) Source Throughput Sink Throughput Mapper Latency Input/output throughput Table Memory Throughput (For all operations) Throughput (For all operations) Query Memory Latency Window Throughput (For all operations) Latency (For all operation) Partition Throughput (For all operations) Latency (For all operation) e.g., the following is a Siddhi application that includes the @app annotation to report performance statistics. @App:name('TestMetrics') @App:Statistics(reporter = 'console') define stream TestStream (message string); @info(name='logQuery') from TestSream#log(\"Message:\") insert into TempSream; Statistics are reported for this Siddhi application as shown in the extract below. Click to view the extract 11/26/17 8:01:20 PM ============================================================ -- Gauges ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.memory value = 5760 io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.size value = 0 -- Meters ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Sources.TestStream.http.throughput count = 0 mean rate = 0.00 events/second 1-minute rate = 0.00 events/second 5-minute rate = 0.00 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TempSream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second io.siddhi.SiddhiApps.TestMetrics.Siddhi.Streams.TestStream.throughput count = 2 mean rate = 0.04 events/second 1-minute rate = 0.03 events/second 5-minute rate = 0.01 events/second 15-minute rate = 0.00 events/second -- Timers ---------------------------------------------------------------------- io.siddhi.SiddhiApps.TestMetrics.Siddhi.Queries.logQuery.latency count = 2 mean rate = 0.11 calls/second 1-minute rate = 0.34 calls/second 5-minute rate = 0.39 calls/second 15-minute rate = 0.40 calls/second min = 0.61 milliseconds max = 1.08 milliseconds mean = 0.84 milliseconds stddev = 0.23 milliseconds median = 0.61 milliseconds 75% < = 1.08 milliseconds 95% < = 1.08 milliseconds 98% < = 1.08 milliseconds 99% < = 1.08 milliseconds 99.9% < = 1.08 milliseconds","title":"Statistics"},{"location":"docs/query-guide/#event-playback","text":"When @app:playback annotation is added to the app, the timestamp of the event (specified via an attribute) is treated as the current time. This results in events being processed faster. The following elements are configured with this annotation. Annotation Description idle.time If no events are received during a time interval specified (in milliseconds) via this element, the Siddhi system time is incremented by a number of seconds specified via the increment element. increment The number of seconds by which the Siddhi system time must be incremented if no events are received during the time interval specified via the idle.time element. e.g., In the following example, the Siddhi system time is incremented by two seconds if no events arrive for a time interval of 100 milliseconds. @app:playback(idle.time = '100 millisecond', increment = '2 sec')","title":"Event Playback"},{"location":"docs/siddhi-as-a-docker-microservice/","text":"Siddhi 5.2 as a Docker Microservice \u00b6 This section provides information on running Siddhi Apps on Docker. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Docker Microservice is as follows. Pull the the latest Siddhi Runner image from Siddhiio Docker Hub . docker pull siddhiio/siddhi-runner-alpine:latest Start SiddhiApps with the runner config by executing the following docker command. docker run -it -v <local-siddhi-file-path>:<siddhi-file-mount-path> -v <local-conf-file-path>:<conf-file-mount-path> siddhiio/siddhi-runner-alpine:latest -Dapps=<siddhi-file-mount-path> -Dconfig=<conf-file-mount-path> E.g., docker run -it -v /home/me/siddhi-apps:/apps -v /home/me/siddhi-configs:/configs siddhiio/siddhi-runner-alpine:latest -Dapps=/apps/Foo.siddhi -Dconfig=/configs/siddhi-config.yaml Running multiple SiddhiApps in one runner instance. To run multiple SiddhiApps in one runtime instance, have all SiddhiApps in a directory, mount the directory and pass its location through -Dapps parameter as follows, -Dapps=<siddhi-apps-directory> Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Docker Microservice refer Siddhi Config Guide . Samples \u00b6 Running Siddhi App \u00b6 Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Always listen on 0.0.0.0 with the Siddhi Application running inside a docker container. If you listen on localhost inside the container, nothing outside the container can connect to your application. That includes blocking port forwarding from the docker host and container to container networking. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory docker run -it -p 8006:8006 -v <local-absolute-siddhi-file-path>/CountOverTime.siddhi:/apps/CountOverTime.siddhi siddhiio/siddhi-runner-alpine -Dapps=/apps/CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false} Running with runner config \u00b6 When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can be configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following command docker run -it -p 8006:8006 -p 9443:9443 -v <local-absolute-siddhi-file-path>/ConsumeAndStore.siddhi:/apps/ConsumeAndStore.siddhi -v <local-absolute-config-yaml-path>/TestDb.yaml:/conf/TestDb.yaml siddhiio/siddhi-runner-alpine -Dapps=/apps/ConsumeAndStore.siddhi -Dconfig=/conf/TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] } Running with environmental/system variables \u00b6 Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set the below environment variables by passing them during the docker run command: THRESHOLD=20 TO_EMAIL=<to email address> EMAIL_ADDRESS=<gmail address> EMAIL_USERNAME=<gmail username> EMAIL_PASSWORD=<gmail password> Or they can also be passed as system variables by adding them to the end of the docker run command . -DTHRESHOLD=20 -DTO_EMAIL=>to email address> -DEMAIL_ADDRESS=<gmail address> -DEMAIL_USERNAME=<gmail username> -DEMAIL_PASSWORD=<gmail password> Run the SiddhiApp by executing following command. docker run -it -p 8006:8006 -v <local-absolute-siddhi-file-path>/TemplatedFilterAndEmail.siddhi:/apps/TemplatedFilterAndEmail.siddhi -v <local-absolute-config-yaml-path>/EmailConfig.yaml:/conf/EmailConfig.yaml -e THRESHOLD=20 -e TO_EMAIL=<to email address> -e EMAIL_ADDRESS=<gmail address> -e EMAIL_USERNAME=<gmail username> -e EMAIL_PASSWORD=<gmail password> siddhiio/siddhi-runner-alpine -Dapps=/apps/TemplatedFilterAndEmail.siddhi -Dconfig=/conf/EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Siddhi Docker Microservice"},{"location":"docs/siddhi-as-a-docker-microservice/#siddhi-52-as-a-docker-microservice","text":"This section provides information on running Siddhi Apps on Docker. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Docker Microservice is as follows. Pull the the latest Siddhi Runner image from Siddhiio Docker Hub . docker pull siddhiio/siddhi-runner-alpine:latest Start SiddhiApps with the runner config by executing the following docker command. docker run -it -v <local-siddhi-file-path>:<siddhi-file-mount-path> -v <local-conf-file-path>:<conf-file-mount-path> siddhiio/siddhi-runner-alpine:latest -Dapps=<siddhi-file-mount-path> -Dconfig=<conf-file-mount-path> E.g., docker run -it -v /home/me/siddhi-apps:/apps -v /home/me/siddhi-configs:/configs siddhiio/siddhi-runner-alpine:latest -Dapps=/apps/Foo.siddhi -Dconfig=/configs/siddhi-config.yaml Running multiple SiddhiApps in one runner instance. To run multiple SiddhiApps in one runtime instance, have all SiddhiApps in a directory, mount the directory and pass its location through -Dapps parameter as follows, -Dapps=<siddhi-apps-directory> Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Docker Microservice refer Siddhi Config Guide .","title":"Siddhi 5.2 as a Docker Microservice"},{"location":"docs/siddhi-as-a-docker-microservice/#samples","text":"","title":"Samples"},{"location":"docs/siddhi-as-a-docker-microservice/#running-siddhi-app","text":"Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Always listen on 0.0.0.0 with the Siddhi Application running inside a docker container. If you listen on localhost inside the container, nothing outside the container can connect to your application. That includes blocking port forwarding from the docker host and container to container networking. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory docker run -it -p 8006:8006 -v <local-absolute-siddhi-file-path>/CountOverTime.siddhi:/apps/CountOverTime.siddhi siddhiio/siddhi-runner-alpine -Dapps=/apps/CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false}","title":"Running Siddhi App"},{"location":"docs/siddhi-as-a-docker-microservice/#running-with-runner-config","text":"When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can be configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following command docker run -it -p 8006:8006 -p 9443:9443 -v <local-absolute-siddhi-file-path>/ConsumeAndStore.siddhi:/apps/ConsumeAndStore.siddhi -v <local-absolute-config-yaml-path>/TestDb.yaml:/conf/TestDb.yaml siddhiio/siddhi-runner-alpine -Dapps=/apps/ConsumeAndStore.siddhi -Dconfig=/conf/TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] }","title":"Running with runner config"},{"location":"docs/siddhi-as-a-docker-microservice/#running-with-environmentalsystem-variables","text":"Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set the below environment variables by passing them during the docker run command: THRESHOLD=20 TO_EMAIL=<to email address> EMAIL_ADDRESS=<gmail address> EMAIL_USERNAME=<gmail username> EMAIL_PASSWORD=<gmail password> Or they can also be passed as system variables by adding them to the end of the docker run command . -DTHRESHOLD=20 -DTO_EMAIL=>to email address> -DEMAIL_ADDRESS=<gmail address> -DEMAIL_USERNAME=<gmail username> -DEMAIL_PASSWORD=<gmail password> Run the SiddhiApp by executing following command. docker run -it -p 8006:8006 -v <local-absolute-siddhi-file-path>/TemplatedFilterAndEmail.siddhi:/apps/TemplatedFilterAndEmail.siddhi -v <local-absolute-config-yaml-path>/EmailConfig.yaml:/conf/EmailConfig.yaml -e THRESHOLD=20 -e TO_EMAIL=<to email address> -e EMAIL_ADDRESS=<gmail address> -e EMAIL_USERNAME=<gmail username> -e EMAIL_PASSWORD=<gmail password> siddhiio/siddhi-runner-alpine -Dapps=/apps/TemplatedFilterAndEmail.siddhi -Dconfig=/conf/EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Running with environmental/system variables"},{"location":"docs/siddhi-as-a-java-library/","text":"Siddhi 5.2 as a Java library \u00b6 Siddhi can be used as a library in any Java program (including in OSGi runtimes) just by adding Siddhi and its extension jars as dependencies. Find a sample Siddhi project that's implemented as a Java program using Maven here , this can be used as a reference for any based implementation. Following are the mandatory dependencies that need to be added to the Maven pom.xml file (or to the program classpath). <dependency> <groupId>io.siddhi</groupId> <artifactId>siddhi-core</artifactId> <version>5.x.x</version> </dependency> <dependency> <groupId>io.siddhi</groupId> <artifactId>siddhi-query-api</artifactId> <version>5.x.x</version> </dependency> <dependency> <groupId>io.siddhi</groupId> <artifactId>siddhi-query-compiler</artifactId> <version>5.x.x</version> </dependency> <dependency> <groupId>io.siddhi</groupId> <artifactId>siddhi-annotations</artifactId> <version>5.x.x</version> </dependency> Sample \u00b6 Sample Java class using Siddhi is as follows.","title":"Siddhi Java library"},{"location":"docs/siddhi-as-a-java-library/#siddhi-52-as-a-java-library","text":"Siddhi can be used as a library in any Java program (including in OSGi runtimes) just by adding Siddhi and its extension jars as dependencies. Find a sample Siddhi project that's implemented as a Java program using Maven here , this can be used as a reference for any based implementation. Following are the mandatory dependencies that need to be added to the Maven pom.xml file (or to the program classpath). <dependency> <groupId>io.siddhi</groupId> <artifactId>siddhi-core</artifactId> <version>5.x.x</version> </dependency> <dependency> <groupId>io.siddhi</groupId> <artifactId>siddhi-query-api</artifactId> <version>5.x.x</version> </dependency> <dependency> <groupId>io.siddhi</groupId> <artifactId>siddhi-query-compiler</artifactId> <version>5.x.x</version> </dependency> <dependency> <groupId>io.siddhi</groupId> <artifactId>siddhi-annotations</artifactId> <version>5.x.x</version> </dependency>","title":"Siddhi 5.2 as a Java library"},{"location":"docs/siddhi-as-a-java-library/#sample","text":"Sample Java class using Siddhi is as follows.","title":"Sample"},{"location":"docs/siddhi-as-a-kubernetes-microservice/","text":"Siddhi 5.2 as a Kubernetes Microservice \u00b6 This section provides information on running Siddhi Apps natively in Kubernetes via Siddhi Kubernetes Operator. Siddhi can be configured in SiddhiProcess kind and passed to the CRD for deployment. Here, the Siddhi applications containing stream processing logic can be written inline in SiddhiProcess yaml or passed as .siddhi files via configmaps. SiddhiProcess yaml can also be configured with the necessary system configurations. Prerequisites \u00b6 A Kubernetes cluster v1.10.11 or higher. Minikube Google Kubernetes Engine(GKE) Cluster Docker for Mac Or any other Kubernetes cluster Admin privileges to install Siddhi operator Minikube Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Minikube using the following command. minikube addons enable ingress or disable Siddhi operator's automatically ingress creation . Google Kubernetes Engine (GKE) Cluster To install Siddhi operator, you have to give cluster admin permission to your account. In order to do that execute the following command (by replacing \"your-address@email.com\" with your account email address). kubectl create clusterrolebinding user-cluster-admin-binding --clusterrole=cluster-admin --user=your-address@email.com Docker for Mac Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Docker for mac following the official documentation or disable Siddhi operator's automatically ingress creation . Install Siddhi Operator \u00b6 To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.1.1/prerequisites.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.1.1/siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE siddhi-operator 1 1 1 1 1m siddhi-parser 1 1 1 1 1m Using a custom-built siddhi-runner image If you need to use a custom-built siddhi-runner image for all the SiddhiProcess deployments, you have to configure SIDDHI_RUNNER_IMAGE and SIDDHI_RUNNER_IMAGE_TAG environment variables with the custom-built image name and image tag respectively in the siddhi-operator.yaml file. Refer the documentation on creating custom siddhi-runner images bundling additional JARs here . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding kubernetes secret as SIDDHI_RUNNER_IMAGE_SECRET environment variable in the siddhi-operator.yaml file. For more details on using docker images from private registries/repositories refer this documentation . Deploy and run Siddhi App \u00b6 Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Here we will creating a very simple Siddhi stream processing application that consumes events via HTTP, filers the input events on the type 'monitored' and logs the output on the console. This can be created using a SiddhiProcess YAML file as given below. Always listen on 0.0.0.0 with the Siddhi Application running inside a container environment. If you listen on localhost inside the container, nothing outside the container can connect to your application. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Kubernetes Microservice refer Siddhi Config Guide . To deploy the above Siddhi app in your Kubernetes cluster, copy to a YAML file with name monitor-app.yaml and execute the following command. kubectl create -f <absolute-yaml-file-path>/monitor-app.yaml tls secret Within the SiddhiProcess, a tls secret named siddhi-tls is configured. If a Kubernetes secret with the same name does not exist in the Kubernetes cluster, the NGINX will ignore it and use a self-generated certificate. Configuring a secret will be necessary for calling HTTPS endpoints, refer deploy and run Siddhi apps with HTTPS section for more details. If the monitor-app is deployed successfully, the created SiddhiProcess, deployment, service, and ingress can be viewed as follows. $ kubectl get SiddhiProcesses NAME AGE monitor-app 2m $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE monitor-app 1 1 1 1 1m siddhi-operator 1 1 1 1 1m siddhi-parser 1 1 1 1 1m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 10d monitor-app ClusterIP 10.101.242.132 <none> 8280/TCP 1m siddhi-operator ClusterIP 10.111.138.250 <none> 8383/TCP 1m siddhi-parser LoadBalancer 10.102.172.142 <pending> 9090:31830/TCP 1m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Using a custom-built siddhi-runner image If you need to use a custom-built siddhi-runner image for a specific SiddhiProcess deployment, you have to configure image and imageTag arguments with the custom-built image name and image tag respectively in the monitor-app.yaml file. Refer the documentation on creating custom siddhi-runner images bundling additional JARs here . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding kubernetes secret as imagePullSecret argument in the monitor-app.yaml file. For more details on using docker images from private registries/repositories refer this documentation . Invoke Siddhi Applications To invoke the Siddhi App, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Docker for Mac For Docker for Mac, you have to use 0.0.0.0 as the external IP. Use the following CURL command to send events to monitor-app deployed in Kubernetes. curl -X POST \\ https://siddhi/monitor-app/8280/example \\ -H 'Content-Type: application/json' \\ -d '{ \"type\": \"monitored\", \"deviceID\": \"001\", \"power\": 341 }' -k Note: Here -k option is used to turn off curl's verification of the certificate. View Siddhi Process Logs Since the output of monitor-app is logged, you can see the output by monitoring the associated pod's logs. To find the monitor-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Here, the pod starting with the SiddhiProcess name (in this case monitor-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs <pod name> command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs monitor-app-7f8584875f-krz6t [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false} Get Siddhi process status \u00b6 List Siddhi processes \u00b6 List the Siddhi process using the kubectl get sps or kubectl get SiddhiProcesses commands as follows. $ kubectl get sps NAME AGE monitor-app 2m $ kubectl get SiddhiProcesses NAME AGE monitor-app 2m View Siddhi process configs \u00b6 Get the Siddhi process configuration details using kubectl describe sp command as follows. $ kubectl describe sp monitor-app Name: monitor-app Namespace: default Labels: <none> Annotations: <none> API Version: siddhi.io/v1alpha1 Kind: SiddhiProcess Metadata: Creation Timestamp: 2019-04-18T18:05:39Z Generation: 1 Resource Version: 497702 Self Link: /apis/siddhi.io/v1alpha1/namespaces/default/siddhiprocesses/monitor-app UID: 92b2293b-6204-11e9-996c-0800279e6dba Spec: Env: Name: RECEIVER_URL Value: http://0.0.0.0:8280/example Name: BASIC_AUTH_ENABLED Value: false Pod: Image: siddhiio/siddhi-runner-alpine Image Tag: 0.1.0 Query: @App:name(\"MonitorApp\") @App:description(\"Description of the plan\") @sink(type='log', prefix='LOGGER') @source(type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='${BASIC_AUTH_ENABLED}', @map(type='json')) define stream DevicePowerStream (type string, deviceID string, power int); define stream MonitorDevicesPowerStream(deviceID string, power int); @info(name='monitored-filter') from DevicePowerStream[type == 'monitored'] select deviceID, power insert into MonitorDevicesPowerStream; Siddhi . Runner . Configs: state.persistence: enabled: true intervalInMin: 5 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Status: Nodes: <nil> Status: Running Events: <none> Get the Siddhi process YAML using kubectl get sp command as follows. $ kubectl get sp monitor-app -o yaml apiVersion: siddhi.io/v1alpha1 kind: SiddhiProcess metadata: creationTimestamp: 2019-04-18T18:05:39Z generation: 1 name: monitor-app namespace: default resourceVersion: \"497702\" selfLink: /apis/siddhi.io/v1alpha1/namespaces/default/siddhiprocesses/monitor-app uid: 92b2293b-6204-11e9-996c-0800279e6dba spec: env: - name: RECEIVER_URL value: http://0.0.0.0:8280/example - name: BASIC_AUTH_ENABLED value: \"false\" pod: image: siddhiio/siddhi-runner-alpine imageTag: 0.1.0 query: \"@App:name(\\\"MonitorApp\\\")\\n@App:description(\\\"Description of the plan\\\") \\n\\n@sink(type='log', prefix='LOGGER')\\n@source(type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='${BASIC_AUTH_ENABLED}', @map(type='json'))\\ndefine stream DevicePowerStream (type string, deviceID string, power int);\\n\\n\\ndefine stream MonitorDevicesPowerStream(deviceID string, power int);\\n\\n@info(name='monitored-filter')\\nfrom DevicePowerStream[type == 'monitored']\\nselect deviceID, power\\ninsert into MonitorDevicesPowerStream;\\n\" siddhi.runner.configs: | state.persistence: enabled: true intervalInMin: 5 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence status: nodes: null status: Running View Siddhi process logs \u00b6 To view the Siddhi process logs, first get the Siddhi process pods using the kubectl get pods command as follows. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Then to retrieve the Siddhi process logs, run kubectl logs <pod name> command. Here <pod name> should be replaced with the name of the pod that starts with the relevant SiddhiProcess's name. A sample output logs is of this command is as follows. $ kubectl logs monitor-app-7f8584875f-krz6t JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0 RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-04-20 3:58:57,734] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner osgi> [2019-04-20 03:59:00,208] INFO {org.wso2.carbon.config.reader.ConfigFileReader} - Default deployment configuration updated with provided custom configuration file monitor-app-deployment.yaml [2019-04-20 03:59:01,551] INFO {org.wso2.msf4j.internal.websocket.WebSocketServerSC} - All required capabilities are available of WebSocket service component is available. [2019-04-20 03:59:01,584] INFO {org.wso2.carbon.metrics.core.config.model.JmxReporterConfig} - Creating JMX reporter for Metrics with domain 'org.wso2.carbon.metrics' [2019-04-20 03:59:01,609] INFO {org.wso2.msf4j.analytics.metrics.MetricsComponent} - Metrics Component is activated [2019-04-20 03:59:01,614] INFO {org.wso2.carbon.databridge.agent.internal.DataAgentDS} - Successfully deployed Agent Server [2019-04-20 03:59:02,219] INFO {io.siddhi.distribution.core.internal.ServiceComponent} - Periodic state persistence started with an interval of 5 using io.siddhi.distribution.core.persistence.FileSystemPersistenceStore [2019-04-20 03:59:02,229] INFO {io.siddhi.distribution.event.simulator.core.service.CSVFileDeployer} - CSV file deployer initiated. [2019-04-20 03:59:02,233] INFO {io.siddhi.distribution.event.simulator.core.service.SimulationConfigDeployer} - Simulation config deployer initiated. [2019-04-20 03:59:02,279] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiverServiceComponent} - org.wso2.carbon.databridge.receiver.binary.internal.Service Component is activated [2019-04-20 03:59:02,312] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary SSL Transport on port : 9712 [2019-04-20 03:59:02,321] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary TCP Transport on port : 9612 [2019-04-20 03:59:02,322] INFO {org.wso2.carbon.databridge.receiver.thrift.internal.ThriftDataReceiverDS} - Service Component is activated [2019-04-20 03:59:02,344] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift Server started at 0.0.0.0 [2019-04-20 03:59:02,356] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift SSL port : 7711 [2019-04-20 03:59:02,363] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift port : 7611 [2019-04-20 03:59:02,449] INFO {org.wso2.msf4j.internal.MicroservicesServerSC} - All microservices are available [2019-04-20 03:59:02,516] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-04-20 03:59:02,520] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-04-20 03:59:03,068] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Periodic State persistence enabled. Restoring last persisted state of MonitorApp [2019-04-20 03:59:03,075] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 8280 [2019-04-20 03:59:03,077] INFO {org.wso2.extension.siddhi.io.http.source.HttpConnectorPortBindingListener} - HTTP source 0.0.0.0:8280 has been started [2019-04-20 03:59:03,084] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Siddhi App MonitorApp deployed successfully [2019-04-20 03:59:03,093] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 5.941 sec [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false} Deploy and run Siddhi App using configmaps \u00b6 Siddhi operator allows you to deploy Siddhi app configurations via configmaps instead of just adding them inline. Through this you can also run multiple Siddhi Apps in a single SiddhiProcess. This can be done by passing the configmaps containing Siddhi app files to the SiddhiProcess's apps configuration as follows. apps: - config-map-name1 - config-map-name2 Sample on deploying and running Siddhi Apps via configmaps Here we will creating a very simple Siddhi application as follows, that consumes events via HTTP, filers the input events on type 'monitored' and logs the output on the console. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Save the above Siddhi App file as MonitorApp.siddhi , and use this file to create a Kubernetes config map with the name monitor-app-cm . This can be achieved by running the following command. kubectl create configmap monitor-app-cm --from-file=<absolute-file-path>/MonitorApp.siddhi The created config map can be added to SiddhiProcess YAML under the apps entry as follows. Save the YAML file as monitor-app.yaml , and use the following command to deploy the SiddhiProcess. kubectl create -f <absolute-yaml-file-path>/monitor-app.yaml Using a config, created from a directory containing multiple Siddhi files SiddhiProcess's apps configuration also supports a config map that is created from a directory containing multiple Siddhi files. Use kubectl create configmap siddhi-apps --from-file=<DIRECTORY_PATH> command to create a config map from a directory. Invoke Siddhi Applications To invoke the Siddhi Apps, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Use the following CURL command to send events to monitor-app deployed in Kubernetes. curl -X POST \\ https://siddhi/monitor-app/8280/example \\ -H 'Content-Type: application/json' \\ -d '{ \"type\": \"monitored\", \"deviceID\": \"001\", \"power\": 341 }' -k Note: Here -k option is used to turn off curl's verification of the certificate. View Siddhi Process Logs Since the output of monitor-app is logged, you can see the output by monitoring the associated pod's logs. To find the monitor-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Here, the pod starting with the SiddhiProcess name (in this case monitor-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs <pod name> command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs monitor-app-7f8584875f-krz6t [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false} Deploy Siddhi Apps without Ingress creation \u00b6 By default, Siddhi operator creates an NGINX ingress and exposes your HTTP/HTTPS through that ingress. If you need to disable automatic ingress creation, you have to change the AUTO_INGRESS_CREATION value in the Siddhi operator.yaml file to false or null as below. Deploy and run Siddhi App with HTTPS \u00b6 Configuring tls will allow Siddhi ingress NGINX to expose HTTPS endpoints of your Siddhi Apps. To do so, created a Kubernetes secret and add that to the SiddhiProcess's tls configuration as following. tls: ingressSecret: siddhi-tls Sample on deploying and running Siddhi App with HTTPS First, you need to create a certificate using the following commands. For more details about the certificate creation refers this . openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout siddhi.key -out siddhi.crt -subj \"/CN=siddhi/O=siddhi\" After that, create a kubernetes secret called siddhi-tls , which we intended to add to the TLS configurations using the following command. kubectl create secret tls siddhi-tls --key siddhi.key --cert siddhi.crt The created secret then need to be added to the created SiddhiProcess's tls configuration as following. When this is done Siddhi operator will now enable TLS support via the NGINX ingress, and you will be able to access all the HTTPS endpoints. Invoke Siddhi Applications You can use now send the events to following HTTPS endpoint. https://siddhi/monitor-app/8280/example Further, you can use the following CURL command to send a request to the deployed siddhi applications via HTTPS. curl --cacert siddhi.crt -X POST \\ https://siddhi/monitor-app/8280/example \\ -H 'Content-Type: application/json' \\ -d '{ \"type\": \"monitored\", \"deviceID\": \"001\", \"power\": 341 }' View Siddhi Process Logs The output logs show the event that you sent using the previous CURL command. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-667c97c898-rrtfs 1/1 Running 0 2m siddhi-operator-79dcc45959-fkk4d 1/1 Running 0 3m siddhi-parser-64d4cd86ff-k8b87 1/1 Running 0 3m $ kubectl logs monitor-app-667c97c898-rrtfs JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0 RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-05-06 5:36:54,894] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner osgi> [2019-05-06 05:36:57,692] INFO {org.wso2.msf4j.internal.websocket.WebSocketServerSC} - All required capabilities are available of WebSocket service component is available. [2019-05-06 05:36:57,749] INFO {org.wso2.carbon.metrics.core.config.model.JmxReporterConfig} - Creating JMX reporter for Metrics with domain 'org.wso2.carbon.metrics' [2019-05-06 05:36:57,779] INFO {org.wso2.msf4j.analytics.metrics.MetricsComponent} - Metrics Component is activated [2019-05-06 05:36:57,784] INFO {org.wso2.carbon.databridge.agent.internal.DataAgentDS} - Successfully deployed Agent Server [2019-05-06 05:36:58,292] INFO {io.siddhi.distribution.event.simulator.core.service.CSVFileDeployer} - CSV file deployer initiated. [2019-05-06 05:36:58,295] INFO {io.siddhi.distribution.event.simulator.core.service.SimulationConfigDeployer} - Simulation config deployer initiated. [2019-05-06 05:36:58,331] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiverServiceComponent} - org.wso2.carbon.databridge.receiver.binary.internal.Service Component is activated [2019-05-06 05:36:58,342] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary SSL Transport on port : 9712 [2019-05-06 05:36:58,343] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary TCP Transport on port : 9612 [2019-05-06 05:36:58,343] INFO {org.wso2.carbon.databridge.receiver.thrift.internal.ThriftDataReceiverDS} - Service Component is activated [2019-05-06 05:36:58,360] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift Server started at 0.0.0.0 [2019-05-06 05:36:58,369] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift SSL port : 7711 [2019-05-06 05:36:58,371] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift port : 7611 [2019-05-06 05:36:58,466] INFO {org.wso2.msf4j.internal.MicroservicesServerSC} - All microservices are available [2019-05-06 05:36:58,567] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-05-06 05:36:58,574] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-05-06 05:36:59,091] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 8280 [2019-05-06 05:36:59,092] INFO {org.wso2.extension.siddhi.io.http.source.HttpConnectorPortBindingListener} - HTTP source 0.0.0.0:8280 has been started [2019-05-06 05:36:59,093] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Siddhi App MonitorApp deployed successfully [2019-05-06 05:36:59,100] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 4.710 sec [2019-05-06 05:39:33,804] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1557121173802, data=[monitored, 001, 341], isExpired=false}","title":"Siddhi Kubernetes Microservice"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#siddhi-52-as-a-kubernetes-microservice","text":"This section provides information on running Siddhi Apps natively in Kubernetes via Siddhi Kubernetes Operator. Siddhi can be configured in SiddhiProcess kind and passed to the CRD for deployment. Here, the Siddhi applications containing stream processing logic can be written inline in SiddhiProcess yaml or passed as .siddhi files via configmaps. SiddhiProcess yaml can also be configured with the necessary system configurations.","title":"Siddhi 5.2 as a Kubernetes Microservice"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#prerequisites","text":"A Kubernetes cluster v1.10.11 or higher. Minikube Google Kubernetes Engine(GKE) Cluster Docker for Mac Or any other Kubernetes cluster Admin privileges to install Siddhi operator Minikube Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Minikube using the following command. minikube addons enable ingress or disable Siddhi operator's automatically ingress creation . Google Kubernetes Engine (GKE) Cluster To install Siddhi operator, you have to give cluster admin permission to your account. In order to do that execute the following command (by replacing \"your-address@email.com\" with your account email address). kubectl create clusterrolebinding user-cluster-admin-binding --clusterrole=cluster-admin --user=your-address@email.com Docker for Mac Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Docker for mac following the official documentation or disable Siddhi operator's automatically ingress creation .","title":"Prerequisites"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#install-siddhi-operator","text":"To install the Siddhi Kubernetes operator run the following commands. kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.1.1/prerequisites.yaml kubectl apply -f https://github.com/siddhi-io/siddhi-operator/releases/download/v0.1.1/siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE siddhi-operator 1 1 1 1 1m siddhi-parser 1 1 1 1 1m Using a custom-built siddhi-runner image If you need to use a custom-built siddhi-runner image for all the SiddhiProcess deployments, you have to configure SIDDHI_RUNNER_IMAGE and SIDDHI_RUNNER_IMAGE_TAG environment variables with the custom-built image name and image tag respectively in the siddhi-operator.yaml file. Refer the documentation on creating custom siddhi-runner images bundling additional JARs here . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding kubernetes secret as SIDDHI_RUNNER_IMAGE_SECRET environment variable in the siddhi-operator.yaml file. For more details on using docker images from private registries/repositories refer this documentation .","title":"Install Siddhi Operator"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app","text":"Siddhi applications can be deployed on Kubernetes using the Siddhi operator. Here we will creating a very simple Siddhi stream processing application that consumes events via HTTP, filers the input events on the type 'monitored' and logs the output on the console. This can be created using a SiddhiProcess YAML file as given below. Always listen on 0.0.0.0 with the Siddhi Application running inside a container environment. If you listen on localhost inside the container, nothing outside the container can connect to your application. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Kubernetes Microservice refer Siddhi Config Guide . To deploy the above Siddhi app in your Kubernetes cluster, copy to a YAML file with name monitor-app.yaml and execute the following command. kubectl create -f <absolute-yaml-file-path>/monitor-app.yaml tls secret Within the SiddhiProcess, a tls secret named siddhi-tls is configured. If a Kubernetes secret with the same name does not exist in the Kubernetes cluster, the NGINX will ignore it and use a self-generated certificate. Configuring a secret will be necessary for calling HTTPS endpoints, refer deploy and run Siddhi apps with HTTPS section for more details. If the monitor-app is deployed successfully, the created SiddhiProcess, deployment, service, and ingress can be viewed as follows. $ kubectl get SiddhiProcesses NAME AGE monitor-app 2m $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE monitor-app 1 1 1 1 1m siddhi-operator 1 1 1 1 1m siddhi-parser 1 1 1 1 1m $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 10d monitor-app ClusterIP 10.101.242.132 <none> 8280/TCP 1m siddhi-operator ClusterIP 10.111.138.250 <none> 8383/TCP 1m siddhi-parser LoadBalancer 10.102.172.142 <pending> 9090:31830/TCP 1m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Using a custom-built siddhi-runner image If you need to use a custom-built siddhi-runner image for a specific SiddhiProcess deployment, you have to configure image and imageTag arguments with the custom-built image name and image tag respectively in the monitor-app.yaml file. Refer the documentation on creating custom siddhi-runner images bundling additional JARs here . If you are pulling the custom-built image from a private Docker registry/repository, specify the corresponding kubernetes secret as imagePullSecret argument in the monitor-app.yaml file. For more details on using docker images from private registries/repositories refer this documentation . Invoke Siddhi Applications To invoke the Siddhi App, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Docker for Mac For Docker for Mac, you have to use 0.0.0.0 as the external IP. Use the following CURL command to send events to monitor-app deployed in Kubernetes. curl -X POST \\ https://siddhi/monitor-app/8280/example \\ -H 'Content-Type: application/json' \\ -d '{ \"type\": \"monitored\", \"deviceID\": \"001\", \"power\": 341 }' -k Note: Here -k option is used to turn off curl's verification of the certificate. View Siddhi Process Logs Since the output of monitor-app is logged, you can see the output by monitoring the associated pod's logs. To find the monitor-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Here, the pod starting with the SiddhiProcess name (in this case monitor-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs <pod name> command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs monitor-app-7f8584875f-krz6t [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false}","title":"Deploy and run Siddhi App"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#get-siddhi-process-status","text":"","title":"Get Siddhi process status"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#list-siddhi-processes","text":"List the Siddhi process using the kubectl get sps or kubectl get SiddhiProcesses commands as follows. $ kubectl get sps NAME AGE monitor-app 2m $ kubectl get SiddhiProcesses NAME AGE monitor-app 2m","title":"List Siddhi processes"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#view-siddhi-process-configs","text":"Get the Siddhi process configuration details using kubectl describe sp command as follows. $ kubectl describe sp monitor-app Name: monitor-app Namespace: default Labels: <none> Annotations: <none> API Version: siddhi.io/v1alpha1 Kind: SiddhiProcess Metadata: Creation Timestamp: 2019-04-18T18:05:39Z Generation: 1 Resource Version: 497702 Self Link: /apis/siddhi.io/v1alpha1/namespaces/default/siddhiprocesses/monitor-app UID: 92b2293b-6204-11e9-996c-0800279e6dba Spec: Env: Name: RECEIVER_URL Value: http://0.0.0.0:8280/example Name: BASIC_AUTH_ENABLED Value: false Pod: Image: siddhiio/siddhi-runner-alpine Image Tag: 0.1.0 Query: @App:name(\"MonitorApp\") @App:description(\"Description of the plan\") @sink(type='log', prefix='LOGGER') @source(type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='${BASIC_AUTH_ENABLED}', @map(type='json')) define stream DevicePowerStream (type string, deviceID string, power int); define stream MonitorDevicesPowerStream(deviceID string, power int); @info(name='monitored-filter') from DevicePowerStream[type == 'monitored'] select deviceID, power insert into MonitorDevicesPowerStream; Siddhi . Runner . Configs: state.persistence: enabled: true intervalInMin: 5 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Status: Nodes: <nil> Status: Running Events: <none> Get the Siddhi process YAML using kubectl get sp command as follows. $ kubectl get sp monitor-app -o yaml apiVersion: siddhi.io/v1alpha1 kind: SiddhiProcess metadata: creationTimestamp: 2019-04-18T18:05:39Z generation: 1 name: monitor-app namespace: default resourceVersion: \"497702\" selfLink: /apis/siddhi.io/v1alpha1/namespaces/default/siddhiprocesses/monitor-app uid: 92b2293b-6204-11e9-996c-0800279e6dba spec: env: - name: RECEIVER_URL value: http://0.0.0.0:8280/example - name: BASIC_AUTH_ENABLED value: \"false\" pod: image: siddhiio/siddhi-runner-alpine imageTag: 0.1.0 query: \"@App:name(\\\"MonitorApp\\\")\\n@App:description(\\\"Description of the plan\\\") \\n\\n@sink(type='log', prefix='LOGGER')\\n@source(type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='${BASIC_AUTH_ENABLED}', @map(type='json'))\\ndefine stream DevicePowerStream (type string, deviceID string, power int);\\n\\n\\ndefine stream MonitorDevicesPowerStream(deviceID string, power int);\\n\\n@info(name='monitored-filter')\\nfrom DevicePowerStream[type == 'monitored']\\nselect deviceID, power\\ninsert into MonitorDevicesPowerStream;\\n\" siddhi.runner.configs: | state.persistence: enabled: true intervalInMin: 5 revisionsToKeep: 2 persistenceStore: io.siddhi.distribution.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence status: nodes: null status: Running","title":"View Siddhi process configs"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#view-siddhi-process-logs","text":"To view the Siddhi process logs, first get the Siddhi process pods using the kubectl get pods command as follows. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Then to retrieve the Siddhi process logs, run kubectl logs <pod name> command. Here <pod name> should be replaced with the name of the pod that starts with the relevant SiddhiProcess's name. A sample output logs is of this command is as follows. $ kubectl logs monitor-app-7f8584875f-krz6t JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0 RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-04-20 3:58:57,734] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner osgi> [2019-04-20 03:59:00,208] INFO {org.wso2.carbon.config.reader.ConfigFileReader} - Default deployment configuration updated with provided custom configuration file monitor-app-deployment.yaml [2019-04-20 03:59:01,551] INFO {org.wso2.msf4j.internal.websocket.WebSocketServerSC} - All required capabilities are available of WebSocket service component is available. [2019-04-20 03:59:01,584] INFO {org.wso2.carbon.metrics.core.config.model.JmxReporterConfig} - Creating JMX reporter for Metrics with domain 'org.wso2.carbon.metrics' [2019-04-20 03:59:01,609] INFO {org.wso2.msf4j.analytics.metrics.MetricsComponent} - Metrics Component is activated [2019-04-20 03:59:01,614] INFO {org.wso2.carbon.databridge.agent.internal.DataAgentDS} - Successfully deployed Agent Server [2019-04-20 03:59:02,219] INFO {io.siddhi.distribution.core.internal.ServiceComponent} - Periodic state persistence started with an interval of 5 using io.siddhi.distribution.core.persistence.FileSystemPersistenceStore [2019-04-20 03:59:02,229] INFO {io.siddhi.distribution.event.simulator.core.service.CSVFileDeployer} - CSV file deployer initiated. [2019-04-20 03:59:02,233] INFO {io.siddhi.distribution.event.simulator.core.service.SimulationConfigDeployer} - Simulation config deployer initiated. [2019-04-20 03:59:02,279] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiverServiceComponent} - org.wso2.carbon.databridge.receiver.binary.internal.Service Component is activated [2019-04-20 03:59:02,312] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary SSL Transport on port : 9712 [2019-04-20 03:59:02,321] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary TCP Transport on port : 9612 [2019-04-20 03:59:02,322] INFO {org.wso2.carbon.databridge.receiver.thrift.internal.ThriftDataReceiverDS} - Service Component is activated [2019-04-20 03:59:02,344] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift Server started at 0.0.0.0 [2019-04-20 03:59:02,356] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift SSL port : 7711 [2019-04-20 03:59:02,363] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift port : 7611 [2019-04-20 03:59:02,449] INFO {org.wso2.msf4j.internal.MicroservicesServerSC} - All microservices are available [2019-04-20 03:59:02,516] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-04-20 03:59:02,520] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-04-20 03:59:03,068] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Periodic State persistence enabled. Restoring last persisted state of MonitorApp [2019-04-20 03:59:03,075] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 8280 [2019-04-20 03:59:03,077] INFO {org.wso2.extension.siddhi.io.http.source.HttpConnectorPortBindingListener} - HTTP source 0.0.0.0:8280 has been started [2019-04-20 03:59:03,084] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Siddhi App MonitorApp deployed successfully [2019-04-20 03:59:03,093] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 5.941 sec [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false}","title":"View Siddhi process logs"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app-using-configmaps","text":"Siddhi operator allows you to deploy Siddhi app configurations via configmaps instead of just adding them inline. Through this you can also run multiple Siddhi Apps in a single SiddhiProcess. This can be done by passing the configmaps containing Siddhi app files to the SiddhiProcess's apps configuration as follows. apps: - config-map-name1 - config-map-name2 Sample on deploying and running Siddhi Apps via configmaps Here we will creating a very simple Siddhi application as follows, that consumes events via HTTP, filers the input events on type 'monitored' and logs the output on the console. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Save the above Siddhi App file as MonitorApp.siddhi , and use this file to create a Kubernetes config map with the name monitor-app-cm . This can be achieved by running the following command. kubectl create configmap monitor-app-cm --from-file=<absolute-file-path>/MonitorApp.siddhi The created config map can be added to SiddhiProcess YAML under the apps entry as follows. Save the YAML file as monitor-app.yaml , and use the following command to deploy the SiddhiProcess. kubectl create -f <absolute-yaml-file-path>/monitor-app.yaml Using a config, created from a directory containing multiple Siddhi files SiddhiProcess's apps configuration also supports a config map that is created from a directory containing multiple Siddhi files. Use kubectl create configmap siddhi-apps --from-file=<DIRECTORY_PATH> command to create a config map from a directory. Invoke Siddhi Applications To invoke the Siddhi Apps, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80, 443 1m Then, add the host siddhi and related external IP ( ADDRESS ) to the /etc/hosts file in your machine. Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Use the following CURL command to send events to monitor-app deployed in Kubernetes. curl -X POST \\ https://siddhi/monitor-app/8280/example \\ -H 'Content-Type: application/json' \\ -d '{ \"type\": \"monitored\", \"deviceID\": \"001\", \"power\": 341 }' -k Note: Here -k option is used to turn off curl's verification of the certificate. View Siddhi Process Logs Since the output of monitor-app is logged, you can see the output by monitoring the associated pod's logs. To find the monitor-app pod use the kubectl get pods command. This will list down all the deployed pods. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-7f8584875f-krz6t 1/1 Running 0 2m siddhi-operator-8589c4fc69-6xbtx 1/1 Running 0 2m siddhi-parser-64d4cd86ff-pfq2s 1/1 Running 0 2m Here, the pod starting with the SiddhiProcess name (in this case monitor-app- ) is the pod we need to monitor. To view the logs, run the kubectl logs <pod name> command. This will show all the Siddhi process logs, along with the filtered output events as given below. $ kubectl logs monitor-app-7f8584875f-krz6t [2019-04-20 04:04:02,216] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has paused for http://0.0.0.0:8280/example [2019-04-20 04:04:02,235] INFO {org.wso2.extension.siddhi.io.http.source.HttpSourceListener} - Event input has resume for http://0.0.0.0:8280/example [2019-04-20 04:05:29,741] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1555733129736, data=[monitored, 001, 341], isExpired=false}","title":"Deploy and run Siddhi App using configmaps"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-siddhi-apps-without-ingress-creation","text":"By default, Siddhi operator creates an NGINX ingress and exposes your HTTP/HTTPS through that ingress. If you need to disable automatic ingress creation, you have to change the AUTO_INGRESS_CREATION value in the Siddhi operator.yaml file to false or null as below.","title":"Deploy Siddhi Apps without Ingress creation"},{"location":"docs/siddhi-as-a-kubernetes-microservice/#deploy-and-run-siddhi-app-with-https","text":"Configuring tls will allow Siddhi ingress NGINX to expose HTTPS endpoints of your Siddhi Apps. To do so, created a Kubernetes secret and add that to the SiddhiProcess's tls configuration as following. tls: ingressSecret: siddhi-tls Sample on deploying and running Siddhi App with HTTPS First, you need to create a certificate using the following commands. For more details about the certificate creation refers this . openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout siddhi.key -out siddhi.crt -subj \"/CN=siddhi/O=siddhi\" After that, create a kubernetes secret called siddhi-tls , which we intended to add to the TLS configurations using the following command. kubectl create secret tls siddhi-tls --key siddhi.key --cert siddhi.crt The created secret then need to be added to the created SiddhiProcess's tls configuration as following. When this is done Siddhi operator will now enable TLS support via the NGINX ingress, and you will be able to access all the HTTPS endpoints. Invoke Siddhi Applications You can use now send the events to following HTTPS endpoint. https://siddhi/monitor-app/8280/example Further, you can use the following CURL command to send a request to the deployed siddhi applications via HTTPS. curl --cacert siddhi.crt -X POST \\ https://siddhi/monitor-app/8280/example \\ -H 'Content-Type: application/json' \\ -d '{ \"type\": \"monitored\", \"deviceID\": \"001\", \"power\": 341 }' View Siddhi Process Logs The output logs show the event that you sent using the previous CURL command. $ kubectl get pods NAME READY STATUS RESTARTS AGE monitor-app-667c97c898-rrtfs 1/1 Running 0 2m siddhi-operator-79dcc45959-fkk4d 1/1 Running 0 3m siddhi-parser-64d4cd86ff-k8b87 1/1 Running 0 3m $ kubectl logs monitor-app-667c97c898-rrtfs JAVA_HOME environment variable is set to /opt/java/openjdk CARBON_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0 RUNTIME_HOME environment variable is set to /home/siddhi_user/siddhi-runner-0.1.0/wso2/runner Picked up JAVA_TOOL_OPTIONS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap [2019-05-06 5:36:54,894] INFO {org.wso2.carbon.launcher.extensions.OSGiLibBundleDeployerUtils updateOSGiLib} - Successfully updated the OSGi bundle information of Carbon Runtime: runner osgi> [2019-05-06 05:36:57,692] INFO {org.wso2.msf4j.internal.websocket.WebSocketServerSC} - All required capabilities are available of WebSocket service component is available. [2019-05-06 05:36:57,749] INFO {org.wso2.carbon.metrics.core.config.model.JmxReporterConfig} - Creating JMX reporter for Metrics with domain 'org.wso2.carbon.metrics' [2019-05-06 05:36:57,779] INFO {org.wso2.msf4j.analytics.metrics.MetricsComponent} - Metrics Component is activated [2019-05-06 05:36:57,784] INFO {org.wso2.carbon.databridge.agent.internal.DataAgentDS} - Successfully deployed Agent Server [2019-05-06 05:36:58,292] INFO {io.siddhi.distribution.event.simulator.core.service.CSVFileDeployer} - CSV file deployer initiated. [2019-05-06 05:36:58,295] INFO {io.siddhi.distribution.event.simulator.core.service.SimulationConfigDeployer} - Simulation config deployer initiated. [2019-05-06 05:36:58,331] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiverServiceComponent} - org.wso2.carbon.databridge.receiver.binary.internal.Service Component is activated [2019-05-06 05:36:58,342] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary SSL Transport on port : 9712 [2019-05-06 05:36:58,343] INFO {org.wso2.carbon.databridge.receiver.binary.internal.BinaryDataReceiver} - Started Binary TCP Transport on port : 9612 [2019-05-06 05:36:58,343] INFO {org.wso2.carbon.databridge.receiver.thrift.internal.ThriftDataReceiverDS} - Service Component is activated [2019-05-06 05:36:58,360] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift Server started at 0.0.0.0 [2019-05-06 05:36:58,369] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift SSL port : 7711 [2019-05-06 05:36:58,371] INFO {org.wso2.carbon.databridge.receiver.thrift.ThriftDataReceiver} - Thrift port : 7611 [2019-05-06 05:36:58,466] INFO {org.wso2.msf4j.internal.MicroservicesServerSC} - All microservices are available [2019-05-06 05:36:58,567] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9090 [2019-05-06 05:36:58,574] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 9443 [2019-05-06 05:36:59,091] INFO {org.wso2.transport.http.netty.contractimpl.listener.ServerConnectorBootstrap$HttpServerConnector} - HTTP(S) Interface starting on host 0.0.0.0 and port 8280 [2019-05-06 05:36:59,092] INFO {org.wso2.extension.siddhi.io.http.source.HttpConnectorPortBindingListener} - HTTP source 0.0.0.0:8280 has been started [2019-05-06 05:36:59,093] INFO {io.siddhi.distribution.core.internal.StreamProcessorService} - Siddhi App MonitorApp deployed successfully [2019-05-06 05:36:59,100] INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - Siddhi Runner Distribution started in 4.710 sec [2019-05-06 05:39:33,804] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1557121173802, data=[monitored, 001, 341], isExpired=false}","title":"Deploy and run Siddhi App with HTTPS"},{"location":"docs/siddhi-as-a-local-microservice/","text":"Siddhi 5.2 as a Local Microservice \u00b6 This section provides information on running Siddhi Apps on Bare Metal or VM. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Local Microservice is as follows. Download the latest Siddhi Runner distribution Unzip the siddhi-runner-x.x.x.zip Start SiddhiApps with the runner config by executing the following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps=<siddhi-file> -Dconfig=<config-yaml-file> Windows : bin\\runner.bat -Dapps=<siddhi-file> -Dconfig=<config-yaml-file> Running Multiple SiddhiApps in one runner. To run multiple SiddhiApps in one runtime, have all SiddhiApps in a directory and pass its location through -Dapps parameter as follows, -Dapps=<siddhi-apps-directory> Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Local Microservice refer Siddhi Config Guide . Samples \u00b6 Running Siddhi App \u00b6 Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps=<absolute-siddhi-file-path>/CountOverTime.siddhi Windows : bin\\runner.bat -Dapps=<absolute-siddhi-file-path>\\CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false} Running with runner config \u00b6 When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can by configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps=<absolute-siddhi-file-path>/ConsumeAndStore.siddhi \\ -Dconfig=<absolute-config-yaml-path>/TestDb.yaml Windows : bin\\runner.sh -Dapps=<absolute-siddhi-file-path>\\ConsumeAndStore.siddhi ^ -Dconfig=<absolute-config-yaml-path>\\TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] } Running with environmental/system variables \u00b6 Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set environment variables by running following in the termial Siddhi is about to run: export THRESHOLD=20 export TO_EMAIL=<to email address> export EMAIL_ADDRESS=<gmail address> export EMAIL_USERNAME=<gmail username> export EMAIL_PASSWORD=<gmail password> Or they can also be passed as system variables by adding -DTHRESHOLD=20 -DTO_EMAIL=>to email address> -DEMAIL_ADDRESS=<gmail address> -DEMAIL_USERNAME=<gmail username> -DEMAIL_PASSWORD=<gmail password> to the end of the runner startup script. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps=<absolute-file-path>/TemplatedFilterAndEmail.siddhi \\ -Dconfig=<absolute-config-yaml-path>/EmailConfig.yaml Windows : bin\\runner.bat -Dapps=<absolute-file-path>\\TemplatedFilterAndEmail.siddhi ^ -Dconfig=<absolute-config-yaml-path>\\EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Siddhi Local Microservice"},{"location":"docs/siddhi-as-a-local-microservice/#siddhi-52-as-a-local-microservice","text":"This section provides information on running Siddhi Apps on Bare Metal or VM. Siddhi Microservice can run one or more Siddhi Applications with required system configurations. Here, the Siddhi application ( .siddhi file) contains stream processing logic and the necessary system configurations can be passed via the Siddhi configuration .yaml file. Steps to Run Siddhi Local Microservice is as follows. Download the latest Siddhi Runner distribution Unzip the siddhi-runner-x.x.x.zip Start SiddhiApps with the runner config by executing the following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps=<siddhi-file> -Dconfig=<config-yaml-file> Windows : bin\\runner.bat -Dapps=<siddhi-file> -Dconfig=<config-yaml-file> Running Multiple SiddhiApps in one runner. To run multiple SiddhiApps in one runtime, have all SiddhiApps in a directory and pass its location through -Dapps parameter as follows, -Dapps=<siddhi-apps-directory> Always use absolute path for SiddhiApps and runner configs. Providing absolute path of SiddhiApp file, or directory in -Dapps parameter, and when providing the Siddhi runner config yaml on -Dconfig parameter while starting Siddhi runner. Siddhi Tooling You can also use the powerful Siddhi Editor to implement and test steam processing applications. Configuring Siddhi To configure databases, extensions, authentication, periodic state persistence, and statistics for Siddhi as Local Microservice refer Siddhi Config Guide .","title":"Siddhi 5.2 as a Local Microservice"},{"location":"docs/siddhi-as-a-local-microservice/#samples","text":"","title":"Samples"},{"location":"docs/siddhi-as-a-local-microservice/#running-siddhi-app","text":"Following SiddhiApp collects events via HTTP and logs the number of events arrived during last 15 seconds. Copy the above SiddhiApp, and create the SiddhiApp file CountOverTime.siddhi . Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps=<absolute-siddhi-file-path>/CountOverTime.siddhi Windows : bin\\runner.bat -Dapps=<absolute-siddhi-file-path>\\CountOverTime.siddhi Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Runner logs the total count on the console. Note, how the count increments with every event sent. [2019-04-11 13:36:03,517] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969963512, data=[1], isExpired=false} [2019-04-11 13:36:10,267] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554969970267, data=[2], isExpired=false} [2019-04-11 13:36:41,694] INFO {io.siddhi.core.stream.output.sink.LogSink} - CountOverTime : TotalCountStream : Event{timestamp=1554970001694, data=[1], isExpired=false}","title":"Running Siddhi App"},{"location":"docs/siddhi-as-a-local-microservice/#running-with-runner-config","text":"When running SiddhiApps users can optionally provide a config yaml to Siddhi runner to manage configurations such as state persistence, databases connections and secure vault. Following SiddhiApp collects events via HTTP and store them in H2 Database. The runner config can by configured with the relevant datasource information and passed when starting the runner Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file ConsumeAndStore.siddhi and TestDb.yaml files. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps=<absolute-siddhi-file-path>/ConsumeAndStore.siddhi \\ -Dconfig=<absolute-config-yaml-path>/TestDb.yaml Windows : bin\\runner.sh -Dapps=<absolute-siddhi-file-path>\\ConsumeAndStore.siddhi ^ -Dconfig=<absolute-config-yaml-path>\\TestDb.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":20.12}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Query Siddhi Store APIs to retrieve 10 records from the table. Query stored events with curl command: Publish few json to the http endpoint as follows, curl -X POST https://localhost:9443/stores/query \\ -H \"content-type: application/json\" \\ -u \"admin:admin\" \\ -d '{\"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" }' -k Query stored events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'https://localhost:9443/stores/query' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"appName\" : \"ConsumeAndStore\", \"query\" : \"from ProductionTable select * limit 10;\" } The results of the query will be as follows, { \"records\":[ [\"Cake\",20.12] ] }","title":"Running with runner config"},{"location":"docs/siddhi-as-a-local-microservice/#running-with-environmentalsystem-variables","text":"Templating SiddhiApps allows users to provide environment/system variables to siddhiApps at runtime. This can help users to migrate SiddhiApps from one environment to another (E.g from dev, test and to prod). Following templated SiddhiApp collects events via HTTP, filters them based on amount greater than a given threshold value, and only sends the filtered events via email. Here the THRESHOLD value, and TO_EMAIL are templated in the TemplatedFilterAndEmail.siddhi SiddhiApp. The runner config is configured with a gmail account to send email messages in EmailConfig.yaml by templating sending EMAIL_ADDRESS , EMAIL_USERNAME and EMAIL_PASSWORD . Copy the above SiddhiApp, & config yaml, and create corresponding the SiddhiApp file TemplatedFilterAndEmail.siddhi and EmailConfig.yaml files. Set environment variables by running following in the termial Siddhi is about to run: export THRESHOLD=20 export TO_EMAIL=<to email address> export EMAIL_ADDRESS=<gmail address> export EMAIL_USERNAME=<gmail username> export EMAIL_PASSWORD=<gmail password> Or they can also be passed as system variables by adding -DTHRESHOLD=20 -DTO_EMAIL=>to email address> -DEMAIL_ADDRESS=<gmail address> -DEMAIL_USERNAME=<gmail username> -DEMAIL_PASSWORD=<gmail password> to the end of the runner startup script. Run the SiddhiApp by executing following commands from the distribution directory Linux/Mac : ./bin/runner.sh -Dapps=<absolute-file-path>/TemplatedFilterAndEmail.siddhi \\ -Dconfig=<absolute-config-yaml-path>/EmailConfig.yaml Windows : bin\\runner.bat -Dapps=<absolute-file-path>\\TemplatedFilterAndEmail.siddhi ^ -Dconfig=<absolute-config-yaml-path>\\EmailConfig.yaml Test the SiddhiApp by calling the HTTP endpoint using curl or Postman as follows Publish events with curl command: Publish few json to the http endpoint as follows, curl -X POST http://localhost:8006/production \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"name\":\"Cake\",\"amount\":2000.0}}' Publish events with Postman: Install 'Postman' application from Chrome web store Launch the application Make a 'Post' request to 'http://localhost:8006/production' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"Cake\", \"amount\": 2000.0 } } Check the to.email for the published email message, which will look as follows, Subject : High Cake production! Hi, High production of Cake, with amount 2000.0 identified. For more information please contact production department. Thank you","title":"Running with environmental/system variables"},{"location":"docs/tooling/","text":"Siddhi 5.2 Tooling \u00b6 Siddhi Editor \u00b6 Siddhi provides tooling that supports following features to develop and test stream processing applications: Text Query Editor with syntax highlighting and advanced auto completion support. Event Simulator and Debugger to test Siddhi Applications. Graphical Query Editor with drag and drop query building support. Graphical Query Editor Text Query Editor","title":"Tooling"},{"location":"docs/tooling/#siddhi-52-tooling","text":"","title":"Siddhi 5.2 Tooling"},{"location":"docs/tooling/#siddhi-editor","text":"Siddhi provides tooling that supports following features to develop and test stream processing applications: Text Query Editor with syntax highlighting and advanced auto completion support. Event Simulator and Debugger to test Siddhi Applications. Graphical Query Editor with drag and drop query building support. Graphical Query Editor Text Query Editor","title":"Siddhi Editor"},{"location":"docs/api/5.2.0/","text":"API Docs - v5.2.0 \u00b6 Core \u00b6 and (Aggregate Function) \u00b6 Returns the results of AND operation for all the events. Syntax <BOOL> and(<BOOL> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch. avg (Aggregate Function) \u00b6 Calculates the average for all the events. Syntax <DOUBLE> avg(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry. count (Aggregate Function) \u00b6 Returns the count of all the events. Syntax <LONG> count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds. distinctCount (Aggregate Function) \u00b6 This returns the count of distinct occurrences for a given arg. Syntax <LONG> distinctCount(<INT|LONG|DOUBLE|FLOAT|STRING> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'. max (Aggregate Function) \u00b6 Returns the maximum value for all the events. Syntax <INT|LONG|DOUBLE|FLOAT> max(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry. maxForever (Aggregate Function) \u00b6 This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax <INT|LONG|DOUBLE|FLOAT> maxForever(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query. min (Aggregate Function) \u00b6 Returns the minimum value for all the events. Syntax <INT|LONG|DOUBLE|FLOAT> min(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry. minForever (Aggregate Function) \u00b6 This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax <INT|LONG|DOUBLE|FLOAT> minForever(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query. or (Aggregate Function) \u00b6 Returns the results of OR operation for all the events. Syntax <BOOL> or(<BOOL> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch. stdDev (Aggregate Function) \u00b6 Returns the calculated standard deviation for all the events. Syntax <DOUBLE> stdDev(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry. sum (Aggregate Function) \u00b6 Returns the sum for all the events. Syntax <LONG|DOUBLE> sum(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry. unionSet (Aggregate Function) \u00b6 Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax <OBJECT> unionSet(<OBJECT> set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds. UUID (Function) \u00b6 Generates a UUID (Universally Unique Identifier). Syntax <STRING> UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; cast (Function) \u00b6 Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> cast(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> to.be.caster, <STRING> cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format. coalesce (Function) \u00b6 Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> coalesce(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values. convert (Function) \u00b6 Converts the first input parameter according to the convertedTo parameter. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL> convert(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL> to.be.converted, <STRING> converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\"). createSet (Function) \u00b6 Includes the given input parameter in a java.util.HashSet and returns the set. Syntax <OBJECT> createSet(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL> input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream. currentTimeMillis (Function) \u00b6 Returns the current timestamp of siddhi application in milliseconds. Syntax <LONG> currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp. default (Function) \u00b6 Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> default(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> attribute, <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null. eventTimestamp (Function) \u00b6 Returns the timestamp of the processed event. Syntax <LONG> eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp. ifThenElse (Function) \u00b6 Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> ifThenElse(<BOOL> condition, <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> if.expression, <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue>35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage < 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin. instanceOfBoolean (Function) \u00b6 Checks whether the parameter is an instance of Boolean or not. Syntax <BOOL> instanceOfBoolean(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean. instanceOfDouble (Function) \u00b6 Checks whether the parameter is an instance of Double or not. Syntax <BOOL> instanceOfDouble(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double. instanceOfFloat (Function) \u00b6 Checks whether the parameter is an instance of Float or not. Syntax <BOOL> instanceOfFloat(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float. instanceOfInteger (Function) \u00b6 Checks whether the parameter is an instance of Integer or not. Syntax <BOOL> instanceOfInteger(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfLong (Function) \u00b6 Checks whether the parameter is an instance of Long or not. Syntax <BOOL> instanceOfLong(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfString (Function) \u00b6 Checks whether the parameter is an instance of String or not. Syntax <BOOL> instanceOfString(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string. maximum (Function) \u00b6 Returns the maximum value of the input parameters. Syntax <INT|LONG|DOUBLE|FLOAT> maximum(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3. minimum (Function) \u00b6 Returns the minimum value of the input parameters. Syntax <INT|LONG|DOUBLE|FLOAT> minimum(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3. sizeOfSet (Function) \u00b6 Returns the size of an object of type java.util.Set. Syntax <INT> sizeOfSet(<OBJECT> set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds. pol2Cart (Stream Function) \u00b6 The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax pol2Cart(<DOUBLE> theta, <DOUBLE> rho, <DOUBLE> z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4. log (Stream Processor) \u00b6 The logger logs the message on the given priority with or without processed event. Syntax log(<STRING> priority, <STRING> log.message, <BOOL> is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log(\"INFO\", \"Sample Event :\", true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log(\"Sample Event :\", true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log(\"Sample Event :\", fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log(\"Sample Event :\") select * insert into barStream; This will log message and fooStream:events. batch (Window) \u00b6 A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax batch(<INT> window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events. cron (Window) \u00b6 This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax cron(<STRING> cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. delay (Window) \u00b6 A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax delay(<INT|LONG|TIME> window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase. externalTime (Window) \u00b6 A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax externalTime(<LONG> timestamp, <INT|LONG|TIME> window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events. externalTimeBatch (Window) \u00b6 A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax externalTimeBatch(<LONG> timestamp, <INT|LONG|TIME> window.time, <INT|LONG|TIME> start.time, <INT|LONG|TIME> timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch. frequent (Window) \u00b6 This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax frequent(<INT> event.count, <STRING> attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = 'query1') from purchase[price >= 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price >= 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers. length (Window) \u00b6 A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax length(<INT> window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner. lengthBatch (Window) \u00b6 A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax lengthBatch(<INT> window.length, <BOOL> stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events. lossyFrequent (Window) \u00b6 This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax lossyFrequent(<DOUBLE> support.threshold, <DOUBLE> error.bound, <STRING> attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price >= 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price >= 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05. session (Window) \u00b6 This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax session(<INT|LONG|TIME> window.session, <STRING> window.key, <INT|LONG|TIME> window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name. sort (Window) \u00b6 This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax sort(<INT> window.length, <STRING> attribute, <STRING> order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices. time (Window) \u00b6 A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax time(<INT|LONG|TIME> window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds. timeBatch (Window) \u00b6 A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax timeBatch(<INT|LONG|TIME> window.time, <INT> start.time, <BOOL> stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events. timeLength (Window) \u00b6 A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax timeLength(<INT|LONG|TIME> window.time, <INT> window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry. Sink \u00b6 inMemory (Sink) \u00b6 In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @sink(type=\"inMemory\", topic=\"<STRING>\", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation. log (Sink) \u00b6 This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax @sink(type=\"log\", priority=\"<STRING>\", prefix=\"<STRING>\", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form <Siddhi App Name> : <Stream Name> EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values. Sinkmapper \u00b6 passThrough (Sink Mapper) \u00b6 Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink. Source \u00b6 inMemory (Source) \u00b6 In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @source(type=\"inMemory\", topic=\"<STRING>\", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport. Sourcemapper \u00b6 passThrough (Source Mapper) \u00b6 Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"5.2.0"},{"location":"docs/api/5.2.0/#api-docs-v520","text":"","title":"API Docs - v5.2.0"},{"location":"docs/api/5.2.0/#core","text":"","title":"Core"},{"location":"docs/api/5.2.0/#and-aggregate-function","text":"Returns the results of AND operation for all the events. Syntax <BOOL> and(<BOOL> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"and (Aggregate Function)"},{"location":"docs/api/5.2.0/#avg-aggregate-function","text":"Calculates the average for all the events. Syntax <DOUBLE> avg(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry.","title":"avg (Aggregate Function)"},{"location":"docs/api/5.2.0/#count-aggregate-function","text":"Returns the count of all the events. Syntax <LONG> count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds.","title":"count (Aggregate Function)"},{"location":"docs/api/5.2.0/#distinctcount-aggregate-function","text":"This returns the count of distinct occurrences for a given arg. Syntax <LONG> distinctCount(<INT|LONG|DOUBLE|FLOAT|STRING> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'.","title":"distinctCount (Aggregate Function)"},{"location":"docs/api/5.2.0/#max-aggregate-function","text":"Returns the maximum value for all the events. Syntax <INT|LONG|DOUBLE|FLOAT> max(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry.","title":"max (Aggregate Function)"},{"location":"docs/api/5.2.0/#maxforever-aggregate-function","text":"This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax <INT|LONG|DOUBLE|FLOAT> maxForever(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query.","title":"maxForever (Aggregate Function)"},{"location":"docs/api/5.2.0/#min-aggregate-function","text":"Returns the minimum value for all the events. Syntax <INT|LONG|DOUBLE|FLOAT> min(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry.","title":"min (Aggregate Function)"},{"location":"docs/api/5.2.0/#minforever-aggregate-function","text":"This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax <INT|LONG|DOUBLE|FLOAT> minForever(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query.","title":"minForever (Aggregate Function)"},{"location":"docs/api/5.2.0/#or-aggregate-function","text":"Returns the results of OR operation for all the events. Syntax <BOOL> or(<BOOL> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"or (Aggregate Function)"},{"location":"docs/api/5.2.0/#stddev-aggregate-function","text":"Returns the calculated standard deviation for all the events. Syntax <DOUBLE> stdDev(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry.","title":"stdDev (Aggregate Function)"},{"location":"docs/api/5.2.0/#sum-aggregate-function","text":"Returns the sum for all the events. Syntax <LONG|DOUBLE> sum(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry.","title":"sum (Aggregate Function)"},{"location":"docs/api/5.2.0/#unionset-aggregate-function","text":"Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax <OBJECT> unionSet(<OBJECT> set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds.","title":"unionSet (Aggregate Function)"},{"location":"docs/api/5.2.0/#uuid-function","text":"Generates a UUID (Universally Unique Identifier). Syntax <STRING> UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream;","title":"UUID (Function)"},{"location":"docs/api/5.2.0/#cast-function","text":"Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> cast(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> to.be.caster, <STRING> cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format.","title":"cast (Function)"},{"location":"docs/api/5.2.0/#coalesce-function","text":"Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> coalesce(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values.","title":"coalesce (Function)"},{"location":"docs/api/5.2.0/#convert-function","text":"Converts the first input parameter according to the convertedTo parameter. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL> convert(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL> to.be.converted, <STRING> converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\").","title":"convert (Function)"},{"location":"docs/api/5.2.0/#createset-function","text":"Includes the given input parameter in a java.util.HashSet and returns the set. Syntax <OBJECT> createSet(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL> input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream.","title":"createSet (Function)"},{"location":"docs/api/5.2.0/#currenttimemillis-function","text":"Returns the current timestamp of siddhi application in milliseconds. Syntax <LONG> currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp.","title":"currentTimeMillis (Function)"},{"location":"docs/api/5.2.0/#default-function","text":"Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> default(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> attribute, <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null.","title":"default (Function)"},{"location":"docs/api/5.2.0/#eventtimestamp-function","text":"Returns the timestamp of the processed event. Syntax <LONG> eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp.","title":"eventTimestamp (Function)"},{"location":"docs/api/5.2.0/#ifthenelse-function","text":"Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> ifThenElse(<BOOL> condition, <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> if.expression, <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue>35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage < 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin.","title":"ifThenElse (Function)"},{"location":"docs/api/5.2.0/#instanceofboolean-function","text":"Checks whether the parameter is an instance of Boolean or not. Syntax <BOOL> instanceOfBoolean(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean.","title":"instanceOfBoolean (Function)"},{"location":"docs/api/5.2.0/#instanceofdouble-function","text":"Checks whether the parameter is an instance of Double or not. Syntax <BOOL> instanceOfDouble(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double.","title":"instanceOfDouble (Function)"},{"location":"docs/api/5.2.0/#instanceoffloat-function","text":"Checks whether the parameter is an instance of Float or not. Syntax <BOOL> instanceOfFloat(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float.","title":"instanceOfFloat (Function)"},{"location":"docs/api/5.2.0/#instanceofinteger-function","text":"Checks whether the parameter is an instance of Integer or not. Syntax <BOOL> instanceOfInteger(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfInteger (Function)"},{"location":"docs/api/5.2.0/#instanceoflong-function","text":"Checks whether the parameter is an instance of Long or not. Syntax <BOOL> instanceOfLong(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfLong (Function)"},{"location":"docs/api/5.2.0/#instanceofstring-function","text":"Checks whether the parameter is an instance of String or not. Syntax <BOOL> instanceOfString(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string.","title":"instanceOfString (Function)"},{"location":"docs/api/5.2.0/#maximum-function","text":"Returns the maximum value of the input parameters. Syntax <INT|LONG|DOUBLE|FLOAT> maximum(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3.","title":"maximum (Function)"},{"location":"docs/api/5.2.0/#minimum-function","text":"Returns the minimum value of the input parameters. Syntax <INT|LONG|DOUBLE|FLOAT> minimum(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3.","title":"minimum (Function)"},{"location":"docs/api/5.2.0/#sizeofset-function","text":"Returns the size of an object of type java.util.Set. Syntax <INT> sizeOfSet(<OBJECT> set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds.","title":"sizeOfSet (Function)"},{"location":"docs/api/5.2.0/#pol2cart-stream-function","text":"The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax pol2Cart(<DOUBLE> theta, <DOUBLE> rho, <DOUBLE> z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4.","title":"pol2Cart (Stream Function)"},{"location":"docs/api/5.2.0/#log-stream-processor","text":"The logger logs the message on the given priority with or without processed event. Syntax log(<STRING> priority, <STRING> log.message, <BOOL> is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log(\"INFO\", \"Sample Event :\", true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log(\"Sample Event :\", true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log(\"Sample Event :\", fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log(\"Sample Event :\") select * insert into barStream; This will log message and fooStream:events.","title":"log (Stream Processor)"},{"location":"docs/api/5.2.0/#batch-window","text":"A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax batch(<INT> window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events.","title":"batch (Window)"},{"location":"docs/api/5.2.0/#cron-window","text":"This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax cron(<STRING> cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds.","title":"cron (Window)"},{"location":"docs/api/5.2.0/#delay-window","text":"A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax delay(<INT|LONG|TIME> window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase.","title":"delay (Window)"},{"location":"docs/api/5.2.0/#externaltime-window","text":"A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax externalTime(<LONG> timestamp, <INT|LONG|TIME> window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events.","title":"externalTime (Window)"},{"location":"docs/api/5.2.0/#externaltimebatch-window","text":"A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax externalTimeBatch(<LONG> timestamp, <INT|LONG|TIME> window.time, <INT|LONG|TIME> start.time, <INT|LONG|TIME> timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch.","title":"externalTimeBatch (Window)"},{"location":"docs/api/5.2.0/#frequent-window","text":"This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax frequent(<INT> event.count, <STRING> attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = 'query1') from purchase[price >= 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price >= 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers.","title":"frequent (Window)"},{"location":"docs/api/5.2.0/#length-window","text":"A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax length(<INT> window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner.","title":"length (Window)"},{"location":"docs/api/5.2.0/#lengthbatch-window","text":"A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax lengthBatch(<INT> window.length, <BOOL> stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events.","title":"lengthBatch (Window)"},{"location":"docs/api/5.2.0/#lossyfrequent-window","text":"This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax lossyFrequent(<DOUBLE> support.threshold, <DOUBLE> error.bound, <STRING> attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price >= 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price >= 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05.","title":"lossyFrequent (Window)"},{"location":"docs/api/5.2.0/#session-window","text":"This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax session(<INT|LONG|TIME> window.session, <STRING> window.key, <INT|LONG|TIME> window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name.","title":"session (Window)"},{"location":"docs/api/5.2.0/#sort-window","text":"This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax sort(<INT> window.length, <STRING> attribute, <STRING> order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices.","title":"sort (Window)"},{"location":"docs/api/5.2.0/#time-window","text":"A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax time(<INT|LONG|TIME> window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds.","title":"time (Window)"},{"location":"docs/api/5.2.0/#timebatch-window","text":"A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax timeBatch(<INT|LONG|TIME> window.time, <INT> start.time, <BOOL> stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events.","title":"timeBatch (Window)"},{"location":"docs/api/5.2.0/#timelength-window","text":"A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax timeLength(<INT|LONG|TIME> window.time, <INT> window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry.","title":"timeLength (Window)"},{"location":"docs/api/5.2.0/#sink","text":"","title":"Sink"},{"location":"docs/api/5.2.0/#inmemory-sink","text":"In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @sink(type=\"inMemory\", topic=\"<STRING>\", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation.","title":"inMemory (Sink)"},{"location":"docs/api/5.2.0/#log-sink","text":"This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax @sink(type=\"log\", priority=\"<STRING>\", prefix=\"<STRING>\", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form <Siddhi App Name> : <Stream Name> EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values.","title":"log (Sink)"},{"location":"docs/api/5.2.0/#sinkmapper","text":"","title":"Sinkmapper"},{"location":"docs/api/5.2.0/#passthrough-sink-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink.","title":"passThrough (Sink Mapper)"},{"location":"docs/api/5.2.0/#source","text":"","title":"Source"},{"location":"docs/api/5.2.0/#inmemory-source","text":"In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @source(type=\"inMemory\", topic=\"<STRING>\", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport.","title":"inMemory (Source)"},{"location":"docs/api/5.2.0/#sourcemapper","text":"","title":"Sourcemapper"},{"location":"docs/api/5.2.0/#passthrough-source-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"passThrough (Source Mapper)"},{"location":"docs/api/latest/","text":"API Docs - v5.2.0 \u00b6 Core \u00b6 and (Aggregate Function) \u00b6 Returns the results of AND operation for all the events. Syntax <BOOL> and(<BOOL> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch. avg (Aggregate Function) \u00b6 Calculates the average for all the events. Syntax <DOUBLE> avg(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry. count (Aggregate Function) \u00b6 Returns the count of all the events. Syntax <LONG> count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds. distinctCount (Aggregate Function) \u00b6 This returns the count of distinct occurrences for a given arg. Syntax <LONG> distinctCount(<INT|LONG|DOUBLE|FLOAT|STRING> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'. max (Aggregate Function) \u00b6 Returns the maximum value for all the events. Syntax <INT|LONG|DOUBLE|FLOAT> max(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry. maxForever (Aggregate Function) \u00b6 This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax <INT|LONG|DOUBLE|FLOAT> maxForever(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query. min (Aggregate Function) \u00b6 Returns the minimum value for all the events. Syntax <INT|LONG|DOUBLE|FLOAT> min(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry. minForever (Aggregate Function) \u00b6 This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax <INT|LONG|DOUBLE|FLOAT> minForever(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query. or (Aggregate Function) \u00b6 Returns the results of OR operation for all the events. Syntax <BOOL> or(<BOOL> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch. stdDev (Aggregate Function) \u00b6 Returns the calculated standard deviation for all the events. Syntax <DOUBLE> stdDev(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry. sum (Aggregate Function) \u00b6 Returns the sum for all the events. Syntax <LONG|DOUBLE> sum(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry. unionSet (Aggregate Function) \u00b6 Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax <OBJECT> unionSet(<OBJECT> set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds. UUID (Function) \u00b6 Generates a UUID (Universally Unique Identifier). Syntax <STRING> UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; cast (Function) \u00b6 Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> cast(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> to.be.caster, <STRING> cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format. coalesce (Function) \u00b6 Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> coalesce(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values. convert (Function) \u00b6 Converts the first input parameter according to the convertedTo parameter. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL> convert(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL> to.be.converted, <STRING> converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\"). createSet (Function) \u00b6 Includes the given input parameter in a java.util.HashSet and returns the set. Syntax <OBJECT> createSet(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL> input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream. currentTimeMillis (Function) \u00b6 Returns the current timestamp of siddhi application in milliseconds. Syntax <LONG> currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp. default (Function) \u00b6 Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> default(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> attribute, <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null. eventTimestamp (Function) \u00b6 Returns the timestamp of the processed event. Syntax <LONG> eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp. ifThenElse (Function) \u00b6 Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> ifThenElse(<BOOL> condition, <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> if.expression, <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue>35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage < 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin. instanceOfBoolean (Function) \u00b6 Checks whether the parameter is an instance of Boolean or not. Syntax <BOOL> instanceOfBoolean(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean. instanceOfDouble (Function) \u00b6 Checks whether the parameter is an instance of Double or not. Syntax <BOOL> instanceOfDouble(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double. instanceOfFloat (Function) \u00b6 Checks whether the parameter is an instance of Float or not. Syntax <BOOL> instanceOfFloat(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float. instanceOfInteger (Function) \u00b6 Checks whether the parameter is an instance of Integer or not. Syntax <BOOL> instanceOfInteger(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfLong (Function) \u00b6 Checks whether the parameter is an instance of Long or not. Syntax <BOOL> instanceOfLong(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long. instanceOfString (Function) \u00b6 Checks whether the parameter is an instance of String or not. Syntax <BOOL> instanceOfString(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string. maximum (Function) \u00b6 Returns the maximum value of the input parameters. Syntax <INT|LONG|DOUBLE|FLOAT> maximum(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3. minimum (Function) \u00b6 Returns the minimum value of the input parameters. Syntax <INT|LONG|DOUBLE|FLOAT> minimum(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3. sizeOfSet (Function) \u00b6 Returns the size of an object of type java.util.Set. Syntax <INT> sizeOfSet(<OBJECT> set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds. pol2Cart (Stream Function) \u00b6 The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax pol2Cart(<DOUBLE> theta, <DOUBLE> rho, <DOUBLE> z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4. log (Stream Processor) \u00b6 The logger logs the message on the given priority with or without processed event. Syntax log(<STRING> priority, <STRING> log.message, <BOOL> is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log(\"INFO\", \"Sample Event :\", true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log(\"Sample Event :\", true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log(\"Sample Event :\", fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log(\"Sample Event :\") select * insert into barStream; This will log message and fooStream:events. batch (Window) \u00b6 A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax batch(<INT> window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events. cron (Window) \u00b6 This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax cron(<STRING> cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. delay (Window) \u00b6 A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax delay(<INT|LONG|TIME> window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase. externalTime (Window) \u00b6 A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax externalTime(<LONG> timestamp, <INT|LONG|TIME> window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events. externalTimeBatch (Window) \u00b6 A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax externalTimeBatch(<LONG> timestamp, <INT|LONG|TIME> window.time, <INT|LONG|TIME> start.time, <INT|LONG|TIME> timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch. frequent (Window) \u00b6 This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax frequent(<INT> event.count, <STRING> attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = 'query1') from purchase[price >= 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price >= 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers. length (Window) \u00b6 A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax length(<INT> window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner. lengthBatch (Window) \u00b6 A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax lengthBatch(<INT> window.length, <BOOL> stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events. lossyFrequent (Window) \u00b6 This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax lossyFrequent(<DOUBLE> support.threshold, <DOUBLE> error.bound, <STRING> attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price >= 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price >= 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05. session (Window) \u00b6 This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax session(<INT|LONG|TIME> window.session, <STRING> window.key, <INT|LONG|TIME> window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name. sort (Window) \u00b6 This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax sort(<INT> window.length, <STRING> attribute, <STRING> order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices. time (Window) \u00b6 A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax time(<INT|LONG|TIME> window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds. timeBatch (Window) \u00b6 A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax timeBatch(<INT|LONG|TIME> window.time, <INT> start.time, <BOOL> stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events. timeLength (Window) \u00b6 A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax timeLength(<INT|LONG|TIME> window.time, <INT> window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry. Sink \u00b6 inMemory (Sink) \u00b6 In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @sink(type=\"inMemory\", topic=\"<STRING>\", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation. log (Sink) \u00b6 This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax @sink(type=\"log\", priority=\"<STRING>\", prefix=\"<STRING>\", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form <Siddhi App Name> : <Stream Name> EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values. Sinkmapper \u00b6 passThrough (Sink Mapper) \u00b6 Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink. Source \u00b6 inMemory (Source) \u00b6 In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @source(type=\"inMemory\", topic=\"<STRING>\", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport. Sourcemapper \u00b6 passThrough (Source Mapper) \u00b6 Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"latest"},{"location":"docs/api/latest/#api-docs-v520","text":"","title":"API Docs - v5.2.0"},{"location":"docs/api/latest/#core","text":"","title":"Core"},{"location":"docs/api/latest/#and-aggregate-function","text":"Returns the results of AND operation for all the events. Syntax <BOOL> and(<BOOL> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be AND operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select and(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for AND operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"and (Aggregate Function)"},{"location":"docs/api/latest/#avg-aggregate-function","text":"Calculates the average for all the events. Syntax <DOUBLE> avg(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that need to be averaged. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch select avg(temp) as avgTemp insert into barStream; avg(temp) returns the average temp value for all the events based on their arrival and expiry.","title":"avg (Aggregate Function)"},{"location":"docs/api/latest/#count-aggregate-function","text":"Returns the count of all the events. Syntax <LONG> count() Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select count() as count insert into barStream; This will return the count of all the events for time batch in 10 seconds.","title":"count (Aggregate Function)"},{"location":"docs/api/latest/#distinctcount-aggregate-function","text":"This returns the count of distinct occurrences for a given arg. Syntax <LONG> distinctCount(<INT|LONG|DOUBLE|FLOAT|STRING> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The object for which the number of distinct occurences needs to be counted. INT LONG DOUBLE FLOAT STRING No No Examples EXAMPLE 1 from fooStream select distinctcount(pageID) as count insert into barStream; distinctcount(pageID) for the following output returns '3' when the available values are as follows. \"WEB_PAGE_1\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" \"WEB_PAGE_3\" \"WEB_PAGE_1\" \"WEB_PAGE_2\" The three distinct occurences identified are 'WEB_PAGE_1', 'WEB_PAGE_2', and 'WEB_PAGE_3'.","title":"distinctCount (Aggregate Function)"},{"location":"docs/api/latest/#max-aggregate-function","text":"Returns the maximum value for all the events. Syntax <INT|LONG|DOUBLE|FLOAT> max(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from fooStream#window.timeBatch(10 sec) select max(temp) as maxTemp insert into barStream; max(temp) returns the maximum temp value recorded for all the events based on their arrival and expiry.","title":"max (Aggregate Function)"},{"location":"docs/api/latest/#maxforever-aggregate-function","text":"This is the attribute aggregator to store the maximum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax <INT|LONG|DOUBLE|FLOAT> maxForever(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the maximum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select maxForever(temp) as max insert into outputStream; maxForever(temp) returns the maximum temp value recorded for all the events throughout the lifetime of the query.","title":"maxForever (Aggregate Function)"},{"location":"docs/api/latest/#min-aggregate-function","text":"Returns the minimum value for all the events. Syntax <INT|LONG|DOUBLE|FLOAT> min(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select min(temp) as minTemp insert into outputStream; min(temp) returns the minimum temp value recorded for all the events based on their arrival and expiry.","title":"min (Aggregate Function)"},{"location":"docs/api/latest/#minforever-aggregate-function","text":"This is the attribute aggregator to store the minimum value for a given attribute throughout the lifetime of the query regardless of any windows in-front. Syntax <INT|LONG|DOUBLE|FLOAT> minForever(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be compared to find the minimum value. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select minForever(temp) as max insert into outputStream; minForever(temp) returns the minimum temp value recorded for all the events throughoutthe lifetime of the query.","title":"minForever (Aggregate Function)"},{"location":"docs/api/latest/#or-aggregate-function","text":"Returns the results of OR operation for all the events. Syntax <BOOL> or(<BOOL> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be OR operation. BOOL No No Examples EXAMPLE 1 from cscStream#window.lengthBatch(10) select or(isFraud) as isFraudTransaction insert into alertStream; This will returns the result for OR operation of isFraud values as a boolean value for event chunk expiry by window length batch.","title":"or (Aggregate Function)"},{"location":"docs/api/latest/#stddev-aggregate-function","text":"Returns the calculated standard deviation for all the events. Syntax <DOUBLE> stdDev(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that should be used to calculate the standard deviation. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select stddev(temp) as stdTemp insert into outputStream; stddev(temp) returns the calculated standard deviation of temp for all the events based on their arrival and expiry.","title":"stdDev (Aggregate Function)"},{"location":"docs/api/latest/#sum-aggregate-function","text":"Returns the sum for all the events. Syntax <LONG|DOUBLE> sum(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The value that needs to be summed. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 from inputStream select sum(volume) as sumOfVolume insert into outputStream; This will returns the sum of volume values as a long value for each event arrival and expiry.","title":"sum (Aggregate Function)"},{"location":"docs/api/latest/#unionset-aggregate-function","text":"Union multiple sets. This attribute aggregator maintains a union of sets. The given input set is put into the union set and the union set is returned. Syntax <OBJECT> unionSet(<OBJECT> set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The java.util.Set object that needs to be added into the union set. OBJECT No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream from initStream#window.timeBatch(10 sec) select unionSet(initialSet) as distinctSymbols insert into distinctStockStream; distinctStockStream will return the set object which contains the distinct set of stock symbols received during a sliding window of 10 seconds.","title":"unionSet (Aggregate Function)"},{"location":"docs/api/latest/#uuid-function","text":"Generates a UUID (Universally Unique Identifier). Syntax <STRING> UUID() Examples EXAMPLE 1 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream; This will converts a room number to string, introducing a message ID to each event asUUID() returns a34eec40-32c2-44fe-8075-7f4fde2e2dd8 from TempStream select convert(roomNo, 'string') as roomNo, temp, UUID() as messageID insert into RoomTempStream;","title":"UUID (Function)"},{"location":"docs/api/latest/#cast-function","text":"Converts the first parameter according to the cast.to parameter. Incompatible arguments cause Class Cast exceptions if further processed. This function is used with map extension that returns attributes of the object type. You can use this function to cast the object to an accurate and concrete type. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> cast(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> to.be.caster, <STRING> cast.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.caster This specifies the attribute to be casted. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No cast.to A string constant parameter expressing the cast to type using one of the following strings values: int, long, float, double, string, bool. STRING No No Examples EXAMPLE 1 from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; This will cast the fooStream temp field value into 'double' format.","title":"cast (Function)"},{"location":"docs/api/latest/#coalesce-function","text":"Returns the value of the first input parameter that is not null, and all input parameters have to be on the same type. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> coalesce(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> args) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic args This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select coalesce('123', null, '789') as value insert into barStream; This will returns first null value 123. EXAMPLE 2 from fooStream select coalesce(null, 76, 567) as value insert into barStream; This will returns first null value 76. EXAMPLE 3 from fooStream select coalesce(null, null, null) as value insert into barStream; This will returns null as there are no notnull values.","title":"coalesce (Function)"},{"location":"docs/api/latest/#convert-function","text":"Converts the first input parameter according to the convertedTo parameter. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL> convert(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL> to.be.converted, <STRING> converted.to) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic to.be.converted This specifies the value to be converted. INT LONG DOUBLE FLOAT STRING BOOL No No converted.to A string constant parameter to which type the attribute need to be converted using one of the following strings values: 'int', 'long', 'float', 'double', 'string', 'bool'. STRING No No Examples EXAMPLE 1 from fooStream select convert(temp, 'double') as temp insert into barStream; This will convert fooStream temp value into 'double'. EXAMPLE 2 from fooStream select convert(temp, 'int') as temp insert into barStream; This will convert fooStream temp value into 'int' (value = \"convert(45.9, 'int') returns 46\").","title":"convert (Function)"},{"location":"docs/api/latest/#createset-function","text":"Includes the given input parameter in a java.util.HashSet and returns the set. Syntax <OBJECT> createSet(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL> input) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic input The input that needs to be added into the set. INT LONG DOUBLE FLOAT STRING BOOL No No Examples EXAMPLE 1 from stockStream select createSet(symbol) as initialSet insert into initStream; For every incoming stockStream event, the initStream stream will produce a set object having only one element: the symbol in the incoming stockStream.","title":"createSet (Function)"},{"location":"docs/api/latest/#currenttimemillis-function","text":"Returns the current timestamp of siddhi application in milliseconds. Syntax <LONG> currentTimeMillis() Examples EXAMPLE 1 from fooStream select symbol as name, currentTimeMillis() as eventTimestamp insert into barStream; This will extract current siddhi application timestamp.","title":"currentTimeMillis (Function)"},{"location":"docs/api/latest/#default-function","text":"Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> default(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> attribute, <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> default) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic attribute The attribute that could be null. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No default The default value that will be used when 'attribute' parameter is null INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from TempStream select default(temp, 0.0) as temp, roomNum insert into StandardTempStream; This will replace TempStream's temp attribute with default value if the temp is null.","title":"default (Function)"},{"location":"docs/api/latest/#eventtimestamp-function","text":"Returns the timestamp of the processed event. Syntax <LONG> eventTimestamp() Examples EXAMPLE 1 from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream; This will extract current events timestamp.","title":"eventTimestamp (Function)"},{"location":"docs/api/latest/#ifthenelse-function","text":"Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. Here both 'if.expression' and 'else.expression' should be of the same type. Syntax <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> ifThenElse(<BOOL> condition, <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> if.expression, <INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> else.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic condition This specifies the if then else condition value. BOOL No No if.expression This specifies the value to be returned if the value of the condition parameter is true. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No else.expression This specifies the value to be returned if the value of the condition parameter is false. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(sensorValue>35,'High','Low') as status insert into outputStream; This will returns High if sensorValue = 50. EXAMPLE 2 @info(name = 'query1') from sensorEventStream select sensorValue, ifThenElse(voltage < 5, 0, 1) as status insert into outputStream; This will returns 1 if voltage= 12. EXAMPLE 3 @info(name = 'query1') from userEventStream select userName, ifThenElse(password == 'admin', true, false) as passwordState insert into outputStream; This will returns passwordState as true if password = admin.","title":"ifThenElse (Function)"},{"location":"docs/api/latest/#instanceofboolean-function","text":"Checks whether the parameter is an instance of Boolean or not. Syntax <BOOL> instanceOfBoolean(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfBoolean(switchState) as state insert into barStream; This will return true if the value of switchState is true. EXAMPLE 2 from fooStream select instanceOfBoolean(value) as state insert into barStream; if the value = 32 then this will returns false as the value is not an instance of the boolean.","title":"instanceOfBoolean (Function)"},{"location":"docs/api/latest/#instanceofdouble-function","text":"Checks whether the parameter is an instance of Double or not. Syntax <BOOL> instanceOfDouble(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfDouble(value) as state insert into barStream; This will return true if the value field format is double ex : 56.45. EXAMPLE 2 from fooStream select instanceOfDouble(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is not an instance of the double.","title":"instanceOfDouble (Function)"},{"location":"docs/api/latest/#instanceoffloat-function","text":"Checks whether the parameter is an instance of Float or not. Syntax <BOOL> instanceOfFloat(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfFloat(value) as state insert into barStream; This will return true if the value field format is float ex : 56.45f. EXAMPLE 2 from fooStream select instanceOfFloat(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a float.","title":"instanceOfFloat (Function)"},{"location":"docs/api/latest/#instanceofinteger-function","text":"Checks whether the parameter is an instance of Integer or not. Syntax <BOOL> instanceOfInteger(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfInteger(value) as state insert into barStream; This will return true if the value field format is integer. EXAMPLE 2 from fooStream select instanceOfInteger(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfInteger (Function)"},{"location":"docs/api/latest/#instanceoflong-function","text":"Checks whether the parameter is an instance of Long or not. Syntax <BOOL> instanceOfLong(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfLong(value) as state insert into barStream; This will return true if the value field format is long ex : 56456l. EXAMPLE 2 from fooStream select instanceOfLong(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a long.","title":"instanceOfLong (Function)"},{"location":"docs/api/latest/#instanceofstring-function","text":"Checks whether the parameter is an instance of String or not. Syntax <BOOL> instanceOfString(<INT|LONG|DOUBLE|FLOAT|STRING|BOOL|OBJECT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg The parameter to be checked. INT LONG DOUBLE FLOAT STRING BOOL OBJECT No No Examples EXAMPLE 1 from fooStream select instanceOfString(value) as state insert into barStream; This will return true if the value field format is string ex : 'test'. EXAMPLE 2 from fooStream select instanceOfString(switchState) as state insert into barStream; if the switchState = true then this will returns false as the value is an instance of the boolean not a string.","title":"instanceOfString (Function)"},{"location":"docs/api/latest/#maximum-function","text":"Returns the maximum value of the input parameters. Syntax <INT|LONG|DOUBLE|FLOAT> maximum(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the maximum value of the input parameters price1, price2, price3.","title":"maximum (Function)"},{"location":"docs/api/latest/#minimum-function","text":"Returns the minimum value of the input parameters. Syntax <INT|LONG|DOUBLE|FLOAT> minimum(<INT|LONG|DOUBLE|FLOAT> arg) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic arg This function accepts one or more parameters. They can belong to any one of the available types. All the specified parameters should be of the same type. INT LONG DOUBLE FLOAT No No Examples EXAMPLE 1 @info(name = 'query1') from inputStream select maximum(price1, price2, price3) as max insert into outputStream; This will returns the minimum value of the input parameters price1, price2, price3.","title":"minimum (Function)"},{"location":"docs/api/latest/#sizeofset-function","text":"Returns the size of an object of type java.util.Set. Syntax <INT> sizeOfSet(<OBJECT> set) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic set The set object. This parameter should be of type java.util.Set. A set object may be created by the 'set' attribute aggregator in Siddhi. OBJECT No No Examples EXAMPLE 1 from stockStream select initSet(symbol) as initialSet insert into initStream; ;from initStream#window.timeBatch(10 sec) select union(initialSet) as distinctSymbols insert into distinctStockStream; from distinctStockStream select sizeOfSet(distinctSymbols) sizeOfSymbolSet insert into sizeStream; The sizeStream stream will output the number of distinct stock symbols received during a sliding window of 10 seconds.","title":"sizeOfSet (Function)"},{"location":"docs/api/latest/#pol2cart-stream-function","text":"The pol2Cart function calculating the cartesian coordinates x & y for the given theta, rho coordinates and adding them as new attributes to the existing events. Syntax pol2Cart(<DOUBLE> theta, <DOUBLE> rho, <DOUBLE> z) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic theta The theta value of the coordinates. DOUBLE No No rho The rho value of the coordinates. DOUBLE No No z z value of the cartesian coordinates. If z value is not given, drop the third parameter of the output. DOUBLE Yes No Examples EXAMPLE 1 from PolarStream#pol2Cart(theta, rho) select x, y insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748) for theta: 0.7854 and rho: 5. EXAMPLE 2 from PolarStream#pol2Cart(theta, rho, 3.4) select x, y, z insert into outputStream ; This will return cartesian coordinates (4.99953024681082, 0.06853693328228748, 3.4)for theta: 0.7854 and rho: 5 and z: 3.4.","title":"pol2Cart (Stream Function)"},{"location":"docs/api/latest/#log-stream-processor","text":"The logger logs the message on the given priority with or without processed event. Syntax log(<STRING> priority, <STRING> log.message, <BOOL> is.event.logged) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority The priority/type of this log message (INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE). INFO STRING Yes No log.message This message will be logged. STRING No No is.event.logged To log the processed event. true BOOL Yes No Examples EXAMPLE 1 from fooStream#log(\"INFO\", \"Sample Event :\", true) select * insert into barStream; This will log as INFO with the message \"Sample Event :\" + fooStream:events. EXAMPLE 2 from fooStream#log(\"Sample Event :\", true) select * insert into barStream; This will logs with default log level as INFO. EXAMPLE 3 from fooStream#log(\"Sample Event :\", fasle) select * insert into barStream; This will only log message. EXAMPLE 4 from fooStream#log(true) select * insert into barStream; This will only log fooStream:events. EXAMPLE 5 from fooStream#log(\"Sample Event :\") select * insert into barStream; This will log message and fooStream:events.","title":"log (Stream Processor)"},{"location":"docs/api/latest/#batch-window","text":"A window that holds an incoming events batch. When a new set of events arrives, the previously arrived old events will be expired. Batch window can be used to aggregate events that comes in batches. If it has the parameter length specified, then batch window process the batch as several chunks. Syntax batch(<INT> window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The length of a chunk If length value was not given it assign 0 as length and process the whole batch as once INT Yes No Examples EXAMPLE 1 define stream consumerItemStream (itemId string, price float) from consumerItemStream#window.batch() select price, str:groupConcat(itemId) as itemIds group by price insert into outputStream; This will output comma separated items IDs that have the same price for each incoming batch of events.","title":"batch (Window)"},{"location":"docs/api/latest/#cron-window","text":"This window outputs the arriving events as and when they arrive, and resets (expires) the window periodically based on the given cron expression. Syntax cron(<STRING> cron.expression) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic cron.expression The cron expression that resets the window. STRING No No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#cron('*/5 * * * * ?') select symbol, sum(price) as totalPrice insert into OutputStream; This let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds. EXAMPLE 2 define stream StockEventStream (symbol string, price float, volume int) define window StockEventWindow (symbol string, price float, volume int) cron('*/5 * * * * ?'); @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as totalPrice insert into OutputStream ; The defined window will let the totalPrice to gradually increase and resets to zero as a batch every 5 seconds.","title":"cron (Window)"},{"location":"docs/api/latest/#delay-window","text":"A delay window holds events for a specific time period that is regarded as a delay period before processing them. Syntax delay(<INT|LONG|TIME> window.delay) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.delay The time period (specified in sec, min, ms) for which the window should delay the events. INT LONG TIME No No Examples EXAMPLE 1 define window delayWindow(symbol string, volume int) delay(1 hour); define stream PurchaseStream(symbol string, volume int); define stream DeliveryStream(symbol string); define stream OutputStream(symbol string); @info(name='query1') from PurchaseStream select symbol, volume insert into delayWindow; @info(name='query2') from delayWindow join DeliveryStream on delayWindow.symbol == DeliveryStream.symbol select delayWindow.symbol insert into OutputStream; In this example, purchase events that arrive in the 'PurchaseStream' stream are directed to a delay window. At any given time, this delay window holds purchase events that have arrived within the last hour. These purchase events in the window are matched by the 'symbol' attribute, with delivery events that arrive in the 'DeliveryStream' stream. This monitors whether the delivery of products is done with a minimum delay of one hour after the purchase.","title":"delay (Window)"},{"location":"docs/api/latest/#externaltime-window","text":"A sliding time window based on external time. It holds events that arrived during the last windowTime period from the external timestamp, and gets updated on every monotonically increasing timestamp. Syntax externalTime(<LONG> timestamp, <INT|LONG|TIME> window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTime(eventTime, 20 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; processing events arrived within the last 20 seconds from the eventTime and output expired events.","title":"externalTime (Window)"},{"location":"docs/api/latest/#externaltimebatch-window","text":"A batch (tumbling) time window based on external time, that holds events arrived during windowTime periods, and gets updated for every windowTime. Syntax externalTimeBatch(<LONG> timestamp, <INT|LONG|TIME> window.time, <INT|LONG|TIME> start.time, <INT|LONG|TIME> timeout) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic timestamp The time which the window determines as current time and will act upon. The value of this parameter should be monotonically increasing. LONG No No window.time The batch time period for which the window should hold events. INT LONG TIME No No start.time User defined start time. This could either be a constant (of type int, long or time) or an attribute of the corresponding stream (of type long). If an attribute is provided, initial value of attribute would be considered as startTime. Timestamp of first event INT LONG TIME Yes No timeout Time to wait for arrival of new event, before flushing and giving output for events belonging to a specific batch. System waits till an event from next batch arrives to flush current batch INT LONG TIME Yes No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 1 sec) output expired events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert expired events into outputStream ; This will processing events that arrive every 1 seconds from the eventTime. EXAMPLE 2 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 20 sec, 0) output expired events; This will processing events that arrive every 1 seconds from the eventTime. Starts on 0 th millisecond of an hour. EXAMPLE 3 define window cseEventWindow (symbol string, price float, volume int) externalTimeBatch(eventTime, 2 sec, eventTimestamp, 100) output expired events; This will processing events that arrive every 2 seconds from the eventTim. Considers the first event's eventTimestamp value as startTime. Waits 100 milliseconds for the arrival of a new event before flushing current batch.","title":"externalTimeBatch (Window)"},{"location":"docs/api/latest/#frequent-window","text":"This window returns the latest events with the most frequently occurred value for a given attribute(s). Frequency calculation for this window processor is based on Misra-Gries counting algorithm. Syntax frequent(<INT> event.count, <STRING> attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic event.count The number of most frequent events to be emitted to the stream. INT No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 @info(name = 'query1') from purchase[price >= 30]#window.frequent(2) select cardNo, price insert all events into PotentialFraud; This will returns the 2 most frequent events. EXAMPLE 2 @info(name = 'query1') from purchase[price >= 30]#window.frequent(2, cardNo) select cardNo, price insert all events into PotentialFraud; This will returns the 2 latest events with the most frequently appeared card numbers.","title":"frequent (Window)"},{"location":"docs/api/latest/#length-window","text":"A sliding length window that holds the last 'window.length' events at a given time, and gets updated for each arrival and expiry. Syntax length(<INT> window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events that should be included in a sliding length window. INT No No Examples EXAMPLE 1 define window StockEventWindow (symbol string, price float, volume int) length(10) output all events; @info(name = 'query0') from StockEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will process last 10 events in a sliding manner.","title":"length (Window)"},{"location":"docs/api/latest/#lengthbatch-window","text":"A batch (tumbling) length window that holds and process a number of events as specified in the window.length. Syntax lengthBatch(<INT> window.length, <BOOL> stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The number of events the window should tumble. INT No No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10) select symbol, sum(price) as price insert into OutputStream; This collect and process 10 events as a batch and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#lengthBatch(10, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually, after every 10 events it clears the window as a batch and resets the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) lengthBatch(10) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process 10 events as a batch and output all events.","title":"lengthBatch (Window)"},{"location":"docs/api/latest/#lossyfrequent-window","text":"This window identifies and returns all the events of which the current frequency exceeds the value specified for the supportThreshold parameter. Syntax lossyFrequent(<DOUBLE> support.threshold, <DOUBLE> error.bound, <STRING> attribute) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic support.threshold The support threshold value. DOUBLE No No error.bound The error bound value. DOUBLE No No attribute The attributes to group the events. If no attributes are given, the concatenation of all the attributes of the event is considered. The concatenation of all the attributes of the event is considered. STRING Yes No Examples EXAMPLE 1 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.1, 0.01); @info(name = 'query0') from purchase[price >= 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.1, 0.01) returns all the events of which the current frequency exceeds 0.1, with an error bound of 0.01. EXAMPLE 2 define stream purchase (cardNo string, price float); define window purchaseWindow (cardNo string, price float) lossyFrequent(0.3, 0.05, cardNo); @info(name = 'query0') from purchase[price >= 30] insert into purchaseWindow; @info(name = 'query1') from purchaseWindow select cardNo, price insert all events into PotentialFraud; lossyFrequent(0.3, 0.05, cardNo) returns all the events of which the cardNo attributes frequency exceeds 0.3, with an error bound of 0.05.","title":"lossyFrequent (Window)"},{"location":"docs/api/latest/#session-window","text":"This is a session window that holds events that belong to a specific session. The events that belong to a specific session are identified by a grouping attribute (i.e., a session key). A session gap period is specified to determine the time period after which the session is considered to be expired. A new event that arrives with a specific value for the session key is matched with the session window with the same session key. There can be out of order and late arrival of events, these events can arrive after the session is expired, to include those events to the matching session key specify a latency time period that is less than the session gap period.To have aggregate functions with session windows, the events need to be grouped by the session key via a 'group by' clause. Syntax session(<INT|LONG|TIME> window.session, <STRING> window.key, <INT|LONG|TIME> window.allowedlatency) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.session The time period for which the session considered is valid. This is specified in seconds, minutes, or milliseconds (i.e., 'min', 'sec', or 'ms'. INT LONG TIME No No window.key The grouping attribute for events. default-key STRING Yes No window.allowedlatency This specifies the time period for which the session window is valid after the expiration of the session. The time period specified here should be less than the session time gap (which is specified via the 'window.session' parameter). 0 INT LONG TIME Yes No Examples EXAMPLE 1 define stream PurchaseEventStream (user string, item_number int, price float, quantity int); @info(name='query0) from PurchaseEventStream#window.session(5 sec, user, 2 sec) select * insert all events into OutputStream; This query processes events that arrive at the PurchaseEvent input stream. The 'user' attribute is the session key, and the session gap is 5 seconds. '2 sec' is specified as the allowed latency. Therefore, events with the matching user name that arrive 2 seconds after the expiration of the session are also considered when performing aggregations for the session identified by the given user name.","title":"session (Window)"},{"location":"docs/api/latest/#sort-window","text":"This window holds a batch of events that equal the number specified as the windowLength and sorts them in the given order. Syntax sort(<INT> window.length, <STRING> attribute, <STRING> order) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.length The size of the window length. INT No No attribute The attribute that should be checked for the order. The concatenation of all the attributes of the event is considered. STRING Yes No order The order define as \"asc\" or \"desc\". asc STRING Yes No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume long); define window cseEventWindow (symbol string, price float, volume long) sort(2,volume, 'asc'); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select volume insert all events into outputStream ; sort(5, price, 'asc') keeps the events sorted by price in the ascending order. Therefore, at any given time, the window contains the 5 lowest prices.","title":"sort (Window)"},{"location":"docs/api/latest/#time-window","text":"A sliding time window that holds events that arrived during the last windowTime period at a given time, and gets updated for each event arrival and expiry. Syntax time(<INT|LONG|TIME> window.time) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No Examples EXAMPLE 1 define window cseEventWindow (symbol string, price float, volume int) time(20) output all events; @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, sum(price) as price insert all events into outputStream ; This will processing events that arrived within the last 20 milliseconds.","title":"time (Window)"},{"location":"docs/api/latest/#timebatch-window","text":"A batch (tumbling) time window that holds and process events that arrive during 'window.time' period as a batch. Syntax timeBatch(<INT|LONG|TIME> window.time, <INT> start.time, <BOOL> stream.current.event) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The batch time period in which the window process the events. INT LONG TIME No No start.time This specifies an offset in milliseconds in order to start the window at a time different to the standard time. Timestamp of first event INT Yes No stream.current.event Let the window stream the current events out as and when they arrive to the window while expiring them in batches. false BOOL Yes No Examples EXAMPLE 1 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec) select symbol, sum(price) as price insert into OutputStream; This collect and process incoming events as a batch every 20 seconds and output them. EXAMPLE 2 define stream InputEventStream (symbol string, price float, volume int); @info(name = 'query1') from InputEventStream#timeBatch(20 sec, true) select symbol, sum(price) as sumPrice insert into OutputStream; This window sends the arriving events directly to the output letting the sumPrice to increase gradually and on every 20 second interval it clears the window as a batch resetting the sumPrice to zero. EXAMPLE 3 define stream InputEventStream (symbol string, price float, volume int); define window StockEventWindow (symbol string, price float, volume int) timeBatch(20 sec) output all events; @info(name = 'query0') from InputEventStream insert into StockEventWindow; @info(name = 'query1') from StockEventWindow select symbol, sum(price) as price insert all events into OutputStream ; This uses an defined window to process events arrived every 20 seconds as a batch and output all events.","title":"timeBatch (Window)"},{"location":"docs/api/latest/#timelength-window","text":"A sliding time window that, at a given time holds the last window.length events that arrived during last window.time period, and gets updated for every event arrival and expiry. Syntax timeLength(<INT|LONG|TIME> window.time, <INT> window.length) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic window.time The sliding time period for which the window should hold events. INT LONG TIME No No window.length The number of events that should be be included in a sliding length window.. INT No No Examples EXAMPLE 1 define stream cseEventStream (symbol string, price float, volume int); define window cseEventWindow (symbol string, price float, volume int) timeLength(2 sec, 10); @info(name = 'query0') from cseEventStream insert into cseEventWindow; @info(name = 'query1') from cseEventWindow select symbol, price, volume insert all events into outputStream; window.timeLength(2 sec, 10) holds the last 10 events that arrived during last 2 seconds and gets updated for every event arrival and expiry.","title":"timeLength (Window)"},{"location":"docs/api/latest/#sink","text":"","title":"Sink"},{"location":"docs/api/latest/#inmemory-sink","text":"In-memory transport that can communicate with other in-memory transports within the same JVM, itis assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @sink(type=\"inMemory\", topic=\"<STRING>\", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Event will be delivered to allthe subscribers of the same topic STRING No No Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which emit the Siddhi events internally without using external transport and transformation.","title":"inMemory (Sink)"},{"location":"docs/api/latest/#log-sink","text":"This is a sink that can be used as a logger. This will log the output events in the output stream with user specified priority and a prefix Syntax @sink(type=\"log\", priority=\"<STRING>\", prefix=\"<STRING>\", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic priority This will set the logger priority i.e log level. Accepted values are INFO, DEBUG, WARN, FATAL, ERROR, OFF, TRACE INFO STRING Yes No prefix This will be the prefix to the output message. If the output stream has event [2,4] and the prefix is given as \"Hello\" then the log will show \"Hello : [2,4]\" default prefix will be : STRING Yes No Examples EXAMPLE 1 @sink(type='log', prefix='My Log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. Also the priority is set to DEBUG. EXAMPLE 2 @sink(type='log', priority='DEBUG') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the priority is set to DEBUG. User has not specified prefix so the default prefix will be in the form <Siddhi App Name> : <Stream Name> EXAMPLE 3 @sink(type='log', prefix='My Log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink and the prefix is given as My Log. User has not given a priority so it will be set to default INFO. EXAMPLE 4 @sink(type='log') define stream BarStream (symbol string, price float, volume long) In this example BarStream uses log sink. The user has not given prefix or priority so they will be set to their default values.","title":"log (Sink)"},{"location":"docs/api/latest/#sinkmapper","text":"","title":"Sinkmapper"},{"location":"docs/api/latest/#passthrough-sink-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @sink(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @sink(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In the following example BarStream uses passThrough outputmapper which emit Siddhi event directly without any transformation into sink.","title":"passThrough (Sink Mapper)"},{"location":"docs/api/latest/#source","text":"","title":"Source"},{"location":"docs/api/latest/#inmemory-source","text":"In-memory source that can communicate with other in-memory sinks within the same JVM, it is assumed that the publisher and subscriber of a topic uses same event schema (stream definition). Syntax @source(type=\"inMemory\", topic=\"<STRING>\", @map(...))) QUERY PARAMETERS Name Description Default Value Possible Data Types Optional Dynamic topic Subscribes to sent on the given topic. STRING No No Examples EXAMPLE 1 @source(type='inMemory', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long) In this example BarStream uses inMemory transport which passes the received event internally without using external transport.","title":"inMemory (Source)"},{"location":"docs/api/latest/#sourcemapper","text":"","title":"Sourcemapper"},{"location":"docs/api/latest/#passthrough-source-mapper","text":"Pass-through mapper passed events (Event[]) through without any mapping or modifications. Syntax @source(..., @map(type=\"passThrough\") Examples EXAMPLE 1 @source(type='tcp', @map(type='passThrough')) define stream BarStream (symbol string, price float, volume long); In this example BarStream uses passThrough inputmapper which passes the received Siddhi event directly without any transformation into source.","title":"passThrough (Source Mapper)"},{"location":"docs/quick-start/","text":"Siddhi 5.2 Quick Start Guide \u00b6 Siddhi is a cloud native Streaming and Complex Event Processing engine that understands Streaming SQL queries in order to capture events from diverse data sources, process them, detect complex conditions, and publish output to various endpoints in real time. Siddhi is used by many companies including Uber, eBay, PayPal (via Apache Eagle), here Uber processed more than 20 billion events per day using Siddhi for their fraud analytics use cases. Siddhi is also used in various analytics and integration platforms such as Apache Eagle as a policy enforcement engine, WSO2 API Manager as analytics and throttling engine, WSO2 Identity Server as an adaptive authorization engine. This quick start guide contains the following six sections: Domain of Siddhi Overview of Siddhi architecture Using Siddhi for the first time Writing first Siddhi Application Testing Siddhi Application A bit of Stream Processing 1. Domain of Siddhi \u00b6 Siddhi is an event driven system where all the data it consumes, processes and sends are modeled as events. Therefore, Siddhi can play a vital part in any event-driven architecture. As Siddhi works with events, first let's understand what an event is through an example. If we consider transactions carried out via an ATM as a data stream, one withdrawal from it can be considered as an event . This event contains data such as amount, time, account number, etc. Many such transactions form a stream. Siddhi provides following functionalities, Streaming Data Analytics Forrester defines Streaming Analytics as: Software that provides analytical operators to orchestrate data flow , calculate analytics , and detect patterns on event data from multiple, disparate live data sources to allow developers to build applications that sense, think, and act in real time . Complex Event Processing (CEP) Gartner\u2019s IT Glossary defines CEP as follows: \"CEP is a kind of computing in which incoming data about events is distilled into more useful, higher level \u201ccomplex\u201d event data that provides insight into what is happening.\" \" CEP is event-driven because the computation is triggered by the receipt of event data. CEP is used for highly demanding, continuous-intelligence applications that enhance situation awareness and support real-time decisions.\" Streaming Data Integration Streaming data integration is a way of integrating several systems by processing, correlating, and analyzing the data in memory, while continuously moving data in real-time from one system to another. Alerts & Notifications The system to continuously monitor event streams, and send alerts and notifications, based on defined KPIs and other analytics. Adaptive Decision Making A way to dynamically making real-time decisions based on predefined rules, the current state of the connected systems, and machine learning techniques. Basically, Siddhi receives data event-by-event and processes them in real-time to produce meaningful information. Using the above Siddhi can be used to solve may use-cases as follows: Fraud Analytics Monitoring System Integration Anomaly Detection Sentiment Analysis Processing Customer Behavior .. etc 2. Overview of Siddhi architecture \u00b6 As indicated above, Siddhi can: Accept event inputs from many different types of sources. Process them to transform, enrich, and generate insights. Publish them to multiple types of sinks. To use Siddhi, you need to write the processing logic as a Siddhi Application in the Siddhi Streaming SQL language which is discussed in the section 4 . Here a Siddhi Application is a script file that contains business logic for a scenario. When the Siddhi application is started, it: Consumes data one-by-one as events. Pipe the events to queries through various streams for processing. Generates new events based on the processing done at the queries. Finally, Sends newly generated events through output to streams. 3. Using Siddhi for the first time \u00b6 In this section, we will be using the Siddhi tooling distribution\u200a\u2014\u200aa server version of Siddhi that has a sophisticated web based editor with a GUI (referred to as \u201cSiddhi Editor\u201d ) where you can write Siddhi Apps and simulate events to test your scenario. Step 1 \u200a\u2014\u200aInstall Oracle Java SE Development Kit (JDK) version 1.8. Step 2 \u200a\u2014\u200a Set the JAVA_HOME environment variable. Step 3 \u200a\u2014\u200aDownload the latest tooling distribution from here . Step 4 \u200a\u2014\u200aExtract the downloaded zip and navigate to <TOOLING_HOME>/bin . ( TOOLING_HOME refers to the extracted folder) Step 5 \u200a\u2014\u200aIssue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh After successfully starting the Siddhi Editor, the terminal should look like as shown below: After starting the Siddhi Editor, access the Editor GUI by visiting the following link in your browser (Google Chrome is the Recommended). http://localhost:9390/editor This takes you to the Siddhi Editor landing page. 4. Writing first Siddhi Application \u00b6 Siddhi Streaming SQL is a rich, compact, easy-to-use SQL-like language. As the first Siddhi Application, let's learn how to find the total of values from the incoming events and output the current running total value for each event. Siddhi has lot of in-built functions and extensions available for complex analysis, and you can find more information about the Siddhi grammar and its functions from the Siddhi Query Guide . Let's consider sample scenario where we are loading cargo boxes into a ship . Here, we need to keep track of the total weight of the cargo added, and the weight of each loaded cargo box is considered an event . We can write a Siddhi Application for the above scenario using the following 4 parts . Part 1\u200a\u2014\u200aGiving our Siddhi application a suitable name. This allows us to uniquely identity a Siddhi Application. In this example, let's name our application as \u201cHelloWorldApp\u201d @App:name(\"HelloWorldApp\") Part 2\u200a\u2014\u200aDefining the input stream. The stream needs to have a name and a schema defining the data that each incoming event should contain. The event data attributes are expressed as name and type pairs. We can also attach a \"source\" to the created stream, so that we can consume events from outside and send them to the stream. ( Source is the Siddhi way to consume streams from external systems ). For this scenario we will use an http source to consume Cargo Events. When added the http source will spin up a HTTP endpoint and keep on listening for messages. To learn more about sources, refer source ) In this scenario: The name of the input stream\u200a\u2014\u200a \u201cCargoStream\u201d This contains only one data attribute: The name of the data in each event\u200a\u2014\u200a \u201cweight\u201d Type of the data \u201cweight\u201d \u200a\u2014\u200aint Type of source - HTTP HTTP endpoint address - http://0.0.0.0:8006/cargo Accepted input data format - JSON @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); Part 3 - Defining the output stream. This has the same info as the input \u201cCargoStream\u201d stream\u200adefinition with an additional totalWeight attribute containing the total weight calculated so far. In addition we also need to add a log \"sink\" to log the OutputStream so that we can observe the output produced by the stream. ( Sink is the Siddhi way to publish streams to external systems ). This particular log type sink simply logs the stream events. To learn more about sinks, refer sink ) @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); Part 4\u200a\u2014\u200aWriting the Siddhi query. As part of the query we need to specify the following: A name for the query\u200a\u2014\u200a \u201cHelloWorldQuery\u201d The input stream from which the query consumes events \u2014\u200a \u201cCargoStream\u201d How the output to be calculated - by calculating the sum of the *weight**s The data outputted to the output stream\u200a\u2014\u200a \u201cweight\u201d , \u201ctotalWeight\u201d The output stream to which the event should be outputted\u200a\u2014\u200a \u201cOutputStream\u201d @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; This query will calculate the sum of weights from the start of the Siddhi application. For more complex use cases refer Siddhi Query Guild ) Final Siddhi application in the editor will look like following. You can copy the final Siddhi app from below. @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; 5. Testing Siddhi Application \u00b6 In this section first we will test the logical accuracy of Siddhi query using in-built functions of Siddhi Editor. In a later section we will invoke the HTTP endpoint and perform an end to end test. The Siddhi Editor has in-built support to simulate events. You can do it via the \u201cEvent Simulator\u201d panel at the left of the Siddhi Editor. Before running the event simulation, you should save your HelloWorldApp by browsing to File menu -> and clicking Save . To simulate events, click Event Simulator and configure Single Simulation as shown below. Step 1\u200a\u2014\u200aConfigurations: Siddhi App Name\u200a\u2014\u200a \u201cHelloWorldApp\u201d Stream Name\u200a\u2014\u200a \u201cCargoStream\u201d Timestamp\u200a\u2014\u200a(Leave it blank) weight\u200a\u2014\u200a2 (or some integer) Step 2\u200a\u2014\u200aClick \u201cRun\u201d mode and then click \u201cStart and Send\u201d . This starts the Siddhi Application and send the event. If the Siddhi application is successfully started, the following message is printed in the Stream Processor Studio console: HelloWorldApp.siddhi Started Successfully! Step 3\u200a\u2014\u200aClick \u201cSend\u201d and observe the terminal . This will send a new event for each click. You can see a logs containing outputData=[2, 2] and outputData=[2, 4] , etc. You can change the value of the weight and send it to see how the sum of the weight is updated. Bravo! You have successfully completed building and testing your first Siddhi Application! 6. A bit of Stream Processing \u00b6 This section will improve our Siddhi app to demonstrates how to carry out temporal window processing with Siddhi. Up to this point, we are calculating the sum of weights from the start of the Siddhi app, and now let's improve it to consider only the last three events for the calculation. For this scenario, let's imagine that when we are loading cargo boxes into the ship and we need to keep track of the average weight of the last three loaded boxes so that we can balance the weight across the ship. For this purpose, let's try to find the average weight of last three boxes of each event. For window processing, we need to modify our query as follows: @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; from CargoStream#window.length(3) - Specifies that we need to consider the last three events in a sliding manner. avg(weight) as averageWeight - Specifies calculating the average of events stored in the window and producing the results as \"averageWeight\" (Note: Similarly the sum also calculates the totalWeight based on the last three events). We also need to modify the \"OutputStream\" definition to accommodate the new \"averageWeight\" . define stream OutputStream(weight int, totalWeight long, averageWeight double); The updated Siddhi Application is given below: @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\",@map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long, averageWeight double); @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; Now you can send events using the Event Simulator and observe the log to see the sum and average of the weights based on the last three cargo events. In the earlier scenario when the window is not used, the system only stored the running sum in its memory, and it did not store any events. But for length based window processing the system will retain the events that fall into the window to perform aggregation operations such as average, maximum, etc. In this case when the 4 th event arrives, the first event in the window is removed ensuring the memory usage does not grow beyond a specific limit. Note: some window types in Siddhi are even more optimized to perform the operations with minimal or no event retention. 7. Running Siddhi Application as a Docker microservice \u00b6 In this step we will run above developed Siddhi application as a microservice utilizing Docker. For other available options please refer here . Here we will use siddhi-runner docker distribution. Follow the below steps to obtain the docker. Install docker in your machine and start the daemon ( https://docs.docker.com/install/ ). Pull the latest siddhi-runner image by executing below command. docker pull siddhiio/siddhi-runner-alpine:latest * Navigate to Siddhi Editor and choose File -> Export File for download above Siddhi application as a file. * Move downloaded Siddhi file( HelloWorldApp.siddhi ) to a desired location (e.g. /home/me/siddhi-apps ) * Execute below command to start the Siddhi Application as a microservice. docker run -it -p 8006:8006 -v /home/me/siddhi-apps:/apps siddhiio/siddhi-runner-alpine -Dapps=/apps/HelloWorldApp.siddhi Note: Make sure to update the /home/me/siddhi-apps with the folder path you have stored the HelloWorldApp.siddhi app. * Once container is started use below curl command to send events into \"CargoStream\" curl -X POST http://localhost:8006/cargo \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"weight\":2}}' * You will be able to observe outputs via logs as shown below. [2019-04-24 08:54:51,755] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096091751, data=[2, 2, 2.0], isExpired=false} [2019-04-24 08:56:25,307] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096185307, data=[2, 4, 2.0], isExpired=false} To learn more about the Siddhi functionality, see Siddhi Documentation . If you have questions please post them on Stackoverflow with \"Siddhi\" tag.","title":"Quick Start"},{"location":"docs/quick-start/#siddhi-52-quick-start-guide","text":"Siddhi is a cloud native Streaming and Complex Event Processing engine that understands Streaming SQL queries in order to capture events from diverse data sources, process them, detect complex conditions, and publish output to various endpoints in real time. Siddhi is used by many companies including Uber, eBay, PayPal (via Apache Eagle), here Uber processed more than 20 billion events per day using Siddhi for their fraud analytics use cases. Siddhi is also used in various analytics and integration platforms such as Apache Eagle as a policy enforcement engine, WSO2 API Manager as analytics and throttling engine, WSO2 Identity Server as an adaptive authorization engine. This quick start guide contains the following six sections: Domain of Siddhi Overview of Siddhi architecture Using Siddhi for the first time Writing first Siddhi Application Testing Siddhi Application A bit of Stream Processing","title":"Siddhi 5.2 Quick Start Guide"},{"location":"docs/quick-start/#1-domain-of-siddhi","text":"Siddhi is an event driven system where all the data it consumes, processes and sends are modeled as events. Therefore, Siddhi can play a vital part in any event-driven architecture. As Siddhi works with events, first let's understand what an event is through an example. If we consider transactions carried out via an ATM as a data stream, one withdrawal from it can be considered as an event . This event contains data such as amount, time, account number, etc. Many such transactions form a stream. Siddhi provides following functionalities, Streaming Data Analytics Forrester defines Streaming Analytics as: Software that provides analytical operators to orchestrate data flow , calculate analytics , and detect patterns on event data from multiple, disparate live data sources to allow developers to build applications that sense, think, and act in real time . Complex Event Processing (CEP) Gartner\u2019s IT Glossary defines CEP as follows: \"CEP is a kind of computing in which incoming data about events is distilled into more useful, higher level \u201ccomplex\u201d event data that provides insight into what is happening.\" \" CEP is event-driven because the computation is triggered by the receipt of event data. CEP is used for highly demanding, continuous-intelligence applications that enhance situation awareness and support real-time decisions.\" Streaming Data Integration Streaming data integration is a way of integrating several systems by processing, correlating, and analyzing the data in memory, while continuously moving data in real-time from one system to another. Alerts & Notifications The system to continuously monitor event streams, and send alerts and notifications, based on defined KPIs and other analytics. Adaptive Decision Making A way to dynamically making real-time decisions based on predefined rules, the current state of the connected systems, and machine learning techniques. Basically, Siddhi receives data event-by-event and processes them in real-time to produce meaningful information. Using the above Siddhi can be used to solve may use-cases as follows: Fraud Analytics Monitoring System Integration Anomaly Detection Sentiment Analysis Processing Customer Behavior .. etc","title":"1. Domain of Siddhi"},{"location":"docs/quick-start/#2-overview-of-siddhi-architecture","text":"As indicated above, Siddhi can: Accept event inputs from many different types of sources. Process them to transform, enrich, and generate insights. Publish them to multiple types of sinks. To use Siddhi, you need to write the processing logic as a Siddhi Application in the Siddhi Streaming SQL language which is discussed in the section 4 . Here a Siddhi Application is a script file that contains business logic for a scenario. When the Siddhi application is started, it: Consumes data one-by-one as events. Pipe the events to queries through various streams for processing. Generates new events based on the processing done at the queries. Finally, Sends newly generated events through output to streams.","title":"2. Overview of Siddhi architecture"},{"location":"docs/quick-start/#3-using-siddhi-for-the-first-time","text":"In this section, we will be using the Siddhi tooling distribution\u200a\u2014\u200aa server version of Siddhi that has a sophisticated web based editor with a GUI (referred to as \u201cSiddhi Editor\u201d ) where you can write Siddhi Apps and simulate events to test your scenario. Step 1 \u200a\u2014\u200aInstall Oracle Java SE Development Kit (JDK) version 1.8. Step 2 \u200a\u2014\u200a Set the JAVA_HOME environment variable. Step 3 \u200a\u2014\u200aDownload the latest tooling distribution from here . Step 4 \u200a\u2014\u200aExtract the downloaded zip and navigate to <TOOLING_HOME>/bin . ( TOOLING_HOME refers to the extracted folder) Step 5 \u200a\u2014\u200aIssue the following command in the command prompt (Windows) / terminal (Linux/Mac) For Windows: tooling.bat For Linux/Mac: ./tooling.sh After successfully starting the Siddhi Editor, the terminal should look like as shown below: After starting the Siddhi Editor, access the Editor GUI by visiting the following link in your browser (Google Chrome is the Recommended). http://localhost:9390/editor This takes you to the Siddhi Editor landing page.","title":"3. Using Siddhi for the first time"},{"location":"docs/quick-start/#4-writing-first-siddhi-application","text":"Siddhi Streaming SQL is a rich, compact, easy-to-use SQL-like language. As the first Siddhi Application, let's learn how to find the total of values from the incoming events and output the current running total value for each event. Siddhi has lot of in-built functions and extensions available for complex analysis, and you can find more information about the Siddhi grammar and its functions from the Siddhi Query Guide . Let's consider sample scenario where we are loading cargo boxes into a ship . Here, we need to keep track of the total weight of the cargo added, and the weight of each loaded cargo box is considered an event . We can write a Siddhi Application for the above scenario using the following 4 parts . Part 1\u200a\u2014\u200aGiving our Siddhi application a suitable name. This allows us to uniquely identity a Siddhi Application. In this example, let's name our application as \u201cHelloWorldApp\u201d @App:name(\"HelloWorldApp\") Part 2\u200a\u2014\u200aDefining the input stream. The stream needs to have a name and a schema defining the data that each incoming event should contain. The event data attributes are expressed as name and type pairs. We can also attach a \"source\" to the created stream, so that we can consume events from outside and send them to the stream. ( Source is the Siddhi way to consume streams from external systems ). For this scenario we will use an http source to consume Cargo Events. When added the http source will spin up a HTTP endpoint and keep on listening for messages. To learn more about sources, refer source ) In this scenario: The name of the input stream\u200a\u2014\u200a \u201cCargoStream\u201d This contains only one data attribute: The name of the data in each event\u200a\u2014\u200a \u201cweight\u201d Type of the data \u201cweight\u201d \u200a\u2014\u200aint Type of source - HTTP HTTP endpoint address - http://0.0.0.0:8006/cargo Accepted input data format - JSON @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); Part 3 - Defining the output stream. This has the same info as the input \u201cCargoStream\u201d stream\u200adefinition with an additional totalWeight attribute containing the total weight calculated so far. In addition we also need to add a log \"sink\" to log the OutputStream so that we can observe the output produced by the stream. ( Sink is the Siddhi way to publish streams to external systems ). This particular log type sink simply logs the stream events. To learn more about sinks, refer sink ) @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); Part 4\u200a\u2014\u200aWriting the Siddhi query. As part of the query we need to specify the following: A name for the query\u200a\u2014\u200a \u201cHelloWorldQuery\u201d The input stream from which the query consumes events \u2014\u200a \u201cCargoStream\u201d How the output to be calculated - by calculating the sum of the *weight**s The data outputted to the output stream\u200a\u2014\u200a \u201cweight\u201d , \u201ctotalWeight\u201d The output stream to which the event should be outputted\u200a\u2014\u200a \u201cOutputStream\u201d @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream; This query will calculate the sum of weights from the start of the Siddhi application. For more complex use cases refer Siddhi Query Guild ) Final Siddhi application in the editor will look like following. You can copy the final Siddhi app from below. @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\", @map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long); @info(name='HelloWorldQuery') from CargoStream select weight, sum(weight) as totalWeight insert into OutputStream;","title":"4. Writing first Siddhi Application"},{"location":"docs/quick-start/#5-testing-siddhi-application","text":"In this section first we will test the logical accuracy of Siddhi query using in-built functions of Siddhi Editor. In a later section we will invoke the HTTP endpoint and perform an end to end test. The Siddhi Editor has in-built support to simulate events. You can do it via the \u201cEvent Simulator\u201d panel at the left of the Siddhi Editor. Before running the event simulation, you should save your HelloWorldApp by browsing to File menu -> and clicking Save . To simulate events, click Event Simulator and configure Single Simulation as shown below. Step 1\u200a\u2014\u200aConfigurations: Siddhi App Name\u200a\u2014\u200a \u201cHelloWorldApp\u201d Stream Name\u200a\u2014\u200a \u201cCargoStream\u201d Timestamp\u200a\u2014\u200a(Leave it blank) weight\u200a\u2014\u200a2 (or some integer) Step 2\u200a\u2014\u200aClick \u201cRun\u201d mode and then click \u201cStart and Send\u201d . This starts the Siddhi Application and send the event. If the Siddhi application is successfully started, the following message is printed in the Stream Processor Studio console: HelloWorldApp.siddhi Started Successfully! Step 3\u200a\u2014\u200aClick \u201cSend\u201d and observe the terminal . This will send a new event for each click. You can see a logs containing outputData=[2, 2] and outputData=[2, 4] , etc. You can change the value of the weight and send it to see how the sum of the weight is updated. Bravo! You have successfully completed building and testing your first Siddhi Application!","title":"5. Testing Siddhi Application"},{"location":"docs/quick-start/#6-a-bit-of-stream-processing","text":"This section will improve our Siddhi app to demonstrates how to carry out temporal window processing with Siddhi. Up to this point, we are calculating the sum of weights from the start of the Siddhi app, and now let's improve it to consider only the last three events for the calculation. For this scenario, let's imagine that when we are loading cargo boxes into the ship and we need to keep track of the average weight of the last three loaded boxes so that we can balance the weight across the ship. For this purpose, let's try to find the average weight of last three boxes of each event. For window processing, we need to modify our query as follows: @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; from CargoStream#window.length(3) - Specifies that we need to consider the last three events in a sliding manner. avg(weight) as averageWeight - Specifies calculating the average of events stored in the window and producing the results as \"averageWeight\" (Note: Similarly the sum also calculates the totalWeight based on the last three events). We also need to modify the \"OutputStream\" definition to accommodate the new \"averageWeight\" . define stream OutputStream(weight int, totalWeight long, averageWeight double); The updated Siddhi Application is given below: @App:name(\"HelloWorldApp\") @source(type = 'http', receiver.url = \"http://0.0.0.0:8006/cargo\",@map(type = 'json')) define stream CargoStream (weight int); @sink(type='log', prefix='LOGGER') define stream OutputStream(weight int, totalWeight long, averageWeight double); @info(name='HelloWorldQuery') from CargoStream#window.length(3) select weight, sum(weight) as totalWeight, avg(weight) as averageWeight insert into OutputStream; Now you can send events using the Event Simulator and observe the log to see the sum and average of the weights based on the last three cargo events. In the earlier scenario when the window is not used, the system only stored the running sum in its memory, and it did not store any events. But for length based window processing the system will retain the events that fall into the window to perform aggregation operations such as average, maximum, etc. In this case when the 4 th event arrives, the first event in the window is removed ensuring the memory usage does not grow beyond a specific limit. Note: some window types in Siddhi are even more optimized to perform the operations with minimal or no event retention.","title":"6. A bit of Stream Processing"},{"location":"docs/quick-start/#7-running-siddhi-application-as-a-docker-microservice","text":"In this step we will run above developed Siddhi application as a microservice utilizing Docker. For other available options please refer here . Here we will use siddhi-runner docker distribution. Follow the below steps to obtain the docker. Install docker in your machine and start the daemon ( https://docs.docker.com/install/ ). Pull the latest siddhi-runner image by executing below command. docker pull siddhiio/siddhi-runner-alpine:latest * Navigate to Siddhi Editor and choose File -> Export File for download above Siddhi application as a file. * Move downloaded Siddhi file( HelloWorldApp.siddhi ) to a desired location (e.g. /home/me/siddhi-apps ) * Execute below command to start the Siddhi Application as a microservice. docker run -it -p 8006:8006 -v /home/me/siddhi-apps:/apps siddhiio/siddhi-runner-alpine -Dapps=/apps/HelloWorldApp.siddhi Note: Make sure to update the /home/me/siddhi-apps with the folder path you have stored the HelloWorldApp.siddhi app. * Once container is started use below curl command to send events into \"CargoStream\" curl -X POST http://localhost:8006/cargo \\ --header \"Content-Type:application/json\" \\ -d '{\"event\":{\"weight\":2}}' * You will be able to observe outputs via logs as shown below. [2019-04-24 08:54:51,755] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096091751, data=[2, 2, 2.0], isExpired=false} [2019-04-24 08:56:25,307] INFO {io.siddhi.core.stream.output.sink.LogSink} - LOGGER : Event{timestamp=1556096185307, data=[2, 4, 2.0], isExpired=false} To learn more about the Siddhi functionality, see Siddhi Documentation . If you have questions please post them on Stackoverflow with \"Siddhi\" tag.","title":"7. Running Siddhi Application as a Docker microservice"}]}